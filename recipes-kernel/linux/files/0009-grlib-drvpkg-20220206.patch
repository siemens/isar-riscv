diff -Nru a/drivers/grlib/add_grlib_tree_5.10.patch b/drivers/grlib/add_grlib_tree_5.10.patch
--- a/drivers/grlib/add_grlib_tree_5.10.patch
+++ b/drivers/grlib/add_grlib_tree_5.10.patch
@@ -0,0 +1,19 @@
+diff --git a/drivers/Kconfig b/drivers/Kconfig
+--- a/drivers/Kconfig
++++ b/drivers/Kconfig
+@@ -235,4 +235,7 @@
+ source "drivers/counter/Kconfig"
+ 
+ source "drivers/most/Kconfig"
++
++source "drivers/grlib/Kconfig"
++
+ endmenu
+--- a/drivers/Makefile
++++ b/drivers/Makefile
+@@ -189,3 +189,5 @@
+ obj-$(CONFIG_INTERCONNECT)	+= interconnect/
+ obj-$(CONFIG_COUNTER)		+= counter/
+ obj-$(CONFIG_MOST)		+= most/
++
++obj-$(CONFIG_GRLIB_DRVPKG)	+= grlib/
diff -Nru a/drivers/grlib/include/linux/grlib/devno.h b/drivers/grlib/include/linux/grlib/devno.h
--- a/drivers/grlib/include/linux/grlib/devno.h
+++ b/drivers/grlib/include/linux/grlib/devno.h
@@ -0,0 +1,43 @@
+/*
+ * GRLIB Driver package Major/Minor device numbering
+ *
+ * Copyright (c) 2016-2022 Cobham Gaisler AB
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston,
+ * MA 02110-1301 USA
+ */
+
+#ifndef __GRLIB_DEVNO_H__
+#define __GRLIB_DEVNO_H__
+
+/* Device Node Major and Minor Definitions for GRLIB Drivers */
+
+#define GRLIB_MAJOR_BASE 60
+
+#define MAPLIB_MAJOR		GRLIB_MAJOR_BASE
+#define GRSPWU_MAJOR		(MAPLIB_MAJOR+1)
+#define SPWROUTER_MAJOR		GRSPWU_MAJOR
+
+/* Number of device nodes per driver */
+#define MAPLIB_DEVCNT		32
+#define GRSPWU_DEVCNT		32
+#define SPWROUTER_DEVCNT	4
+
+/* Minor Devices */
+#define MAPLIB_MINOR		0
+#define GRSPWU_MINOR		0
+#define SPWROUTER_MINOR		(GRSPWU_MINOR+GRSPWU_DEVCNT)
+
+#endif
diff -Nru a/drivers/grlib/include/linux/grlib/grspw.h b/drivers/grlib/include/linux/grlib/grspw.h
--- a/drivers/grlib/include/linux/grlib/grspw.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/grlib/include/linux/grlib/grspw.h	2022-01-29 23:16:14.137064547 +0100
@@ -0,0 +1,668 @@
+/*
+ * GRSPW2 SpaceWire Kernel Library Interface
+ *
+ * Copyright (c) 2016-2022 Cobham Gaisler AB
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston,
+ * MA 02110-1301 USA
+ */
+
+
+#ifndef __GRSPW2_LIB_H__
+#define __GRSPW2_LIB_H__
+
+#ifndef __KERNEL__
+#include <stdint.h>
+#endif
+
+struct grspw_pkt;
+
+/* Maximum number of GRSPW devices supported by driver */
+#define GRSPW_MAX 32
+
+/* Device Numbers for different GRSPW implementations in AMBA PnP */
+#ifndef GAISLER_SPW
+#define GAISLER_SPW 0x01f
+#endif
+#ifndef GAISLER_SPW2
+#define GAISLER_SPW2 0x029
+#endif
+#ifndef GAISLER_SPW2_DMA
+#define GAISLER_SPW2_DMA 0x8a
+#endif
+
+#ifndef GRSPW_PKT_FLAGS
+#define GRSPW_PKT_FLAGS
+/*** TX Packet flags ***/
+
+/* Enable IRQ generation */
+#define TXPKT_FLAG_IE 0x0040
+
+/* Enable Header CRC generation (if CRC is available in HW)
+ * Header CRC will be appended (one byte at end of header)
+ */
+#define TXPKT_FLAG_HCRC 0x0100
+
+/* Enable Data CRC generation (if CRC is available in HW)
+ * Data CRC will be appended (one byte at end of packet)
+ */
+#define TXPKT_FLAG_DCRC 0x0200
+
+/* Control how many bytes the beginning of the Header 
+ * the CRC should not be calculated for */
+#define TXPKT_FLAG_NOCRC_MASK 0x0000000f
+#define TXPKT_FLAG_NOCRC_LEN0 0x00000000
+#define TXPKT_FLAG_NOCRC_LEN1 0x00000001
+#define TXPKT_FLAG_NOCRC_LEN2 0x00000002
+#define TXPKT_FLAG_NOCRC_LEN3 0x00000003
+#define TXPKT_FLAG_NOCRC_LEN4 0x00000004
+#define TXPKT_FLAG_NOCRC_LEN5 0x00000005
+#define TXPKT_FLAG_NOCRC_LEN6 0x00000006
+#define TXPKT_FLAG_NOCRC_LEN7 0x00000007
+#define TXPKT_FLAG_NOCRC_LEN8 0x00000008
+#define TXPKT_FLAG_NOCRC_LEN9 0x00000009
+#define TXPKT_FLAG_NOCRC_LENa 0x0000000a
+#define TXPKT_FLAG_NOCRC_LENb 0x0000000b
+#define TXPKT_FLAG_NOCRC_LENc 0x0000000c
+#define TXPKT_FLAG_NOCRC_LENd 0x0000000d
+#define TXPKT_FLAG_NOCRC_LENe 0x0000000e
+#define TXPKT_FLAG_NOCRC_LENf 0x0000000f
+
+#define TXPKT_FLAG_INPUT_MASK (TXPKT_FLAG_NOCRC_MASK | TXPKT_FLAG_IE | \
+				TXPKT_FLAG_HCRC | TXPKT_FLAG_DCRC)
+
+/* Marks if packet was transmitted or not */
+#define TXPKT_FLAG_TX 0x4000
+
+/* Link Error */
+#define TXPKT_FLAG_LINKERR 0x8000
+
+#define TXPKT_FLAG_OUTPUT_MASK (TXPKT_FLAG_TX | TXPKT_FLAG_LINKERR)
+
+/*** RX Packet Flags ***/
+
+/* Enable IRQ generation */
+#define RXPKT_FLAG_IE 0x0010
+
+#define RXPKT_FLAG_INPUT_MASK (RXPKT_FLAG_IE)
+
+/* Packet was truncated */
+#define RXPKT_FLAG_TRUNK 0x0800
+/* Data CRC error (only valid if RMAP CRC is enabled) */
+#define RXPKT_FLAG_DCRC 0x0400
+/* Header CRC error (only valid if RMAP CRC is enabled) */
+#define RXPKT_FLAG_HCRC 0x0200
+/* Error in End-of-Packet */
+#define RXPKT_FLAG_EEOP 0x0100
+/* Marks if packet was recevied or not */
+#define RXPKT_FLAG_RX 0x8000
+
+#define RXPKT_FLAG_OUTPUT_MASK (RXPKT_FLAG_TRUNK | RXPKT_FLAG_DCRC | \
+				RXPKT_FLAG_HCRC | RXPKT_FLAG_EEOP)
+
+/*** General packet flag options ***/
+
+/* Translate Hdr and/or Payload address */
+#define PKT_FLAG_TR_DATA_RESERVED 0x1000
+#define PKT_FLAG_TR_HDR_RESERVED 0x2000
+/* All General options */
+#define PKT_FLAG_MASK_RESERVED 0x3000
+
+#endif
+/* GRSPW RX/TX Packet structure.
+ *
+ * - For RX the 'hdr' and 'hlen' fields are not used, they are not written
+ *   by driver.
+ *
+ * - The 'pkt_id' field is untouched by driver, it is intended for packet
+ *   numbering or user-custom data.
+ *
+ * - The last packet in a list must have 'next' set to NULL.
+ *
+ * - data and hdr pointers are written without modification to hardware,
+ *   this means that caller must do address translation to hardware
+ *   address itself.
+ *
+ * - the 'flags' field are interpreted differently depending on transfer
+ *   type (RX/TX). See XXPKT_FLAG_* options above.
+ */
+struct grspw_pkt {
+	struct grspw_pkt *next;	/* Next packet in list. NULL if last packet */
+	unsigned int pkt_id;	/* User assigned ID (not touched by driver) */
+	unsigned short flags;	/* RX/TX Options and status */
+	unsigned char reserved;	/* Reserved, must be zero */
+	unsigned char hlen;	/* Length of Header Buffer (only TX) */
+	unsigned int dlen;	/* Length of Data Buffer */
+#ifdef __KERNEL__
+	u32 data;		/* 4-byte or byte aligned address depends on HW */
+	u32 hdr;		/* 4-byte or byte aligned addressdepends on HW (only TX) */
+#else
+	uint32_t data;		/* 4-byte or byte aligned address depends on HW */
+	uint32_t hdr;		/* 4-byte or byte aligned address depends on HW (only TX) */
+#endif
+};
+#define GRSPW_PKT_DMAADDR_INVALID 0xFFFFFFFFUL /* indicates no data */
+
+/* GRSPW SpaceWire Packet List */
+struct grspw_list {
+	struct grspw_pkt *head;
+	struct grspw_pkt *tail;
+};
+
+/* SpaceWire Link State */
+typedef enum {
+	SPW_LS_ERRRST = 0,
+	SPW_LS_ERRWAIT = 1,
+	SPW_LS_READY = 2,
+	SPW_LS_CONNECTING = 3,
+	SPW_LS_STARTED = 4,
+	SPW_LS_RUN = 5
+} spw_link_state_t;
+
+/* Address Configuration */
+struct grspw_addr_config {
+	/* Ignore address field and put all received packets to first
+	 * DMA channel.
+	 */
+	int promiscuous;
+
+	/* Default Node Address and Mask */
+	unsigned char def_addr;
+	unsigned char def_mask;
+	/* DMA Channel custom Node Address and Mask */
+	struct {
+		char node_en;			/* Enable Separate Addr */
+		unsigned char node_addr;	/* Node address */
+		unsigned char node_mask;	/* Node address mask */
+	} dma_nacfg[4];
+};
+
+/* Hardware Support in GRSPW Core */
+struct grspw_hw_sup {
+	char	rmap;		/* If RMAP in HW is available */
+	char	rmap_crc;	/* If RMAP CRC is available */
+	char	rx_unalign;	/* RX unaligned (byte boundary) access allowed*/
+	char	nports;		/* Number of Ports (1 or 2) */
+	char	ndma_chans;	/* Number of DMA Channels (1..4) */
+	char	strip_adr;	/* Hardware can strip ADR from packet data */
+	char	strip_pid;	/* Hardware can strip PID from packet data */
+	int	hw_version;	/* GRSPW Hardware Version */
+	char	reserved[2];
+};
+
+struct grspw_core_stats {
+	int irq_cnt;
+	int err_credit;
+	int err_eeop;
+	int err_addr;
+	int err_parity;
+	int err_disconnect;
+	int err_escape;
+	int err_wsync; /* only in GRSPW1 */
+};
+
+/* grspw_link_ctrl() options */
+#define LINKOPTS_ENABLE		0x0000
+#define LINKOPTS_DISABLE	0x0001
+#define LINKOPTS_START		0x0002
+#define LINKOPTS_AUTOSTART	0x0004
+#define LINKOPTS_DIS_ONERR	0x0008	/* Disable DMA transmitter on link error
+					 * Controls LE bit in DMACTRL register.
+					 */
+#define LINKOPTS_DIS_ON_CE	0x0020000/* Disable Link on Credit error */
+#define LINKOPTS_DIS_ON_ER	0x0040000/* Disable Link on Escape error */
+#define LINKOPTS_DIS_ON_DE	0x0080000/* Disable Link on Disconnect error */
+#define LINKOPTS_DIS_ON_PE	0x0100000/* Disable Link on Parity error */
+#define LINKOPTS_DIS_ON_WE	0x0400000/* Disable Link on write synchonization
+					  * error (GRSPW1 only)
+					  */
+#define LINKOPTS_DIS_ON_EE	0x1000000/* Disable Link on Early EOP/EEP error*/
+
+/*#define LINKOPTS_TICK_OUT_IRQ	0x0100*//* Enable Tick-out IRQ */
+#define LINKOPTS_EIRQ		0x0200	/* Enable Error Link IRQ */
+
+#define LINKOPTS_MASK		0x15e020f/* All above options */
+#define LINKOPTS_MASK_DIS_ON	0x15e0000/* All disable link on error options
+					  * On a certain error the link disable
+					  * bit will be written and the work
+					  * task will call dma_stop() for all
+					  * channels.
+					  */
+
+#define LINKSTS_CE		0x002	/* Credit error */
+#define LINKSTS_ER		0x004	/* Escape error */
+#define LINKSTS_DE		0x008	/* Disconnect error */
+#define LINKSTS_PE		0x010	/* Parity error */
+#define LINKSTS_WE		0x040	/* Write synchonization error (GRSPW1 only) */
+#define LINKSTS_IA		0x080	/* Invalid address */
+#define LINKSTS_EE		0x100	/* Early EOP/EEP */
+#define LINKSTS_MASK		0x1de
+
+/* grspw_tc_ctrl() options */
+#define TCOPTS_EN_RXIRQ	0x0001	/* Tick-Out IRQ */
+#define TCOPTS_EN_TX	0x0004
+#define TCOPTS_EN_RX	0x0008
+
+/* grspw_rmap_ctrl() options */
+#define RMAPOPTS_EN_RMAP	0x0001
+#define RMAPOPTS_EN_BUF		0x0002
+
+/* grspw_dma_config.flags options */
+#define DMAFLAG_NO_SPILL	0x0001	/* See HW doc DMA-CTRL NS bit */
+#define DMAFLAG_RESV1		0x0002	/* HAS NO EFFECT */
+#define DMAFLAG_STRIP_ADR	0x0004	/* See HW doc DMA-CTRL SA bit */
+#define DMAFLAG_STRIP_PID	0x0008	/* See HW doc DMA-CTRL SP bit */
+#define DMAFLAG_RESV2		0x0010	/* HAS NO EFFECT */
+#define DMAFLAG_MASK	(DMAFLAG_NO_SPILL|DMAFLAG_STRIP_ADR|DMAFLAG_STRIP_PID)
+/* grspw_dma_config.flags misc options (not shifted internally) */
+#define DMAFLAG2_TXIE	0x00100000	/* See HW doc DMA-CTRL TI bit. 
+					 * Used to enable TX DMA interrupt
+					 * when tx_irq_en_cnt=0.
+					 */
+#define DMAFLAG2_RXIE	0x00200000	/* See HW doc DMA-CTRL RI bit.
+					 * Used to enable RX DMA interrupt
+					 * when rx_irq_en_cnt=0.
+					 */
+#define DMAFLAG2_MASK	(DMAFLAG2_TXIE | DMAFLAG2_RXIE)
+
+struct grspw_dma_config {
+	int flags;		/* DMA config flags, see DMAFLAG1&2_* options */
+	int rxmaxlen;		/* RX Max Packet Length */
+	int rx_irq_en_cnt;	/* Enable RX IRQ every cnt descriptors */
+	int tx_irq_en_cnt;	/* Enable TX IRQ every cnt descriptors */
+};
+
+/* Statistics per DMA channel */
+struct grspw_dma_stats {
+	/* IRQ Statistics */
+	int irq_cnt;		/* Number of DMA IRQs generated by channel */
+
+	/* Descriptor Statistics */
+	int tx_pkts;		/* Number of Transmitted packets */
+	int tx_err_link;	/* Number of Transmitted packets with Link Error*/
+	int rx_pkts;		/* Number of Received packets */
+	int rx_err_trunk;	/* Number of Received Truncated packets */
+	int rx_err_endpkt;	/* Number of Received packets with bad ending */
+
+	/* Diagnostics to help developers sizing their number buffers to avoid
+	 * out-of-buffers or other phenomenons.
+	 */
+	int send_cnt_min;	/* Minimum number of packets in TX SEND Q */
+	int send_cnt_max;	/* Maximum number of packets in TX SEND Q */
+	int tx_sched_cnt_min;	/* Minimum number of packets in TX SCHED Q */
+	int tx_sched_cnt_max;	/* Maximum number of packets in TX SCHED Q */
+	int sent_cnt_max;	/* Maximum number of packets in TX SENT Q */
+	int tx_work_cnt;	/* Times the work thread processed TX BDs */
+	int tx_work_enabled;	/* No. RX BDs enabled by work thread */
+
+	int ready_cnt_min;	/* Minimum number of packets in RX READY Q */
+	int ready_cnt_max;	/* Maximum number of packets in RX READY Q */
+	int rx_sched_cnt_min;	/* Minimum number of packets in RX SCHED Q */
+	int rx_sched_cnt_max;	/* Maximum number of packets in RX SCHED Q */
+	int recv_cnt_max;	/* Maximum number of packets in RX RECV Q */
+	int rx_work_cnt;	/* Times the work thread processed RX BDs */
+	int rx_work_enabled;	/* No. RX BDs enabled by work thread */
+};
+
+#ifdef __KERNEL__
+extern void grspw_initialize_user(
+	/* Callback every time a GRSPW device is found. Args: DeviceIndex */
+	void *(*devfound)(int),
+	/* Callback every time a GRSPW device is removed. Args:
+	 * int   = DeviceIndex
+	 * void* = Return Value from devfound()
+	 */
+	void (*devremove)(int,void*)
+	);
+extern int grspw_dev_count(void);
+extern void *grspw_open(int dev_no);
+extern int grspw_close(void *d);
+extern void grspw_hw_support(void *d, struct grspw_hw_sup *hw);
+extern void grspw_stats_read(void *d, struct grspw_core_stats *sts);
+extern void grspw_stats_clr(void *d);
+
+/* Set and Read current node address configuration. The dma_nacfg[N] field
+ * represents the configuration for DMA Channel N.
+ *
+ * Set cfg->promiscous to -1 in order to only read current configuration.
+ */
+extern void grspw_addr_ctrl(void *d, struct grspw_addr_config *cfg);
+
+/*** Link Control interface ***/
+/* Read Link State */
+extern spw_link_state_t grspw_link_state(void *d);
+/* options [in/out]: set to -1 to only read current config
+ *
+ * CLKDIV register contain:
+ *  bits 7..0  : Clock Div RUN (only run-state)
+ *  bits 15..8 : Clock Div During Startup (all link states except run-state)
+ */
+extern void grspw_link_ctrl(void *d, int *options, int *stscfg, int *clkdiv);
+/* Read the current value of the status register */
+extern unsigned int grspw_link_status(void *d);
+/* Clear bits in the status register */
+extern void grspw_link_status_clr(void *d, unsigned int clearmask);
+
+/*** Time Code Interface ***/
+/* Generate Tick-In (increment Time Counter, Send Time Code) */
+extern void grspw_tc_tx(void *d);
+/* Control Timcode settings of core */
+extern void grspw_tc_ctrl(void *d, int *options);
+/* Assign ISR Function to TimeCode RX IRQ */
+extern void grspw_tc_isr(void *d, void (*tcisr)(void *data, int tc), void *data);
+/* Read/Write TCTRL and TIMECNT. Write if not -1, always read current value
+ * TCTRL   = bits 7 and 6
+ * TIMECNT = bits 5 to 0
+ */
+extern void grspw_tc_time(void *d, int *time);
+
+/*** RMAP Control Interface ***/
+/* Set (not -1) and/or read RMAP options. */
+extern int grspw_rmap_ctrl(void *d, int *options, int *dstkey);
+extern void grspw_rmap_support(void *d, char *rmap, char *rmap_crc);
+
+/*** SpW Port Control Interface ***/
+
+/* Select port, if
+ * -1=The current selected port is returned
+ * 0=Port 0
+ * 1=Port 1
+ * Other positive values=Both Port0 and Port1
+ */
+extern int grspw_port_ctrl(void *d, int *port);
+/* Returns Number ports available in hardware */
+extern int grspw_port_count(void *d);
+/* Returns the current active port */
+extern int grspw_port_active(void *d);
+
+/*** DMA Interface ***/
+extern void *grspw_dma_open(void *d, int chan_no);
+extern int grspw_dma_close(void *c);
+
+extern int grspw_dma_start(void *c);
+extern void grspw_dma_stop(void *c);
+
+/* Schedule List of packets for transmission at some point in
+ * future.
+ *
+ * 1. Move transmitted packets to SENT List (SCHED->SENT)
+ * 2. Add the requested packets to the SEND List (USER->SEND)
+ * 3. Schedule as many packets as possible for transmission (SEND->SCHED)
+ *
+ * Call this function with pkts=NULL to just do step 1 and 3. This may be
+ * required in Polling-mode.
+ *
+ * The above steps 1 and 3 may be skipped by setting 'opts':
+ *  bit0 = 1: Skip Step 1.
+ *  bit1 = 1: Skip Step 3.
+ * Skipping both step 1 and 3 may be usefull when IRQ is enabled, then
+ * the work queue will be totaly responsible for handling descriptors.
+ *
+ * The fastest solution in retreiving sent TX packets and sending new frames
+ * is to call:
+ *  A. grspw_dma_tx_reclaim(opts=0)
+ *  B. grspw_dma_tx_send(opts=1)
+ *
+ * NOTE: the TXPKT_FLAG_TX flag must not be set.
+ *
+ * Return Code
+ *  -1   Error
+ *  0    Successfully added pkts to send/sched list
+ *  1    DMA stopped. No operation.
+ */
+extern int grspw_dma_tx_send(void *c, int opts, struct grspw_list *pkts, int count);
+
+/* Reclaim TX packet buffers that has previously been scheduled for transmission
+ * with grspw_dma_tx_send().
+ *
+ * 1. Move transmitted packets to SENT List (SCHED->SENT)
+ * 2. Move all SENT List to pkts list (SENT->USER)
+ * 3. Schedule as many packets as possible for transmission (SEND->SCHED)
+ *
+ * The above steps 1 may be skipped by setting 'opts':
+ *  bit0 = 1: Skip Step 1.
+ *  bit1 = 1: Skip Step 3.
+ *
+ * The fastest solution in retreiving sent TX packets and sending new frames
+ * is to call:
+ *  A. grspw_dma_tx_reclaim(opts=2) (Skip step 3)
+ *  B. grspw_dma_tx_send(opts=1) (Skip step 1)
+ *
+ * Return Code
+ *  -1   Error
+ *  0    Successful. pkts list filled with all packets from sent list
+ *  1    Same as 0, but indicates that DMA stopped
+ */
+extern int grspw_dma_tx_reclaim(void *c, int opts, struct grspw_list *pkts, int *count);
+
+/* Get current number of Packets in respective TX Queue. */
+extern void grspw_dma_tx_count(void *c, int *send, int *sched, int *sent, int *hw);
+
+#define GRSPW_OP_AND 0
+#define GRSPW_OP_OR 1
+/* Block until send_cnt or fewer packets are Queued in "Send and Scheduled" Q,
+ * op (AND or OR), sent_cnt or more packet "have been sent" (Sent Q) condition
+ * is met.
+ * If a link error occurs and the Stop on Link error is defined, this function
+ * will also return to caller.
+ * The timeout argument is used to return after timeout ticks, regardless of
+ * the other conditions. If timeout is zero, the function will wait forever
+ * until the condition is satisfied.
+ *
+ * NOTE: if IRQ of TX descriptors are not enabled conditions are never
+ *       checked, this may hang infinitely unless a timeout has been specified
+ *
+ * Return Code
+ *  -1   Error
+ *  0    Returing to caller because specified conditions are now fullfilled
+ *  1    DMA stopped
+ *  2    Timeout, conditions are not met
+ *  3    Another task is already waiting. Service is Busy.
+ */
+extern int grspw_dma_tx_wait(void *c, int send_cnt, int op, int sent_cnt, long timeout);
+
+/* Get received RX packet buffers that has previously been scheduled for 
+ * reception with grspw_dma_rx_prepare().
+ *
+ * 1. Move Scheduled packets to RECV List (SCHED->RECV)
+ * 2. Move all RECV packet to the callers list (RECV->USER)
+ * 3. Schedule as many free packet buffers as possible (READY->SCHED)
+ *
+ * The above steps 1 may be skipped by setting 'opts':
+ *  bit0 = 1: Skip Step 1.
+ *  bit1 = 1: Skip Step 3.
+ *
+ * The fastest solution in retreiving received RX packets and preparing new
+ * packet buffers for future receive, is to call:
+ *  A. grspw_dma_rx_recv(opts=2, &recvlist) (Skip step 3)
+ *  B. grspw_dma_rx_prepare(opts=1, &freelist) (Skip step 1)
+ *
+ * Return Code
+ *  -1   Error
+ *  0    Successfully filled pkts list with packets from recv list.
+ *  1    DMA stopped
+ */
+extern int grspw_dma_rx_recv(void *c, int opts, struct grspw_list *pkts, int *count);
+
+/* Add more RX packet buffers for future for reception. The received packets
+ * can later be read out with grspw_dma_rx_recv().
+ *
+ * 1. Move Received packets to RECV List (SCHED->RECV)
+ * 2. Add the "free/ready" packet buffers to the READY List (USER->READY)
+ * 3. Schedule as many packets as possible (READY->SCHED)
+ *
+ * The above steps 1 may be skipped by setting 'opts':
+ *  bit0 = 1: Skip Step 1.
+ *  bit1 = 1: Skip Step 3.
+ *
+ * The fastest solution in retreiving received RX packets and preparing new
+ * packet buffers for future receive, is to call:
+ *  A. grspw_dma_rx_recv(opts=2, &recvlist) (Skip step 3)
+ *  B. grspw_dma_rx_prepare(opts=1, &freelist) (Skip step 1)
+ *
+ * Return Code
+ *  -1   Error
+ *  0    Successfully added packet buffers from pkt list into the ready queue
+ *  1    DMA stopped
+ */
+extern int grspw_dma_rx_prepare(void *c, int opts, struct grspw_list *pkts, int count);
+
+/* Get current number of Packets in respective RX Queue. */
+extern void grspw_dma_rx_count(void *c, int *ready, int *sched, int *recv, int *hw);
+
+/* Block until recv_cnt or more packets are Queued in RECV Q, op (AND or OR), 
+ * ready_cnt or fewer packet buffers are available in the "READY and Scheduled" Q,
+ * condition is met.
+ * If a link error occurs and the Stop on Link error is defined, this function
+ * will also return to caller, however with an error.
+ * The timeout argument is used to return after timeout ticks, regardless of
+ * the other conditions. If timeout is zero, the function will wait forever
+ * until the condition is satisfied.
+ *
+ * NOTE: if IRQ of RX descriptors are not enabled conditions are never
+ *       checked, this may hang infinitely unless a timeout has been specified
+ *
+ * Return Code
+ *  -1   Error
+ *  0    Returing to caller because specified conditions are now fullfilled
+ *  1    DMA stopped
+ *  2    Timeout, conditions are not met
+ *  3    Another task is already waiting. Service is Busy.
+ */
+extern int grspw_dma_rx_wait(void *c, int recv_cnt, int op, int ready_cnt, long timeout);
+
+
+extern int grspw_dma_config(void *c, struct grspw_dma_config *cfg);
+extern void grspw_dma_config_read(void *c, struct grspw_dma_config *cfg);
+
+extern void grspw_dma_stats_read(void *c, struct grspw_dma_stats *sts);
+extern void grspw_dma_stats_clr(void *c);
+
+/*** GRSPW SpaceWire Packet List Handling Routines ***/
+
+static inline void grspw_list_clr(struct grspw_list *list)
+{
+        list->head = NULL;
+        list->tail = NULL;
+}
+
+static inline int grspw_list_is_empty(struct grspw_list *list)
+{
+        return (list->head == NULL);
+}
+
+/* Return Number of entries in list */
+static inline int grspw_list_cnt(struct grspw_list *list)
+{
+	struct grspw_pkt *lastpkt = NULL, *pkt = list->head;
+	int cnt = 0;
+	while ( pkt ) {
+		cnt++;
+		lastpkt = pkt;
+		pkt = pkt->next;
+	}
+	if ( lastpkt && (list->tail != lastpkt) )
+		return -1;
+	return cnt;
+}
+
+static inline void
+grspw_list_append(struct grspw_list *list, struct grspw_pkt *pkt)
+{
+	pkt->next = NULL;
+	if ( list->tail == NULL ) {
+		list->head = pkt;
+	} else {
+		list->tail->next = pkt;
+	}
+	list->tail = pkt;
+}
+
+static inline void 
+grspw_list_prepend(struct grspw_list *list, struct grspw_pkt *pkt)
+{
+	pkt->next = list->head;
+	if ( list->head == NULL ) {
+		list->tail = pkt;
+	}
+	list->head = pkt;
+}
+
+static inline void
+grspw_list_append_list(struct grspw_list *list, struct grspw_list *alist)
+{
+	alist->tail->next = NULL;
+	if ( list->tail == NULL ) {
+		list->head = alist->head;
+	} else {
+		list->tail->next = alist->head;
+	}
+	list->tail = alist->tail;
+}
+
+static inline void
+grspw_list_prepend_list(struct grspw_list *list, struct grspw_list *alist)
+{
+	if ( list->head == NULL ) {
+		list->tail = alist->tail;
+		alist->tail->next = NULL;
+	} else {
+		alist->tail->next = list->head;
+	}
+	list->head = alist->head;
+}
+
+/* Remove dlist (delete-list) from head of list */
+static inline void
+grspw_list_remove_head_list(struct grspw_list *list, struct grspw_list *dlist)
+{
+	list->head = dlist->tail->next;
+	if ( list->head == NULL ) {
+		list->tail = NULL;
+	}
+	dlist->tail->next = NULL;
+}
+
+/* Take A number of entries from head of list 'list' and put the entires
+ * to rlist (result list).
+ */
+static inline int
+grspw_list_take_head_list(struct grspw_list *list, struct grspw_list *rlist, int max)
+{
+	int cnt;
+	struct grspw_pkt *pkt, *last;
+
+	pkt = list->head;
+
+	if ( (max < 1) || (pkt == NULL) ) {
+		grspw_list_clr(rlist);
+		return 0;
+	}
+
+	cnt = 0;
+	rlist->head = pkt;
+	last = pkt;
+	while ((cnt < max) && pkt) {
+		last = pkt;
+		pkt = pkt->next;
+		cnt++;
+	}
+	rlist->tail = last;
+	grspw_list_remove_head_list(list, rlist);
+	return cnt;
+}
+#endif /*_KERNEL__*/
+
+#endif
diff -Nru a/drivers/grlib/include/linux/grlib/grspw_router.h b/drivers/grlib/include/linux/grlib/grspw_router.h
--- a/drivers/grlib/include/linux/grlib/grspw_router.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/grlib/include/linux/grlib/grspw_router.h	2022-01-29 23:16:14.137064547 +0100
@@ -0,0 +1,114 @@
+/*
+ * GRSPW Router APB Register char driver
+ *
+ * Copyright (c) 2016-2022 Cobham Gaisler AB
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston,
+ * MA 02110-1301 USA
+ */
+
+#ifndef __GRSPW_ROUTER_H__
+#define __GRSPW_ROUTER_H__
+
+/* Hardware Information */
+struct router_hw_info {
+	unsigned char nports_spw;
+	unsigned char nports_amba;
+	unsigned char nports_fifo;
+	char timers_avail;
+	char pnp_avail;
+	unsigned char ver_major;
+	unsigned char ver_minor;
+	unsigned char ver_patch;
+	unsigned char iid;
+};
+
+#define ROUTER_FLG_CFG		0x01
+#define ROUTER_FLG_IID		0x02
+#define ROUTER_FLG_IDIV		0x04
+#define ROUTER_FLG_TPRES	0x08
+#define ROUTER_FLG_TRLD		0x10
+#define ROUTER_FLG_ALL		0x1f	/* All Above Flags */
+
+struct router_config {
+	unsigned int flags; /* Determine what configuration should be updated */
+
+	/* Router Configuration Register */
+	unsigned int config;
+
+	/* Set Instance ID */
+	unsigned char iid;
+
+	/* SpaceWire Link Initialization Clock Divisor */
+	unsigned char idiv;
+
+	/* Timer Prescaler and Reload */
+	unsigned int timer_prescaler;
+	unsigned int timer_reload[31];
+};
+
+/* Logical routing table */
+struct router_routes {
+	unsigned int route[224];
+};
+
+/* Port Setup, see register definitions for "Port setup register" */
+struct router_ps {
+	unsigned int ps[31]; /* Port Setup for ports 1-31 */
+	unsigned int ps_logical[224]; /* Port setup for locgical addresses 32-255 */
+};
+
+/* Set/Get Port Control/Status */
+#define ROUTER_PORTFLG_SET_CTRL	0x01
+#define ROUTER_PORTFLG_GET_CTRL	0x02
+#define ROUTER_PORTFLG_SET_STS	0x04
+#define ROUTER_PORTFLG_GET_STS	0x08
+struct router_port {
+	unsigned int flag;
+	int port;
+	unsigned int ctrl;
+	unsigned int sts;
+};
+
+/* Get Hardware support/information available */
+#define GRSPWR_IOCTL_HWINFO	_IOW('a', 0x01, struct router_hw_info)
+
+/* Router Configuration */
+#define GRSPWR_IOCTL_CFG_SET	_IOR('a', 0x02, struct router_config)
+#define GRSPWR_IOCTL_CFG_GET	_IOW('a', 0x03, struct router_config)
+
+/* Routes */
+#define GRSPWR_IOCTL_ROUTES_SET	_IOR('a', 0x04, struct router_routes)
+#define GRSPWR_IOCTL_ROUTES_GET	_IOW('a', 0x05, struct router_routes)
+
+/* Port Setup */
+#define GRSPWR_IOCTL_PS_SET	_IOR('a', 0x06, struct router_ps)
+#define GRSPWR_IOCTL_PS_GET	_IOW('a', 0x07, struct router_ps)
+
+/* Set configuration write enable */
+#define GRSPWR_IOCTL_WE_SET	_IO('a', 0x08)
+
+/* Set/Get Port Control/Status */
+#define GRSPWR_IOCTL_PORT	_IOWR('a', 0x09, struct router_port)
+
+/* Set Router Configuration/Status Register */
+#define GRSPWR_IOCTL_CFGSTS_SET	_IO('a', 0x0a)
+/* Get Router Configuration/Status Register */
+#define GRSPWR_IOCTL_CFGSTS_GET	_IOWR('a', 0x0b, unsigned int *)
+
+/* Get Current Time-Code Register */
+#define GRSPWR_IOCTL_TC_GET	_IOWR('a', 0x0c, unsigned int *)
+
+#endif
diff -Nru a/drivers/grlib/include/linux/grlib/grspw_user.h b/drivers/grlib/include/linux/grlib/grspw_user.h
--- a/drivers/grlib/include/linux/grlib/grspw_user.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/grlib/include/linux/grlib/grspw_user.h	2022-01-29 23:16:14.137064547 +0100
@@ -0,0 +1,355 @@
+/*
+ * GRSPW DMA char driver,
+ * implemented by using the GRSPW kernel library
+ *
+ * Copyright (c) 2016-2022 Cobham Gaisler AB
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston,
+ * MA 02110-1301 USA
+ */
+
+#ifndef __GRSPWU_CHAR_H__
+#define __GRSPWU_CHAR_H__
+
+#ifndef GRSPW_PKT_FLAGS
+#define GRSPW_PKT_FLAGS
+/*** TX Packet flags ***/
+
+/* Enable IRQ generation */
+#define TXPKT_FLAG_IE 0x0040
+
+/* Enable Header CRC generation (if CRC is available in HW)
+ * Header CRC will be appended (one byte at end of header)
+ */
+#define TXPKT_FLAG_HCRC 0x0100
+
+/* Enable Data CRC generation (if CRC is available in HW)
+ * Data CRC will be appended (one byte at end of packet)
+ */
+#define TXPKT_FLAG_DCRC 0x0200
+
+/* Control how many bytes the beginning of the Header 
+ * the CRC should not be calculated for */
+#define TXPKT_FLAG_NOCRC_MASK 0x0000000f
+#define TXPKT_FLAG_NOCRC_LEN0 0x00000000
+#define TXPKT_FLAG_NOCRC_LEN1 0x00000001
+#define TXPKT_FLAG_NOCRC_LEN2 0x00000002
+#define TXPKT_FLAG_NOCRC_LEN3 0x00000003
+#define TXPKT_FLAG_NOCRC_LEN4 0x00000004
+#define TXPKT_FLAG_NOCRC_LEN5 0x00000005
+#define TXPKT_FLAG_NOCRC_LEN6 0x00000006
+#define TXPKT_FLAG_NOCRC_LEN7 0x00000007
+#define TXPKT_FLAG_NOCRC_LEN8 0x00000008
+#define TXPKT_FLAG_NOCRC_LEN9 0x00000009
+#define TXPKT_FLAG_NOCRC_LENa 0x0000000a
+#define TXPKT_FLAG_NOCRC_LENb 0x0000000b
+#define TXPKT_FLAG_NOCRC_LENc 0x0000000c
+#define TXPKT_FLAG_NOCRC_LENd 0x0000000d
+#define TXPKT_FLAG_NOCRC_LENe 0x0000000e
+#define TXPKT_FLAG_NOCRC_LENf 0x0000000f
+
+/* Marks if packet was transmitted or not */
+#define TXPKT_FLAG_TX 0x8000
+
+#define TXPKT_FLAG_INPUT_MASK (TXPKT_FLAG_NOCRC_MASK | TXPKT_FLAG_IE | \
+				TXPKT_FLAG_HCRC | TXPKT_FLAG_DCRC)
+
+/* Link Error */
+#define TXPKT_FLAG_LINKERR 0x4000
+
+#define TXPKT_FLAG_OUTPUT_MASK (TXPKT_FLAG_LINKERR)
+
+/*** RX Packet Flags ***/
+
+/* Enable IRQ generation */
+#define RXPKT_FLAG_IE 0x0010
+
+#define RXPKT_FLAG_INPUT_MASK (RXPKT_FLAG_IE)
+
+/* Packet was truncated */
+#define RXPKT_FLAG_TRUNK 0x0800
+/* Data CRC error (only valid if RMAP CRC is enabled) */
+#define RXPKT_FLAG_DCRC 0x0400
+/* Header CRC error (only valid if RMAP CRC is enabled) */
+#define RXPKT_FLAG_HCRC 0x0200
+/* Error in End-of-Packet */
+#define RXPKT_FLAG_EEOP 0x0100
+/* Marks if packet was recevied or not */
+#define RXPKT_FLAG_RX 0x8000
+
+#define RXPKT_FLAG_OUTPUT_MASK (RXPKT_FLAG_TRUNK | RXPKT_FLAG_DCRC | \
+				RXPKT_FLAG_HCRC | RXPKT_FLAG_EEOP)
+#endif
+
+/* GRSPW Write TX-Packet Entry (SEND PACKET) */
+struct grspw_wtxpkt {
+	int pkt_id;		/* Custom Packet ID */
+	unsigned short flags;	/* See TXPKT_FLAG* above */
+	unsigned char resv;	/* Reserved */
+	unsigned char hlen;	/* Header Length. Set to zero if none. */
+	unsigned int dlen;	/* Data Length. Set to zero if none. */
+	void *hdr;		/* Header Pointer (Address from MMAP Lib) */
+	void *data;		/* Data Pointer (Address from MMAP Lib) */
+} __attribute__((packed));
+
+/* GRSPW Read TX-Packet Entry (RECLAIM TX BUFFER) */
+struct grspw_rtxpkt {
+	int pkt_id;		/* Custom Packet ID */
+	unsigned short flags;	/* See TXPKT_FLAG* above */
+	unsigned char dma_chan;	/* DMA Channel 0..3 */
+	unsigned char resv1;	/* Reserved, must be zero */
+} __attribute__((packed));
+
+/* GRSPW Write RX-Packet Entry (PREPARE RX BUFFER) */
+struct grspw_wrxpkt {
+	int pkt_id;		/* Custom Packet ID */
+	unsigned short flags;	/* See RXPKT_FLAG* above */
+	unsigned short resv1;	/* Reserved, must be zero */
+	void *data;		/* Data Pointer (Address from MMAP Lib). The
+				 * buffer must have room for max-packet */
+} __attribute__((packed));
+
+/* GRSPW Read RX-Packet Entry (RECEIVE) */
+struct grspw_rrxpkt {
+	int pkt_id;		/* Custom Packet ID */
+	unsigned short flags;	/* See RXPKT_FLAG* above */
+	unsigned char dma_chan;	/* DMA Channel 0..3 */
+	unsigned char resv1;	/* Reserved, must be zero */
+	int dlen;		/* Data Length */
+	void *data;		/* Data Pointer (Address from MMAP Lib) */
+} __attribute__((packed));
+
+/* The Read() and Write() calls are used to transfer buffer paramters and
+ * pointers between the driver and user space application. Blow is a summary
+ * of the operations possible:
+ *
+ * read()
+ *  - RX: RECEIVE RX Packets                         (struct grspw_rrxpkt)
+ *  - TX: RECLAIM TX Packet Buffers                  (struct grspw_rtxpkt)
+ * write()
+ *  - RX: PREPARE driver with RX Packet buffers      (struct grspw_wrxpkt)
+ *  - TX: SEND TX Packets                            (struct grspw_wtxpkt)
+ *
+ * To select between the different actions that the read()/write() operation
+ * will take, the MSB of the buffer length is used an input paramters.
+ *   - Bit 28: RX (0) or TX (1) operation
+ *   - read(): Bit 26..24: DMA Channel MASK
+ *   - write(): Bit 25..24: DMA Channel select
+ *
+ * The data format of the read()/write() operations are described with the
+ * grspw_XYxpkt structures, see above.
+ *
+ * The input/output is limited to 65kbytes for read()/write() operation to
+ * the driver.
+ *
+ * The Data and Header pointers is be aquired from the Memory MAP Library
+ * that comes with this driver. The MMAP Lib ensures that memory is linear
+ * and capable of DMA.
+ */
+#define GRSPW_READ_RECLAIM_BIT 20
+#define GRSPW_READ_RECLAIM (1<<GRSPW_READ_RECLAIM_BIT)
+#define GRSPW_READ_CHANMSK_BIT 16
+#define GRSPW_READ_CHANMSK (0xf<<GRSPW_READ_CHANMSK_BIT)
+#define GRSPW_WRITE_SEND_BIT 20
+#define GRSPW_WRITE_SEND (1<<GRSPW_WRITE_SEND_BIT)
+#define GRSPW_WRITE_CHAN_BIT 16
+#define GRSPW_WRITE_CHAN (0x3<<GRSPW_WRITE_CHAN_BIT)
+
+/* Get Hardware support available */
+#define GRSPW_IOCTL_HWSUP	_IOW('a', 0x01, struct grspw_hw_sup)
+
+/* Number of Packet Buffers that the driver can hold simultaneously. THe Packet
+ * buffers are shared between all DMA Channels.
+ * Defaults to 1000 packets for RX and 1000 for TX per DMA channel.
+ */
+struct grspw_bufcfg {
+	int rx_pkts;
+	int tx_pkts;
+	int maplib_pool_idx; /* MAPLIB Pool index used by SPW device */
+};
+#define GRSPW_IOCTL_BUFCFG	_IOR('a', 0x02, struct grspw_bufcfg)
+
+struct grspw_config {
+	struct grspw_addr_config adrcfg;
+	unsigned int rmap_cfg;
+	unsigned char rmap_dstkey;
+	unsigned int tc_cfg;
+	/* Mask of which DMA Channels to enable. Most often only
+	 * one DMA Channel is available, then set this to 1.
+	 *
+	 * By enabling a DMA Channel, the DMA Channels will be
+	 * able to receive SpaceWire packets after SpaceWire buffers 
+	 * has been prepared for it, and transmitt SpaceWire packets
+	 * when the user request transmission of a SpaceWire packet.
+	 */
+	unsigned int enable_chan_mask;
+	/* Per Channel Configuration */
+	struct grspw_dma_config chan[4];
+};
+#define GRSPW_IOCTL_CONFIG_SET	_IOR('a', 0x03, struct grspw_config)
+#define GRSPW_IOCTL_CONFIG_READ	_IOW('a', 0x04, struct grspw_config)
+
+/* Statisics and diagnostics gathered by driver */
+struct grspw_stats {
+	/* Statistics for the Core */
+	struct grspw_core_stats stats;
+
+	/* Per DMA channel statistics */
+	struct grspw_dma_stats chan[4];
+};
+#define GRSPW_IOCTL_STATS_READ	_IOW('a', 0x10, struct grspw_stats)
+/* Clear/Reset statistics */
+#define GRSPW_IOCTL_STATS_CLR	_IO('a', 0x11)
+
+/* Set link config, argument (int) is a bit mask of options, see LINKOPTS_* options */
+struct grspw_link_ctrl {
+	int ctrl;			/* Configuration */
+	unsigned char clkdiv_start;	/* Clock Division during startup */
+	unsigned char clkdiv_run;	/* Clock Division in run-state */
+	int stscfg;			/* Link status control LINKSTS_* */
+};
+#define GRSPW_IOCTL_LINKCTRL	_IOR('a', 0x20, struct grspw_link_ctrl)
+
+/* Set port config, argument (int) is 0=Port0, 1=Port1, else let hardware select
+ * between Port0 and Port1 */
+#define GRSPW_IOCTL_PORTCTRL	_IO('a', 0x21)
+
+/* Current state of configuration and link */
+struct grspw_link_state {
+	int link_ctrl;			/* Link Configuration */
+	unsigned char clkdiv_start;	/* Clock Division during startup (not 
+					 * in state run) */
+	unsigned char clkdiv_run;	/* Clock Division in run-state */
+	spw_link_state_t link_state;	/* State (Error-Reset, Error-wait...) */
+	int port_cfg;			/* Port Configuration */
+	int port_active;		/* Currently active port */
+};
+#define GRSPW_IOCTL_LINKSTATE	_IOW('a', 0x22, struct grspw_link_state)
+
+/* Generate Write TCTRL|TIMECNT and/or generate a Tick-In (sends a timecode)
+ *
+ * Write TCTRL|TIMECNT if bit 8 is 1:
+ *   TCTRL   = bits 7 and 6
+ *   TIMECNT = bits 5 to 0
+ * Generate Tick-In if bit 9 is 1.
+ *
+ * The Tick-In is generated after writing the TCTRL|TIMECNT values to register.
+ *
+ * Argument is a int.
+ */
+#define GRSPW_TC_SEND_OPTION_TCTRL	0x0ff
+#define GRSPW_TC_SEND_OPTION_SET_TCTRL	0x100
+#define GRSPW_TC_SEND_OPTION_TCTX	0x200
+#define GRSPW_IOCTL_TC_SEND	_IO('a', 0x23)
+
+/* Read current value of TCTRL|TIMECNT:
+ *
+ *    [TIMECODE WAS RECEIVED SINCE LAST READ OUT = bit 8] NOT IMPLEMENTED
+ *   TCTRL   = bits 7 and 6
+ *   TIMECNT = bits 5 to 0
+ */
+#define GRSPW_IOCTL_TC_READ	_IOW('a', 0x24, int)
+
+/* Number of Packets in the Packet Queues of the driver */
+struct grspw_qpktcnt_chan {
+	/* RX */
+	int rx_ready;
+	int rx_sched;
+	int rx_recv;
+	int rx_hw;
+	/* TX */
+	int tx_send;
+	int tx_sched;
+	int tx_sent;
+	int tx_hw;
+};
+struct grspw_qpktcnt {
+	/* DMA Channels that does not exists are not written, thus data is
+	 * undefined.
+	 */
+	struct grspw_qpktcnt_chan chan[4];
+};
+/* Read current number of packets in all TX/RX queues of all enabled DMA
+ * channels.
+ */
+#define GRSPW_IOCTL_QPKTCNT	_IOW('a', 0x25, struct grspw_qpktcnt)
+
+/*
+ * Read current value of GRSPW status register.
+ */
+#define GRSPW_IOCTL_STATUS_READ	_IOW('a', 0x26, unsigned int)
+
+/*
+ * Clear one/more bit(s) in the GRSPW status register. It is determined by
+ * the input bit-mask which bits are cleared.
+ */
+#define GRSPW_IOCTL_STATUS_CLR	_IOR('a', 0x27, unsigned int)
+
+/* START DMA operation, after this the followin functions will be available for
+ * every enabled DMA Channel:
+ *  - RX: PREPARE RX BUFFER
+ *  - RX: RECEIVE
+ *  - TX: SEND
+ *  - TX: RECLAIM BUFFER
+ *
+ * After the link has started, the BUFCFG and CONFIG_SET ioctl commands are
+ * disabled, calling them will result in an EBUSY error.
+ */
+#define GRSPW_IOCTL_START	_IO('a', 0x30)
+
+/* STOP DMA operation, after this only the folling operation will be available
+ * for every enabled DMA Channel:
+ *  - RX: RECEIVE (will get PREPARED buffers and RECEIVED Packets)
+ *  - TX: RECLAIM BUFFER (will get SENT packets and unsent Packets)
+ *
+ * Future calls to SEND and PREPARE will result in an error.
+ */
+#define GRSPW_IOCTL_STOP	_IO('a', 0x31)
+
+/* Specifices a RX wait conditions by RX packet queue size */
+struct grspw_rx_wait_chan {
+	int chan;		/* which DMA channel to wait for */
+	int reserved;		/* reserved, set to zero */
+	long timeout_ms;	/* number of milliseconds timeout (converted
+				 * to jiffies). 0 means no wait forever.
+				 */
+	int recv_cnt;
+	int op;
+	int ready_cnt;
+};
+/* Wait for TX queue packet counters to be fulfilled. This call may block until
+ * workqueue unblocks the caller. See grspw_dma_tx_wait() documentation for
+ * details.
+ */
+#define GRSPW_IOCTL_RX_WAIT	_IOR('a', 0x40, struct grspw_rx_wait_chan)
+
+/* Specifices a TX wait conditions by TX packet queue size */
+struct grspw_tx_wait_chan {
+	int chan;		/* which DMA channel to wait for */
+	int reserved;		/* reserved, set to zero */
+	long timeout_ms;	/* number of milliseconds timeout (converted
+				 * to jiffies). 0 means no wait forever.
+				 */
+	int send_cnt;
+	int op;
+	int sent_cnt;
+};
+/* Wait for TX queue packet counters to be fulfilled. This call may block until
+ * workqueue unblocks the caller. See grspw_dma_tx_wait() documentation for
+ * details.
+ */
+#define GRSPW_IOCTL_TX_WAIT	_IOR('a', 0x41, struct grspw_rx_wait_chan)
+
+#endif
diff -Nru a/drivers/grlib/include/linux/grlib/maplib.h b/drivers/grlib/include/linux/grlib/maplib.h
--- a/drivers/grlib/include/linux/grlib/maplib.h	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/grlib/include/linux/grlib/maplib.h	2022-01-29 23:16:14.137064547 +0100
@@ -0,0 +1,100 @@
+/*
+ * Library that provides drivers with a way to have common
+ * Memory MAPed buffers. For example SpW0 buffers can be used
+ * zero-copy to SpW1.
+ *
+ * Copyright (c) 2016-2022 Cobham Gaisler AB
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston,
+ * MA 02110-1301 USA
+ */
+
+#ifndef __MMAPLIB_H__
+#define __MMAPLIB_H__
+
+/*** USER IOCTL INTERFACE ***/
+
+/* MMAP Lib setup information. Actual memory taken will be at least
+ * (blk_size * blk_cnt).
+ */
+struct maplib_setup {
+	unsigned int	blk_cnt;	/* Number of Blocks */
+	unsigned int	blk_size;	/* Size of each block */
+};
+
+/* Block Buffer MMAP information */
+struct maplib_mmap_info {
+	unsigned int	buf_offset;	/* Offset into device memory */
+	int		buf_length;	/* Total Length */
+	int		buf_blk_size;	/* Buffer Block Size */
+};
+
+/* Configure Memory MAP Library, and allocate need memory,
+ * all previous (if any) memory mapped pages must be unmapped
+ * otherwise and error will occur.
+ */
+#define MAPLIB_IOCTL_SETUP	_IOR('a', 0x01, struct maplib_setup)
+/* Get Current MMAP Info from Driver, this tells the user how
+ * to memory map the memory into user space.
+ */
+#define MAPLIB_IOCTL_MMAPINFO	_IOW('a', 0x02, struct maplib_mmap_info)
+
+
+/*** KERNEL INTRFACE ***/
+#ifdef __KERNEL__
+
+/* Driver that uses the library */
+struct maplib_drv {
+	struct maplib_drv *next;		/* Used internally */
+	char *name;
+	void (*unmap)(int idx, void *priv);	/* Called by library when memory is unmapped */
+	void *priv;
+};
+
+/* Register Driver as active user of Memory mapped memory
+ *
+ * This will fail if not all memory blocks has been mapped to user memory.
+ * 
+ * The custom unmap() function is called if the user unmap memory, if this
+ * call is made the user did not close the drivers in the correct order, but to
+ * prevent a bug this must exist. After this the driver may not perform
+ * any memory access to the memory range.
+ */
+extern int maplib_drv_reg(int idx, struct maplib_drv *drv);
+
+/* Unregister Driver */
+extern void maplib_drv_unreg(int idx, struct maplib_drv *drv);
+
+/* Basic check for user-drivers that MAP device is valid */
+extern int maplib_drv_chkvalid(int idx);
+
+/* Translate previously MMAPed User Space Virtual Address into Kernel Address
+ * Return -1 if invalid address
+ *
+ * Check that the complete user/kernel address is mapped and linear
+ * into kernel space (and hence user space). Set LEN=0 to avoid check.
+ */
+extern int maplib_lookup_kern(int idx, void *usr, void **kern, phys_addr_t *hw, int len);
+
+/* Translate previously MMAPed Kernel Virtual Address into UserSpace Address
+ * Return -1 if invalid address
+ *
+ * Check that the complete user/kernel address is mapped and linear
+ * into kernel space (and hence user space). Set LEN=0 to avoid check.
+ */
+extern int maplib_lookup_usr(int idx, void *kern, phys_addr_t hw, void **usr, int len);
+#endif
+
+#endif
diff -Nru a/drivers/grlib/Kbuild b/drivers/grlib/Kbuild
--- a/drivers/grlib/Kbuild	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/grlib/Kbuild	2022-01-29 23:16:14.137064547 +0100
@@ -0,0 +1,2 @@
+obj-y   += spw/
+obj-y   += misc/
diff -Nru a/drivers/grlib/Kconfig b/drivers/grlib/Kconfig
--- a/drivers/grlib/Kconfig	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/grlib/Kconfig	2022-01-29 23:43:29.531193779 +0100
@@ -0,0 +1,9 @@
+menuconfig GRLIB_DRVPKG
+	tristate "GRLIB Driver support"
+	depends on SPARC_LEON || RISCV
+	default n
+
+if GRLIB_DRVPKG
+source "drivers/grlib/misc/Kconfig"
+source "drivers/grlib/spw/Kconfig"
+endif
diff -Nru a/drivers/grlib/Makefile b/drivers/grlib/Makefile
--- a/drivers/grlib/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/grlib/Makefile	2022-01-29 23:16:14.137064547 +0100
@@ -0,0 +1,28 @@
+# Change this to reflect your Linux kernel path
+#KERNELDIR=../linux-5.10
+KERNELDIR=../..
+
+# Determine which modules to build
+MOD_SEL_STR = \
+	CONFIG_GRLIB_GRSPW=m \
+	CONFIG_GRLIB_GRSPWU=m \
+	CONFIG_GRLIB_GRSPWROUTER=m \
+	CONFIG_GRLIB_MAPLIB=m
+
+leon: export CROSS_COMPILE=sparc-linux-
+leon: export ARCH=sparc
+leon: all
+
+noel32: export CROSS_COMPILE=riscv32-linux-
+noel32: export ARCH=riscv
+noel32: all
+
+noel64: export CROSS_COMPILE=riscv64-linux-
+noel64: export ARCH=riscv
+noel64: all
+
+all:
+	make -C $(KERNELDIR) M=$(PWD) $(MOD_SEL_STR) ROOT_PATH=$(PWD)/../../ modules
+
+clean:
+	make ARCH=sparc CROSS_COMPILE=sparc-linux- -C $(KERNELDIR) M=$(PWD) clean
diff -Nru a/drivers/grlib/misc/Kbuild b/drivers/grlib/misc/Kbuild
--- a/drivers/grlib/misc/Kbuild	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/grlib/misc/Kbuild	2022-01-29 23:16:14.137064547 +0100
@@ -0,0 +1,7 @@
+#
+# Kbuild for the kernel MAPLIB device driver.
+#
+
+EXTRA_CFLAGS   += -I$(ROOT_PATH)drivers/grlib/include
+
+obj-$(CONFIG_GRLIB_MAPLIB) += maplib.o
diff -Nru a/drivers/grlib/misc/Kconfig b/drivers/grlib/misc/Kconfig
--- a/drivers/grlib/misc/Kconfig	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/grlib/misc/Kconfig	2022-01-29 23:16:14.137064547 +0100
@@ -0,0 +1,15 @@
+menu "Misc GRLIB drivers Help Functions"
+
+config GRLIB_MAPLIB
+        tristate "GRLIB Memory Mapping Library driver"
+        default y
+	depends on SPARC_LEON || RISCV
+        help
+          Memory Library driver used by GRLIB drivers to memory
+          MAP linear low memory to user space. The memory can be used
+          to implement zero-copy between kernel space and user space.
+          To work efficiently it requires cache snooping in hardware.
+
+          Currently the GRSPW driver uses this driver.
+
+endmenu # Misc help functions
diff -Nru a/drivers/grlib/misc/maplib.c b/drivers/grlib/misc/maplib.c
--- a/drivers/grlib/misc/maplib.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/grlib/misc/maplib.c	2022-01-29 23:16:14.137064547 +0100
@@ -0,0 +1,658 @@
+/*
+ * Device Memory MAP Library help functions for GRLIB Device Drivers
+ *
+ * Copyright (c) 2016-2022 Cobham Gaisler AB
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston,
+ * MA 02110-1301 USA
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/fs.h>
+#include <linux/cdev.h>
+#include <asm/uaccess.h>
+#include <linux/mm.h>
+#include <linux/mman.h>
+#include <linux/slab.h>
+#include <linux/io.h>
+#include <linux/grlib/maplib.h>
+#include <linux/grlib/devno.h>
+
+MODULE_LICENSE ("GPL");
+
+/*#define MAPLIB_DEBUG*/
+
+#ifdef MAPLIB_DEBUG
+	#define MAPLIB_DBG(doCode) doCode
+#else
+	#define MAPLIB_DBG(doCode)
+#endif
+
+/* this interface was not ported because it was unused going from 3.10 to 5.10 kernel */
+#define MAPLIB_KERN_TO_USR_LOOKUP 0
+
+#define MAPLIB_MAX 1
+
+#if (MAPLIB_MAX > MAPLIB_DEVCNT)
+#error MAPLIB: not enough CHAR device nodes
+#endif
+
+#define BLK_SIZE   0x20000
+#define BLK_SHIFT  17
+
+struct maplib_blk {
+	unsigned int length;			/* Length of Block, 0x20000 unless last*/
+	void *start;				/* Kernel Address */
+};
+
+struct maplib_priv {
+	/*** HARDWARE PARAMS ***/
+	char devname[16];
+	int index;
+
+	/*** SOFWTARE PARAMS ***/
+	struct cdev cdev;		/* Char device */
+	int open;			/* If Device is Opened by user */
+
+	/*** MEMORY MAP TO USERSPACE HANDLING ***/
+
+	/* Buffer Blocks, usrblk_per_blk Buffers per block */
+	struct maplib_blk *blks;	/* Array of TM Frame Buffer Blocks */
+	int blk_cnt;			/* Number of Blocks */
+	int usrblk_per_blk;
+	int usrblk_size;
+	int blk_tot_size;		/* Total Size of all blocks */
+
+	unsigned long ustart;
+	unsigned long uend;
+	struct vm_area_struct *user_vma;
+
+#if MAPLIB_KERN_TO_USR_LOOKUP
+	/* Kernel Address to Block number Translation Table.
+	 * MSB 8-bit.
+	 */
+	short *kern_tab[256];
+#endif
+
+	/* All drivers attached */
+	struct maplib_drv *drvs;
+};
+
+struct maplib_priv *maplib_privs = NULL;
+int maplib_count = 0;
+
+static int maplib_open(struct inode *inode, struct file *file);
+static int maplib_release(struct inode *inode, struct file *file);
+static long maplib_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);
+static int maplib_mmap(struct file *filp, struct vm_area_struct *vma);
+static void maplib_vma_close(struct vm_area_struct *area);
+
+struct file_operations maplib_fops = {
+	.owner          = THIS_MODULE,
+	.open           = maplib_open,
+	.release        = maplib_release,
+	.unlocked_ioctl = maplib_ioctl,
+	.mmap           = maplib_mmap,
+};
+
+struct vm_operations_struct maplib_vops = {
+	.close = maplib_vma_close,
+};
+
+/* We go from all memory being mapped into not all memory being
+ * mapped. This means that we must signal to all drivers to
+ * stop using the memory.
+ */
+void maplib_drv_unreg_all(struct maplib_priv *priv)
+{
+	struct maplib_drv *drv = priv->drvs;
+	while ( drv ) {
+		drv->unmap(priv->index, drv->priv);
+		drv = drv->next;
+	}
+	priv->drvs = NULL;
+}
+
+/* Register Driver as active user of Memory mapped memory */
+int maplib_drv_reg(int idx, struct maplib_drv *drv)
+{
+	struct maplib_priv *priv = &maplib_privs[idx];
+
+	if ( priv->user_vma == NULL )
+		return -1;
+
+	drv->next = priv->drvs;
+	priv->drvs = drv;
+	return 0;
+}
+EXPORT_SYMBOL(maplib_drv_reg);
+
+/* Unregister Driver */
+void maplib_drv_unreg(int idx, struct maplib_drv *drv)
+{
+	struct maplib_priv *priv = &maplib_privs[idx];
+	struct maplib_drv **prev, *d = priv->drvs;
+
+	prev = &priv->drvs;
+	while ( (d != drv) && d ) {
+		prev = &d->next;
+		d = d->next;
+	}
+	if ( d == drv ) {
+		*prev = d->next;
+	}
+}
+EXPORT_SYMBOL(maplib_drv_unreg);
+
+/* Unregister Driver */
+int maplib_drv_chkvalid(int idx)
+{
+	if (idx < 0 || idx >= maplib_count)
+		return -1;
+	if (maplib_privs == NULL)
+		return -1;
+	return 0; /* MAP index exists */
+}
+EXPORT_SYMBOL(maplib_drv_chkvalid);
+
+#if 0
+void maplib_tab_print(struct maplib_priv *priv, short **usrtab)
+{
+	struct maplib_blk *blk;
+	short *blktab;
+	int i,j;
+
+	for(i=0; i<256; i++){
+		if ( usrtab[i] ) {
+			blktab = usrtab[i];
+			for (j=0; j<128; j++) {
+				if ( blktab[j] != -1 ) {
+					blk = &priv->blks[blktab[j]];
+					printk(KERN_DEBUG "  %02x:%02x:%d: %x-%x\n",
+						i, j, blktab[j],
+						blk->user_vma->vm_start,
+						blk->user_vma->vm_end);
+				}
+			}
+		}
+	}
+}
+#endif
+
+/* Translate previously MMAPed User Space Virtual Address into Kernel Address
+ * Return -1 if invalid address
+ */
+int maplib_lookup_kern(int idx, void *usr, void **kern, phys_addr_t *hw, int len)
+{
+	struct maplib_priv *priv = &maplib_privs[idx];
+	unsigned long uadr = (unsigned long)usr;
+	int blkno, blkno_end;
+	struct maplib_blk *blk;
+	unsigned long ofs;
+
+	if ( (uadr < priv->ustart) || (uadr >= priv->uend) )
+		return -1;
+
+	ofs = uadr - priv->ustart;
+	blkno = ofs >> BLK_SHIFT;
+	blkno_end = (ofs+len-1) >> BLK_SHIFT;
+	if ( blkno != blkno_end ) /* Check that block boundary is not passed */
+		return -1;
+	ofs = ofs & (BLK_SIZE-1);
+	blk = &priv->blks[blkno];
+	if ( kern )
+		*kern = (void *)((unsigned long)blk->start + ofs);
+	if ( hw )
+		*hw = virt_to_phys(blk->start + ofs);
+
+	return 0;
+}
+EXPORT_SYMBOL(maplib_lookup_kern);
+
+#if MAPLIB_KERN_TO_USR_LOOKUP
+/* Translate previously MMAPed Kernel Virtual Address into UserSpace Address
+ * Return -1 if invalid address
+ */
+int maplib_lookup_usr(int idx, void *kern, phys_addr_t hw, void **usr, int len)
+{
+	int msb = (unsigned int)kern >> 24;
+	int blkidx = ((unsigned int)kern & 0x00FFFFFF) >> BLK_SHIFT;
+	int blkno;
+	struct maplib_blk *blk;
+	struct maplib_priv *priv = &maplib_privs[idx];
+	void *usrbase;
+
+	if ( hw )
+		kern = phys_to_virt(hw);
+
+	if ( priv->kern_tab[msb] && (priv->kern_tab[msb][blkidx] != -1) ) {
+		blkno = priv->kern_tab[msb][blkidx];
+		blk = &priv->blks[blkno];
+		usrbase = (void *)(priv->ustart + (blkno<<BLK_SHIFT));
+		*usr = (void *)((unsigned int)usrbase +
+			((unsigned int)kern & (BLK_SIZE-1)));
+		/* Check Range */
+		if ( ((unsigned int)*usr + len) > ((unsigned int)usrbase + blk->length) )
+			return -1;
+		return 0;
+	}
+	return -1;
+}
+EXPORT_SYMBOL(maplib_lookup_usr);
+
+/* Build KernAdr->BlkNo Translation tree */
+static void maplib_tabs_build(struct maplib_priv *priv)
+{
+	struct maplib_blk *blk;
+	unsigned int kern;
+	short **blktab;
+	int i, j;
+
+	for (i=0; i<priv->blk_cnt; i++) {
+		blk = &priv->blks[i];
+
+		/* Kernel VA Table entry */
+		kern = (unsigned int)blk->start;
+		blktab = &priv->kern_tab[kern >> 24];
+
+		if ( *blktab == NULL ) {
+			*blktab = kmalloc(128 * sizeof(unsigned short), GFP_KERNEL);
+			for(j=0; j<128; j++)
+				(*blktab)[j] = -1;
+		}
+		(*blktab)[(kern & 0x00ffffff) >> BLK_SHIFT] = i;
+	}
+}
+
+static void maplib_tabs_free(struct maplib_priv *priv)
+{
+	int i;
+
+	for (i=0; i<256; i++) {
+		if ( priv->kern_tab[i] ) {
+			kfree(priv->kern_tab[i]);
+			priv->kern_tab[i] = NULL;
+		}
+	}
+}
+#endif
+
+static void maplib_free_blks(struct maplib_priv *priv)
+{
+	struct maplib_blk *blk;
+	int i;
+
+	if ( priv->blks ) {
+		blk = &priv->blks[0];
+		for (i=0; i<priv->blk_cnt; i++) {
+			if ( blk[i].start ) {
+				free_pages((unsigned long)blk[i].start,
+					get_order(blk[i].length));
+				blk[i].start = NULL;
+			}
+		}
+		kfree(priv->blks);
+	}
+	priv->blks = NULL;
+	priv->blk_cnt = 0;
+}
+
+static int maplib_open(struct inode *inode, struct file *filp)
+{
+	struct maplib_priv *priv;
+
+	priv = container_of(inode->i_cdev, struct maplib_priv, cdev);
+
+	if (priv == NULL) {
+		printk (KERN_WARNING "MAPLIB: device doesnt exist\n");
+		return -ENODEV;
+	}
+
+	if (priv->open != 0) {
+		printk (KERN_WARNING "MAPLIB: device %d already opened\n", priv->index);
+		return -EBUSY;
+	}
+
+	/* Take Device */
+	priv->open = 1;
+	filp->private_data = priv;
+
+	return 0;
+}
+
+static int maplib_release (struct inode *inode, struct file *file)
+{
+	struct maplib_priv *priv;
+
+	priv = container_of(inode->i_cdev, struct maplib_priv, cdev);
+
+	/* Free buffer memory */
+	maplib_free_blks(priv);
+
+	priv->open = 0;
+
+	return 0;
+}
+
+static int maplib_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	struct maplib_priv *priv;
+	int i, len, result;
+	struct maplib_blk *blk;
+	unsigned long ustart;
+
+	priv = (struct maplib_priv *)filp->private_data;
+
+#ifdef MAPLIB_DEBUG
+	printk(KERN_DEBUG "MAPLIB: MMAP Request\n");
+	printk(KERN_DEBUG "        VM_START:         0x%x\n", (unsigned int)vma->vm_start);
+	printk(KERN_DEBUG "        VM_END:           0x%x\n", (unsigned int)vma->vm_end);
+	printk(KERN_DEBUG "        VM_pgoff:         0x%x\n", (unsigned int)vma->vm_pgoff);
+	printk(KERN_DEBUG "        VM_private_data:  %p\n", vma->vm_private_data);
+	printk(KERN_DEBUG "        VM_page_prot:     0x%x (%x,%x,%x)\n",
+		(unsigned int)vma->vm_page_prot, PROT_READ, PROT_WRITE, PROT_EXEC);
+	printk(KERN_DEBUG "        VM_flags:         0x%x\n", (unsigned int)vma->vm_flags);
+	printk(KERN_DEBUG "        VM_ops:           0x%x\n", (unsigned int)vma->vm_ops);
+#endif
+	vma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;
+	vma->vm_private_data = priv;
+	vma->vm_ops = &maplib_vops;
+
+	/* First 1Mb addresses are reserved of something else */
+	if (vma->vm_pgoff < (0x100000 >> PAGE_SHIFT)) {
+		return -EINVAL;
+	}
+
+	/*** 1Mb - CFG_MAX Mb is Buffer Blocks ***/
+
+	if (  (vma->vm_end - vma->vm_start) != priv->blk_tot_size ) {
+		printk(KERN_DEBUG "MAPLIB: map size %x, required %x\n",
+			(unsigned int)(vma->vm_end - vma->vm_start),
+			(unsigned int)priv->blk_tot_size);
+		return -EINVAL;
+	}
+
+	/* MAP All blocks. */
+	ustart = vma->vm_start;
+	for (i=0; i<priv->blk_cnt; i++) {
+		/* Do the Mapping */
+		len = BLK_SIZE;
+		blk = &priv->blks[i];
+		if ( i == (priv->blk_cnt-1) )
+			len = priv->blks[i-1].length;
+		result = remap_pfn_range(vma, ustart, __pa(blk->start) >> PAGE_SHIFT,
+			len, vma->vm_page_prot);
+		if ( result ) {
+			printk(KERN_INFO "MAPLIB: Block mapping failed: %d, %p\n",
+				result, blk->start);
+			return -EAGAIN;
+		}
+#ifdef MAPLIB_DEBUG
+		printk(KERN_DEBUG "MAPLIB: MAPPED 0x%x-0x%x to 0x%x-0x%x\n",
+			(unsigned int)ustart, (unsigned int)(ustart+len),
+			(unsigned int)__pa(blk->start), (unsigned int)__pa(blk->start)+len);
+#endif
+		/* Next USpace Address */
+		ustart += len;
+	}
+	priv->ustart = vma->vm_start;
+	priv->uend = vma->vm_end;
+	/* Remember the User VMA that we have mapped */
+	priv->user_vma = vma;
+
+#if MAPLIB_KERN_TO_USR_LOOKUP
+	/* Now that we know the User-address and Kernel addresses,
+	 * the Kernel->UserAdress Table can be built.
+	 */
+	maplib_tabs_build(priv);
+#endif
+
+#ifdef MAPLIB_DEBUG
+	printk(KERN_DEBUG "MAPLIB: suceeded in mapping memory\n");
+#endif
+
+	return 0;
+}
+
+static void maplib_vma_close(struct vm_area_struct *area)
+{
+	struct maplib_priv *priv = area->vm_private_data;
+
+	/* First block is beeing unmapped, lets inform
+	 * all active drivers that the memory is not
+	 * longer available. The need to stop using the
+	 * memory to avoid data overwrite.
+	 */
+#ifdef MAPLIB_DEBUG
+	printk(KERN_DEBUG "MAPLIB: unmapping, telling drivers\n");
+#endif
+	maplib_drv_unreg_all(priv);
+#if MAPLIB_KERN_TO_USR_LOOKUP
+	maplib_tabs_free(priv);
+#endif
+
+	/* Mark Block as not mapped */
+	priv->user_vma = NULL;
+}
+
+static int maplib_buf_setup(struct maplib_priv *priv, struct maplib_setup *setup)
+{
+	int i;
+
+	if ( priv->blks ) {
+		/* All Buffers must be unmapped from previous configurations.
+		 */
+		if ( priv->user_vma ) {
+			printk(KERN_DEBUG "MAPLIB: setup: not all unmapped\n");
+			return -EINVAL;
+		}
+
+		/* All unmapped since last time. We free old buffers. */
+		maplib_free_blks(priv);
+	}
+
+	/* Check if request was to free buffers only */
+	if ( (setup->blk_cnt < 1) || (setup->blk_size < 1) ||
+	     (setup->blk_size > BLK_SIZE))
+		return 0;
+
+	/*** Setup Buffers ***/
+
+	/* Calculate number of MMAP-Blocks required to fit all 
+	 * user's blocks. A MMAP block is limited to a specific
+	 * size limited by the arch port. 128Kb seems standard.
+	 */
+	priv->usrblk_per_blk = BLK_SIZE / setup->blk_size;
+	priv->blk_cnt = (setup->blk_cnt + priv->usrblk_per_blk - 1) /
+				priv->usrblk_per_blk;
+	priv->usrblk_size = setup->blk_size;
+
+	/* Allocate memory for Block structures */
+	priv->blks = (struct maplib_blk *)kmalloc(
+			priv->blk_cnt *	sizeof(struct maplib_blk),
+			GFP_KERNEL);
+	memset(priv->blks, 0, priv->blk_cnt * sizeof(struct maplib_blk));
+
+	/* Init All block structures */
+	for (i=0; i<priv->blk_cnt; i++) {
+		if ( i == (priv->blk_cnt-1) ) {
+			/* Last Block */
+			priv->blks[i].length = (priv->blk_cnt - priv->usrblk_per_blk*i);
+		} else {
+			priv->blks[i].length = priv->usrblk_per_blk;
+		}
+		priv->blks[i].length = PAGE_ALIGN(priv->blks[i].length * priv->usrblk_size);
+
+#ifdef MAPLIB_DEBUG
+		printk(KERN_DEBUG "MAPLIB: allocating Buffer block: %d, %d (%x)\n",
+				i,
+				priv->blks[i].length,
+				(unsigned int)PAGE_ALIGN(priv->blks[i].length));
+#endif
+		/* Allocate Block Buffer memory. Must be linear */
+		priv->blks[i].start = (void *)__get_free_pages(GFP_KERNEL,
+						get_order(priv->blks[i].length));
+		if ( priv->blks[i].start == NULL ) {
+			/* Failed to allocate memory */
+			printk(KERN_INFO "MAPLIB: Failed alloc Buffer blk: %d\n", i);
+			maplib_free_blks(priv);
+			return -ENOMEM;
+		}
+	}
+
+	priv->user_vma = NULL;
+	priv->blk_tot_size = ((priv->blk_cnt-1) * priv->blks[i-1].length) +
+				priv->blks[priv->blk_cnt-1].length;
+
+	return 0;
+}
+
+static int maplib_mmapinfo_get(struct maplib_priv *priv, struct maplib_mmap_info *info)
+{
+	if ( priv->blks == NULL ) {
+		printk(KERN_INFO "MAPLIB: MMAP Info not available until configured\n");
+		return -EINVAL;
+	}
+
+	/* Get Frame Structure MMAP Info */
+	info->buf_offset = 0x00100000;
+	info->buf_length = ((priv->blk_cnt-1) * BLK_SIZE) +
+				priv->blks[priv->blk_cnt-1].length;
+	info->buf_blk_size = BLK_SIZE;
+
+	return 0;
+}
+
+static long maplib_ioctl(
+	struct file *filp,
+	unsigned int cmd,
+	unsigned long arg)
+{
+	struct maplib_priv *priv;
+	void __user *argp = (void __user *)arg;
+
+	priv = (struct maplib_priv *)filp->private_data;
+
+	if (priv == NULL) {
+		printk (KERN_WARNING "MAPLIB: device %d doesnt exist\n", priv->index);
+		return -ENODEV;
+	}
+
+	/* Verify READ/WRITE Access to user provided buffer */
+	if (_IOC_DIR(cmd) & (_IOC_WRITE | _IOC_READ))
+		if (!access_ok((void __user *)arg, _IOC_SIZE(cmd)))
+			return -EFAULT;
+
+	switch (cmd) {
+	case MAPLIB_IOCTL_SETUP:
+	{
+		struct maplib_setup setup;
+		if(copy_from_user(&setup, argp, sizeof(struct maplib_setup)))
+			return -EFAULT;
+		return maplib_buf_setup(priv, &setup);
+	}
+
+	case MAPLIB_IOCTL_MMAPINFO:
+	{
+		struct maplib_mmap_info mmapinfo;
+		int err = maplib_mmapinfo_get(priv, &mmapinfo);
+		if ( err )
+			return err;
+		if(copy_to_user(argp, &mmapinfo, sizeof(struct maplib_mmap_info)))
+			return -EFAULT;
+		break;
+	}
+
+	default:
+		return -ENOSYS;
+	}
+
+	return 0;
+}
+
+static int __init maplib_init (void)
+{
+	int result, i;
+	struct maplib_priv *priv;
+	dev_t dev_id;
+
+	maplib_privs = NULL;
+	maplib_count = MAPLIB_MAX;
+
+	/* Get Major Number of CHAR driver */
+	dev_id = MKDEV(MAPLIB_MAJOR, MAPLIB_MINOR);
+	result = register_chrdev_region(dev_id, maplib_count, "maplib");
+	if ( result < 0 ) {
+		printk(KERN_WARNING "MAPLIB: Failed to register CHAR region\n");
+		return 0;
+	}
+
+	/* Allocate private memory for all MAPLIB cores */
+	maplib_privs = (struct maplib_priv *)
+		kmalloc(sizeof(struct maplib_priv) * maplib_count, GFP_KERNEL);
+	memset(maplib_privs, 0, sizeof(struct maplib_priv) * maplib_count);
+
+	/* Init all private structures */
+	for (i=0; i<maplib_count; i++) {
+		priv = &maplib_privs[i];
+		priv->index = i;
+		strcat(priv->devname, "MAPLIB_N");
+		priv->devname[7] = '0' + i;
+
+		/* Init and Register CHAR driver */
+		dev_id = MKDEV(MAPLIB_MAJOR, MAPLIB_MINOR+i);
+		cdev_init(&priv->cdev, &maplib_fops);
+		result = cdev_add(&priv->cdev, dev_id, 1);
+		if ( result ) {
+			printk(KERN_NOTICE "MAPLIB: Failed adding CHAR dev\n");
+		}
+	}
+
+	printk(KERN_DEBUG "MAPLIB: max %d mmap-pools\n", maplib_count);
+
+	return 0;
+}
+
+static void __exit maplib_exit (void)
+{
+	int i;
+	dev_t devno = MKDEV (MAPLIB_MAJOR, MAPLIB_MINOR);
+	struct maplib_priv *priv;
+
+	for (i = 0; i<maplib_count; i++) {
+		priv = &maplib_privs[i];
+		cdev_del(&priv->cdev);
+	}
+
+	unregister_chrdev_region(devno, maplib_count);
+
+	if ( maplib_privs )
+		kfree(maplib_privs);
+	maplib_privs = NULL;
+	maplib_count = 0;
+#ifdef MAPLIB_DEBUG
+	printk (KERN_DEBUG "MAPLIB: char driver cleaned up\n");
+#endif
+}
+
+module_init(maplib_init);
+module_exit(maplib_exit);
+
+MODULE_AUTHOR("Cobham Gaisler AB.");
+MODULE_DESCRIPTION("GRLIB Help Memory MAP Library Driver");
+MODULE_LICENSE("GPL");
+MODULE_ALIAS("platform:grlib-maplib");
diff -Nru a/drivers/grlib/README b/drivers/grlib/README
--- a/drivers/grlib/README	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/grlib/README	2022-01-29 23:16:14.137064547 +0100
@@ -0,0 +1,50 @@
+GRLIB driver library
+====================
+The purpose of the GRLIB Driver package is for Cobham Gaisler to provide
+Linux drivers for GRLIB cores that does not really benefit from being part
+of the official kernel tree. SpaceWire for example does not have a driver
+model.
+
+Drivers can be built outside of the kernel source tree as modules or
+within the kernel by installing the drivers into the kernel sources tree.
+Currently Makefiles for building outside of the kernel tree is not included,
+so for the time being please install the driver sources into the kernel.
+
+After installing the package into the kernel source tree a menu named
+"GRLIB Drivers" will appear in the bottom of the "Device Drivers" directory
+in the kernel configuration GUI. The Kernel Configuration GUI is invoked
+as normal:
+ [linux/]$ make ARCH=sparc CROSS_COMPILE=sparc-linux- xconfig
+
+
+Drivers included in Package
+===========================
+ * GRSPW2 Kernel Library (for custom kernel driver, or GRSPW Driver)
+ * GRSPW2 Driver (Char device accessible from Linux user space)
+ * GRSPW-ROUTER APB Register Driver
+ * MAPLIB, Device memory handling. Enables a user to memory map blocks of
+   linear memory that can be used by device drivers for DMA access. GRLIB 
+   Drivers that implement zero-copy to user-space and between device nodes
+   though user-space require the MAPLIB char driver.
+
+
+Requirements
+============
+The GRLIB Drivers package is built against one specific Linux release, it
+is expected that drivers may fail to build or does not function properly
+if used under another Linux version. The kernel that must be used is taken from
+www.kernel.org and may require patching using the Cobham Gaisler
+"unofficial LEON patches" distributed until they are included in the official
+kernel tree.
+
+Please check which GIT commit version is required in the VERSION file.
+
+
+Installing GRLIB Drivers to Kernel source tree
+==============================================
+Copy the this directory into the linux/drivers directory and
+name it grlib, then add the newly added grlib directory to the Linux build
+files (drivers/Makefile) and to the kernel configuration GUI (drivers/Kconfig)
+by applying the patch in add_grlib_tree.patch.
+ $ cd linux/drivers
+ $ patch -p1 grlib/add_grlib_tree-X.Y.Z.patch
diff -Nru a/drivers/grlib/spw/grspw.c b/drivers/grlib/spw/grspw.c
--- a/drivers/grlib/spw/grspw.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/grlib/spw/grspw.c	2022-02-06 00:16:06.538940375 +0100
@@ -0,0 +1,2884 @@
+/*
+ * Cobham Gaisler GRSPW2 SpaceWire Kernel Library Interface.
+ *
+ * Copyright (c) 2016-2022 Cobham Gaisler AB
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston,
+ * MA 02110-1301 USA
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/fs.h>
+#include <linux/cdev.h>
+#include <asm/uaccess.h>
+#include <linux/mm.h>
+#include <linux/slab.h>
+#include <linux/mman.h>
+#include <linux/io.h>
+#include <linux/interrupt.h>
+#include <linux/semaphore.h>
+#include <linux/workqueue.h>
+
+#include <linux/of_device.h>
+#include <linux/of_irq.h>
+#include <linux/of_platform.h>
+#include <linux/dma-mapping.h>
+
+#ifdef CONFIG_SPARC_LEON
+#include <asm/leon.h>
+#endif
+#include <linux/grlib/grspw.h>
+#include <linux/version.h>
+
+#define SPIN_DECLARE(name) spinlock_t name
+#define SPIN_INIT(lock) spin_lock_init(lock)
+#define SPIN_LOCK(lock, level) spin_lock(lock)
+#define SPIN_LOCK_IRQ(lock, level) spin_lock_irqsave(lock, level)
+#define SPIN_UNLOCK(lock, level) spin_unlock(lock)
+#define SPIN_UNLOCK_IRQ(lock, level) spin_unlock_irqrestore(lock, level)
+#define IRQFLAGS_TYPE unsigned long
+
+/*#define STATIC*/
+#define STATIC static
+
+#ifndef init_MUTEX
+#define init_MUTEX(a) sema_init(a, 1);
+#endif
+#ifndef init_MUTEX_LOCKED
+#define init_MUTEX_LOCKED(a) sema_init(a, 0);
+#endif
+
+struct grspw_dma_regs {
+	volatile u32 ctrl;	/* DMA Channel Control */
+	volatile u32 rxmax;	/* RX Max Packet Length */
+	volatile u32 txdesc;	/* TX Descriptor Base/Current */
+	volatile u32 rxdesc;	/* RX Descriptor Base/Current */
+	volatile u32 addr;	/* Address Register */
+	volatile u32 resv[3];
+};
+
+struct grspw_regs {
+	volatile u32 ctrl;
+	volatile u32 status;
+	volatile u32 nodeaddr;
+	volatile u32 clkdiv;
+	volatile u32 destkey;
+	volatile u32 time;
+	volatile u32 timer;	/* Used only in GRSPW1 */
+	volatile u32 resv1;
+
+	/* DMA Registers, ctrl.NCH determines number of ports, 
+	 * up to 4 channels are supported
+	 */
+	struct grspw_dma_regs dma[4];
+
+	volatile u32 icctrl;
+	volatile u32 icrx;
+	volatile u32 icack;
+	volatile u32 ictimeout;
+	volatile u32 ictickomask;
+	volatile u32 icaamask;
+	volatile u32 icrlpresc;
+	volatile u32 icrlisr;
+	volatile u32 icrlintack;
+	volatile u32 resv2;
+	volatile u32 icisr;
+	volatile u32 resv3;
+};
+
+/* GRSPW - Control Register - 0x00 */
+#define GRSPW_CTRL_RA_BIT	31
+#define GRSPW_CTRL_RX_BIT	30
+#define GRSPW_CTRL_RC_BIT	29
+#define GRSPW_CTRL_NCH_BIT	27
+#define GRSPW_CTRL_PO_BIT	26
+#define GRSPW_CTRL_PS_BIT	21
+#define GRSPW_CTRL_NP_BIT	20
+#define GRSPW_CTRL_RD_BIT	17
+#define GRSPW_CTRL_RE_BIT	16
+#define GRSPW_CTRL_TR_BIT	11
+#define GRSPW_CTRL_TT_BIT	10
+#define GRSPW_CTRL_LI_BIT	9
+#define GRSPW_CTRL_TQ_BIT	8
+#define GRSPW_CTRL_RS_BIT	6
+#define GRSPW_CTRL_PM_BIT	5
+#define GRSPW_CTRL_TI_BIT	4
+#define GRSPW_CTRL_IE_BIT	3
+#define GRSPW_CTRL_AS_BIT	2
+#define GRSPW_CTRL_LS_BIT	1
+#define GRSPW_CTRL_LD_BIT	0
+
+#define GRSPW_CTRL_RA	(1<<GRSPW_CTRL_RA_BIT)
+#define GRSPW_CTRL_RX	(1<<GRSPW_CTRL_RX_BIT)
+#define GRSPW_CTRL_RC	(1<<GRSPW_CTRL_RC_BIT)
+#define GRSPW_CTRL_NCH	(0x3<<GRSPW_CTRL_NCH_BIT)
+#define GRSPW_CTRL_PO	(1<<GRSPW_CTRL_PO_BIT)
+#define GRSPW_CTRL_PS	(1<<GRSPW_CTRL_PS_BIT)
+#define GRSPW_CTRL_NP	(1<<GRSPW_CTRL_NP_BIT)
+#define GRSPW_CTRL_RD	(1<<GRSPW_CTRL_RD_BIT)
+#define GRSPW_CTRL_RE	(1<<GRSPW_CTRL_RE_BIT)
+#define GRSPW_CTRL_TR	(1<<GRSPW_CTRL_TR_BIT)
+#define GRSPW_CTRL_TT	(1<<GRSPW_CTRL_TT_BIT)
+#define GRSPW_CTRL_LI	(1<<GRSPW_CTRL_LI_BIT)
+#define GRSPW_CTRL_TQ	(1<<GRSPW_CTRL_TQ_BIT)
+#define GRSPW_CTRL_RS	(1<<GRSPW_CTRL_RS_BIT)
+#define GRSPW_CTRL_PM	(1<<GRSPW_CTRL_PM_BIT)
+#define GRSPW_CTRL_TI	(1<<GRSPW_CTRL_TI_BIT)
+#define GRSPW_CTRL_IE	(1<<GRSPW_CTRL_IE_BIT)
+#define GRSPW_CTRL_AS	(1<<GRSPW_CTRL_AS_BIT)
+#define GRSPW_CTRL_LS	(1<<GRSPW_CTRL_LS_BIT)
+#define GRSPW_CTRL_LD	(1<<GRSPW_CTRL_LD_BIT)
+
+/* GRSPW - Status Register - 0x04 */
+#define GRSPW_STS_LS_BIT	21
+#define GRSPW_STS_AP_BIT	9
+#define GRSPW_STS_EE_BIT	8
+#define GRSPW_STS_IA_BIT	7
+#define GRSPW_STS_WE_BIT	6	/* GRSPW1 */
+#define GRSPW_STS_PE_BIT	4
+#define GRSPW_STS_DE_BIT	3
+#define GRSPW_STS_ER_BIT	2
+#define GRSPW_STS_CE_BIT	1
+#define GRSPW_STS_TO_BIT	0
+
+#define GRSPW_STS_LS	(0x7<<GRSPW_STS_LS_BIT)
+#define GRSPW_STS_AP	(1<<GRSPW_STS_AP_BIT)
+#define GRSPW_STS_EE	(1<<GRSPW_STS_EE_BIT)
+#define GRSPW_STS_IA	(1<<GRSPW_STS_IA_BIT)
+#define GRSPW_STS_WE	(1<<GRSPW_STS_WE_BIT)	/* GRSPW1 */
+#define GRSPW_STS_PE	(1<<GRSPW_STS_PE_BIT)
+#define GRSPW_STS_DE	(1<<GRSPW_STS_DE_BIT)
+#define GRSPW_STS_ER	(1<<GRSPW_STS_ER_BIT)
+#define GRSPW_STS_CE	(1<<GRSPW_STS_CE_BIT)
+#define GRSPW_STS_TO	(1<<GRSPW_STS_TO_BIT)
+
+/* GRSPW - Default Address Register - 0x08 */
+#define GRSPW_DEF_ADDR_BIT	0
+#define GRSPW_DEF_MASK_BIT	8
+#define GRSPW_DEF_ADDR	(0xff<<GRSPW_DEF_ADDR_BIT)
+#define GRSPW_DEF_MASK	(0xff<<GRSPW_DEF_MASK_BIT)
+
+/* GRSPW - Clock Divisor Register - 0x0C */
+#define GRSPW_CLKDIV_START_BIT	8
+#define GRSPW_CLKDIV_RUN_BIT	0
+#define GRSPW_CLKDIV_START	(0xff<<GRSPW_CLKDIV_START_BIT)
+#define GRSPW_CLKDIV_RUN	(0xff<<GRSPW_CLKDIV_RUN_BIT)
+#define GRSPW_CLKDIV_MASK	(GRSPW_CLKDIV_START|GRSPW_CLKDIV_RUN)
+
+/* GRSPW - Destination key Register - 0x10 */
+#define GRSPW_DK_DESTKEY_BIT	0
+#define GRSPW_DK_DESTKEY	(0xff<<GRSPW_DK_DESTKEY_BIT)
+
+/* GRSPW - Time Register - 0x14 */
+#define GRSPW_TIME_CTRL_BIT	6
+#define GRSPW_TIME_CNT_BIT	0
+#define GRSPW_TIME_CTRL		(0x3<<GRSPW_TIME_CTRL_BIT)
+#define GRSPW_TIME_TCNT		(0x3f<<GRSPW_TIME_CNT_BIT)
+
+/* GRSPW - DMA Control Register - 0x20*N */
+#define GRSPW_DMACTRL_LE_BIT	16
+#define GRSPW_DMACTRL_SP_BIT	15
+#define GRSPW_DMACTRL_SA_BIT	14
+#define GRSPW_DMACTRL_EN_BIT	13
+#define GRSPW_DMACTRL_NS_BIT	12
+#define GRSPW_DMACTRL_RD_BIT	11
+#define GRSPW_DMACTRL_RX_BIT	10
+#define GRSPW_DMACTRL_AT_BIT	9
+#define GRSPW_DMACTRL_RA_BIT	8
+#define GRSPW_DMACTRL_TA_BIT	7
+#define GRSPW_DMACTRL_PR_BIT	6
+#define GRSPW_DMACTRL_PS_BIT	5
+#define GRSPW_DMACTRL_AI_BIT	4
+#define GRSPW_DMACTRL_RI_BIT	3
+#define GRSPW_DMACTRL_TI_BIT	2
+#define GRSPW_DMACTRL_RE_BIT	1
+#define GRSPW_DMACTRL_TE_BIT	0
+
+#define GRSPW_DMACTRL_LE	(1<<GRSPW_DMACTRL_LE_BIT)
+#define GRSPW_DMACTRL_SP	(1<<GRSPW_DMACTRL_SP_BIT)
+#define GRSPW_DMACTRL_SA	(1<<GRSPW_DMACTRL_SA_BIT)
+#define GRSPW_DMACTRL_EN	(1<<GRSPW_DMACTRL_EN_BIT)
+#define GRSPW_DMACTRL_NS	(1<<GRSPW_DMACTRL_NS_BIT)
+#define GRSPW_DMACTRL_RD	(1<<GRSPW_DMACTRL_RD_BIT)
+#define GRSPW_DMACTRL_RX	(1<<GRSPW_DMACTRL_RX_BIT)
+#define GRSPW_DMACTRL_AT	(1<<GRSPW_DMACTRL_AT_BIT)
+#define GRSPW_DMACTRL_RA	(1<<GRSPW_DMACTRL_RA_BIT)
+#define GRSPW_DMACTRL_TA	(1<<GRSPW_DMACTRL_TA_BIT)
+#define GRSPW_DMACTRL_PR	(1<<GRSPW_DMACTRL_PR_BIT)
+#define GRSPW_DMACTRL_PS	(1<<GRSPW_DMACTRL_PS_BIT)
+#define GRSPW_DMACTRL_AI	(1<<GRSPW_DMACTRL_AI_BIT)
+#define GRSPW_DMACTRL_RI	(1<<GRSPW_DMACTRL_RI_BIT)
+#define GRSPW_DMACTRL_TI	(1<<GRSPW_DMACTRL_TI_BIT)
+#define GRSPW_DMACTRL_RE	(1<<GRSPW_DMACTRL_RE_BIT)
+#define GRSPW_DMACTRL_TE	(1<<GRSPW_DMACTRL_TE_BIT)
+
+/* GRSPW - DMA Channel Max Packet Length Register - (0x20*N + 0x04) */
+#define GRSPW_DMARXLEN_MAX_BIT	0
+#define GRSPW_DMARXLEN_MAX	(0xffffff<<GRSPW_DMARXLEN_MAX_BIT)
+
+/* GRSPW - DMA Channel Address Register - (0x20*N + 0x10) */
+#define GRSPW_DMAADR_ADDR_BIT	0
+#define GRSPW_DMAADR_MASK_BIT	8
+#define GRSPW_DMAADR_ADDR	(0xff<<GRSPW_DMAADR_ADDR_BIT)
+#define GRSPW_DMAADR_MASK	(0xff<<GRSPW_DMAADR_MASK_BIT)
+
+/* GRSPW - Interrupt code receive register - 0xa4 */
+#define GRSPW_ICCTRL_INUM_BIT	27
+#define GRSPW_ICCTRL_IA_BIT	24
+#define GRSPW_ICCTRL_LE_BIT	23
+#define GRSPW_ICCTRL_PR_BIT	22
+#define GRSPW_ICCTRL_DQ_BIT	21 /* never used */
+#define GRSPW_ICCTRL_TQ_BIT	20
+#define GRSPW_ICCTRL_AQ_BIT	19
+#define GRSPW_ICCTRL_IQ_BIT	18
+#define GRSPW_ICCTRL_IR_BIT	17
+#define GRSPW_ICCTRL_IT_BIT	16
+#define GRSPW_ICCTRL_NUMI_BIT	13
+#define GRSPW_ICCTRL_BIRQ_BIT	8
+#define GRSPW_ICCTRL_ID_BIT	7
+#define GRSPW_ICCTRL_II_BIT	6
+#define GRSPW_ICCTRL_TXIRQ_BIT	0
+#define GRSPW_ICCTRL_INUM	(0x3f << GRSPW_ICCTRL_INUM_BIT)
+#define GRSPW_ICCTRL_IA		(1 << GRSPW_ICCTRL_IA_BIT)
+#define GRSPW_ICCTRL_LE		(1 << GRSPW_ICCTRL_LE_BIT)
+#define GRSPW_ICCTRL_PR		(1 << GRSPW_ICCTRL_PR_BIT)
+#define GRSPW_ICCTRL_DQ		(1 << GRSPW_ICCTRL_DQ_BIT)
+#define GRSPW_ICCTRL_TQ		(1 << GRSPW_ICCTRL_TQ_BIT)
+#define GRSPW_ICCTRL_AQ		(1 << GRSPW_ICCTRL_AQ_BIT)
+#define GRSPW_ICCTRL_IQ		(1 << GRSPW_ICCTRL_IQ_BIT)
+#define GRSPW_ICCTRL_IR		(1 << GRSPW_ICCTRL_IR_BIT)
+#define GRSPW_ICCTRL_IT		(1 << GRSPW_ICCTRL_IT_BIT)
+#define GRSPW_ICCTRL_NUMI	(0x7 << GRSPW_ICCTRL_NUMI_BIT)
+#define GRSPW_ICCTRL_BIRQ	(0x1f << GRSPW_ICCTRL_BIRQ_BIT)
+#define GRSPW_ICCTRL_ID		(1 << GRSPW_ICCTRL_ID_BIT)
+#define GRSPW_ICCTRL_II		(1 << GRSPW_ICCTRL_II_BIT)
+#define GRSPW_ICCTRL_TXIRQ	(0x3f << GRSPW_ICCTRL_TXIRQ_BIT)
+
+/* RX Buffer Descriptor */
+struct grspw_rxbd {
+   volatile u32 ctrl;
+   volatile u32 addr;
+};
+
+/* TX Buffer Descriptor */
+struct grspw_txbd {
+   volatile u32 ctrl;
+   volatile u32 haddr;
+   volatile u32 dlen;
+   volatile u32 daddr;
+};
+
+/* GRSPW - DMA RXBD Ctrl */
+#define GRSPW_RXBD_LEN_BIT 0
+#define GRSPW_RXBD_LEN	(0x1ffffff<<GRSPW_RXBD_LEN_BIT)
+#define GRSPW_RXBD_EN	(1<<25)
+#define GRSPW_RXBD_WR	(1<<26)
+#define GRSPW_RXBD_IE	(1<<27)
+#define GRSPW_RXBD_EP	(1<<28)
+#define GRSPW_RXBD_HC	(1<<29)
+#define GRSPW_RXBD_DC	(1<<30)
+#define GRSPW_RXBD_TR	(1<<31)
+
+#define GRSPW_TXBD_HLEN	(0xff<<0)
+#define GRSPW_TXBD_NCL	(0xf<<8)
+#define GRSPW_TXBD_EN	(1<<12)
+#define GRSPW_TXBD_WR	(1<<13)
+#define GRSPW_TXBD_IE	(1<<14)
+#define GRSPW_TXBD_LE	(1<<15)
+#define GRSPW_TXBD_HC	(1<<16)
+#define GRSPW_TXBD_DC	(1<<17)
+
+#define GRSPW_DMAADR_MASK_BIT	8
+#define GRSPW_DMAADR_ADDR	(0xff<<GRSPW_DMAADR_ADDR_BIT)
+#define GRSPW_DMAADR_MASK	(0xff<<GRSPW_DMAADR_MASK_BIT)
+
+
+/* GRSPW Error Condition */
+#define GRSPW_STAT_ERROR	(GRSPW_STS_EE | GRSPW_STS_IA | GRSPW_STS_WE | GRSPW_STS_PE | GRSPW_STS_DE | GRSPW_STS_ER | GRSPW_STS_CE)
+#define GRSPW_DMA_STATUS_ERROR	(GRSPW_DMACTRL_RA | GRSPW_DMACTRL_TA)
+/* GRSPW Link configuration options */
+#define GRSPW_LINK_CFG		(GRSPW_CTRL_LI | GRSPW_CTRL_LD | GRSPW_CTRL_LS | GRSPW_CTRL_AS)
+#define GRSPW_LINKSTATE(status)	((status & GRSPW_CTRL_LS) >> GRSPW_CTRL_LS_BIT)
+
+/* Software Defaults */
+#define DEFAULT_RXMAX 1024	/* 1 KBytes Max RX Packet Size */
+
+/* GRSPW Constants */
+#define GRSPW_TXBD_NR 64	/* Maximum number of TX Descriptors */
+#define GRSPW_RXBD_NR 128	/* Maximum number of RX Descriptors */
+#define GRSPW_TXBD_SIZE 16	/* Size in bytes of one TX descriptor */
+#define GRSPW_RXBD_SIZE 8	/* Size in bytes of one RX descriptor */
+#define BDTAB_SIZE 0x400	/* BD Table Size (RX or TX) */
+#define BDTAB_ALIGN 0x400	/* BD Table Alignment Requirement */
+
+#if (defined(CONFIG_SPARC_LEON) || defined(CONFIG_RISCV))
+/* Memory and HW Registers Access routines. All 32-bit access routines */
+static inline void grspw_write_bd(volatile u32 *bd, u32 val)
+{
+#if 0
+	__raw_writel(cpu_to_be32(val), bd);
+#else
+	__raw_writel(val, bd);
+#endif
+}
+static inline u32 grspw_read_bd(volatile u32 *bd)
+{
+#if 0
+	return be32_to_cpu(__raw_readl(bd));
+#else
+	return __raw_readl(bd);
+#endif
+}
+#define BD_WRITE(addr, val) (grspw_write_bd((volatile u32 *)(addr), (u32)(val)))
+#define BD_READ(addr) grspw_read_bd((volatile u32 *)(addr))
+#define REG_WRITE(addr, val) (*(volatile u32 *)(addr) = (u32)(val))
+#define REG_READ(addr) (*(volatile u32 *)(addr))
+#else
+#error UNSUPPORTED ARCHITECTURE
+#endif
+
+struct grspw_ring {
+	struct grspw_ring *next;	/* Next Descriptor */
+	union {
+		struct grspw_txbd *tx;	/* Descriptor Address */
+		struct grspw_rxbd *rx;	/* Descriptor Address */
+	} bd;
+	struct grspw_pkt *pkt;		/* Packet description associated.NULL if none*/	
+};
+
+/* An entry in the TX descriptor Ring */
+struct grspw_txring {
+	struct grspw_txring *next;	/* Next Descriptor */
+	struct grspw_txbd *bd;		/* Descriptor Address */
+	struct grspw_pkt *pkt;		/* Packet description associated.NULL if none*/
+};
+
+/* An entry in the RX descriptor Ring */
+struct grspw_rxring {
+	struct grspw_rxring *next;	/* Next Descriptor */
+	struct grspw_rxbd *bd;		/* Descriptor Address */
+	struct grspw_pkt *pkt;		/* Packet description associated.NULL if none*/
+};
+
+
+struct grspw_dma_priv {
+	struct grspw_priv *core;	/* GRSPW Core */
+	struct grspw_dma_regs __iomem *regs;	/* DMA Channel Registers */
+	int index;			/* DMA Channel Index @ GRSPW core */
+	int open;			/* DMA Channel opened by user */
+	int started;			/* DMA Channel activity (start|stop) */
+	struct semaphore sem_rxdma;	/* DMA Channel RX Semaphore */
+	struct semaphore sem_txdma;	/* DMA Channel TX Semaphore */
+	struct grspw_dma_stats stats;	/* DMA Channel Statistics */
+	struct grspw_dma_config cfg;	/* DMA Channel Configuration */
+
+	/* Scheduled Work on the Work queue for this particular DMA
+	 * channel.
+	 */
+	struct work_struct work;
+
+
+	/*** RX ***/
+
+	/* RX Descriptor Ring */
+	struct grspw_rxbd *rx_bds;		/* Descriptor Address */
+	dma_addr_t rx_bds_pa;			/* Descriptor Physical Address */
+	struct grspw_rxring *rx_ring_base;
+	struct grspw_rxring *rx_ring_head;	/* Next descriptor to enable */
+	struct grspw_rxring *rx_ring_tail;	/* Oldest enabled Descriptor */
+	int rx_irq_en_cnt_curr;
+	struct {
+		int waiting;
+		int ready_cnt;
+		int op;
+		int recv_cnt;
+		struct semaphore sem_wait;	/* RX Semaphore used to implement RX blocking */
+	} rx_wait;
+
+	/* Queue of Packets READY to be scheduled */
+	struct grspw_list ready;
+	int ready_cnt;
+
+	/* Scheduled RX Packets Queue */
+	struct grspw_list rx_sched;
+	int rx_sched_cnt;
+
+	/* Queue of Packets that has been RECIEVED */
+	struct grspw_list recv;
+	int recv_cnt;
+
+
+	/*** TX ***/
+
+	/* TX Descriptor Ring */
+	struct grspw_txbd *tx_bds;		/* Descriptor Address */
+	dma_addr_t tx_bds_pa;			/* Descriptor Physical Address */
+	struct grspw_txring *tx_ring_base;
+	struct grspw_txring *tx_ring_head;
+	struct grspw_txring *tx_ring_tail;
+	int tx_irq_en_cnt_curr;
+	struct {
+		int waiting;
+		int send_cnt;
+		int op;
+		int sent_cnt;
+		struct semaphore sem_wait;	/* TX Semaphore used to implement TX blocking */
+	} tx_wait;
+
+	/* Queue of Packets ready to be scheduled for transmission */
+	struct grspw_list send;
+	int send_cnt;
+
+	/* Scheduled TX Packets Queue */
+	struct grspw_list tx_sched;
+	int tx_sched_cnt;
+
+	/* Queue of Packets that has been SENT */
+	struct grspw_list sent;
+	int sent_cnt;
+};
+
+struct grspw_priv {
+	char devname[8];		/* Device name "grspw%d" */
+	struct device *dev;		/* Linux platform device */
+	struct grspw_regs *regs;	/* Virtual Address of APB Registers */
+	int irq;			/* AMBA IRQ number of core */
+	int index;			/* Index in order it was probed */
+	int core_index;			/* Core Bus Index */
+	int open;			/* If Device is alrady opened (=1) or not (=0) */
+	void *data;			/* User private Data for this device instance, set by grspw_initialize_user */
+
+	/* Features supported by Hardware */
+	struct grspw_hw_sup hwsup;
+
+	/* Pointer to an array of Maximally 4 DMA Channels */
+	struct grspw_dma_priv *dma;
+
+	/* Work global for the GRSPW core */
+	struct work_struct work;
+
+	/* Spin-lock ISR protection */
+	SPIN_DECLARE(devlock);
+
+	/* Descriptor Memory Area for TX & RX and all DMA channels */
+	void *bd_mem;
+	dma_addr_t bd_mem_pa;
+	unsigned int bdtabsize;
+
+	/*** Time Code Handling ***/
+	void (*tcisr)(void *data, int timecode);
+	void *tcisr_arg;
+
+	/* Bit mask representing events which shall cause link disable. */
+	unsigned int dis_link_on_err;
+
+	/* Bit mask for link status bits to clear by ISR */
+	unsigned int stscfg;
+
+	/* "Core Global" Statistics gathered, not dependent on DMA channel */
+	struct grspw_core_stats stats;
+};
+
+int grspw_initialized = 0;
+int grspw_count = 0;
+struct workqueue_struct *grspw_workq = NULL;
+struct semaphore grspw_sem;
+static struct grspw_priv *priv_tab[GRSPW_MAX];
+
+/* callback to upper layer when devices are discovered/removed */
+void *(*grspw_dev_add)(int) = NULL;
+void (*grspw_dev_del)(int,void*) = NULL;
+
+STATIC void grspw_hw_stop(struct grspw_priv *priv);
+STATIC void grspw_hw_dma_stop(struct grspw_dma_priv *dma);
+STATIC void grspw_dma_reset(struct grspw_dma_priv *dma);
+
+void *grspw_open(int dev_no)
+{
+	struct grspw_priv *priv;
+	int i;
+	int semrc;
+
+	if (grspw_initialized != 1 || (dev_no >= grspw_count))
+		return NULL;
+
+	priv = priv_tab[dev_no];
+
+	/* Take GRSPW lock */
+	while ( (semrc=down_interruptible(&grspw_sem)) )
+		printk(KERN_WARNING "grspw_open: down_interruptible %d\n", semrc);
+
+	if (priv->open) {
+		up(&grspw_sem);
+		return NULL;
+	}
+
+	/* Take the device */
+	priv->open = 1;
+
+	/* Return GRSPW Lock */
+	up(&grspw_sem);
+
+	/* Initialize Spin-lock for GRSPW Device. This is to protect
+	 * CTRL and DMACTRL registers from ISR.
+	 */
+	SPIN_INIT(&priv->devlock);
+
+	priv->tcisr = NULL;
+	priv->tcisr_arg = NULL;
+	priv->stscfg = LINKSTS_MASK;
+
+	grspw_stats_clr(priv);
+
+	/* Allocate TX & RX Descriptor memory area for all DMA
+	 * channels. Max-size descriptor area is allocated:
+	 *  - 128 RX descriptors per DMA Channel
+	 *  - 64 TX descriptors per DMA Channel
+ 	 */
+	priv->bdtabsize = 2 * BDTAB_SIZE * priv->hwsup.ndma_chans;
+	priv->bd_mem = dma_alloc_coherent(priv->dev, priv->bdtabsize,
+				          &priv->bd_mem_pa,
+					  GFP_KERNEL | __GFP_ZERO);
+	if (!priv->bd_mem) {
+		priv->open = 0;
+		return NULL;
+		}
+	if ((long)priv->bd_mem & (BDTAB_ALIGN-1)) {
+		printk(KERN_INFO "GRSPW: Descriptors not aligned to boundary: %p\n",
+					priv->bd_mem);
+		dma_free_coherent(priv->dev, priv->bdtabsize, priv->bd_mem,
+				  priv->bd_mem_pa);
+		priv->open = 0;
+		return NULL;
+	}
+
+	for (i=0; i<priv->hwsup.ndma_chans; i++) {
+		/* Do DMA Channel Init, other variables etc. are inited
+		 * when respective DMA channel is opened.
+		 *
+		 * index & core are initialized by probe function.
+		 */
+		priv->dma[i].open = 0;
+		priv->dma[i].rx_bds = (struct grspw_rxbd *)
+			(priv->bd_mem + i*BDTAB_SIZE*2);
+		priv->dma[i].rx_bds_pa = priv->bd_mem_pa + BDTAB_SIZE*(2*i);
+		priv->dma[i].tx_bds = (struct grspw_txbd *)
+			(priv->bd_mem + BDTAB_SIZE*(2*i+1));
+		priv->dma[i].tx_bds_pa = priv->bd_mem_pa + BDTAB_SIZE*(2*i+1);
+	}
+
+	/* Basic initialization of hardware, clear some registers but
+	 * keep Link/RMAP/Node-Address registers intact.
+	 */
+	grspw_hw_stop(priv);
+
+	return priv;
+}
+EXPORT_SYMBOL(grspw_open);
+
+int grspw_close(void *d)
+{
+	struct grspw_priv *priv = d;
+	int i;
+	int semrc;
+
+	/* Take GRSPW lock */
+	while ( (semrc=down_interruptible(&grspw_sem)) )
+		printk(KERN_WARNING "grspw_close: down_interruptible %d\n", semrc);
+
+	/* Check that user has stopped and closed all DMA channels
+	 * appropriately. At this point the Hardware shall not be doing DMA
+	 * or generating Interrupts. We want HW in a "startup-state".
+	 */
+	for (i=0; i<priv->hwsup.ndma_chans; i++) {
+		if (priv->dma[i].open) {
+			up(&grspw_sem);
+			return 1;
+		}
+	}
+	grspw_hw_stop(priv);
+
+	dma_free_coherent(priv->dev, priv->bdtabsize, priv->bd_mem,
+			  priv->bd_mem_pa);
+
+	/* Mark not open */
+	priv->open = 0;
+	up(&grspw_sem);
+	return 0;
+}
+EXPORT_SYMBOL(grspw_close);
+
+void grspw_hw_support(void *d, struct grspw_hw_sup *hw)
+{
+	struct grspw_priv *priv = d;
+
+	*hw = priv->hwsup;
+}
+EXPORT_SYMBOL(grspw_hw_support);
+
+void grspw_addr_ctrl(void *d, struct grspw_addr_config *cfg)
+{
+	struct grspw_priv *priv = d;
+	struct grspw_regs *regs = priv->regs;
+	unsigned int ctrl, nodeaddr;
+	IRQFLAGS_TYPE irqflags;
+	int i;
+
+	if (!priv || !cfg)
+		return;
+
+	SPIN_LOCK_IRQ(&priv->devlock, irqflags);
+
+	if (cfg->promiscuous != -1) {
+		/* Set Configuration */
+		ctrl = REG_READ(&regs->ctrl);
+		if (cfg->promiscuous)
+			ctrl |= GRSPW_CTRL_PM;
+		else
+			ctrl &= ~GRSPW_CTRL_PM;
+		REG_WRITE(&regs->ctrl, ctrl);
+		REG_WRITE(&regs->nodeaddr, (cfg->def_mask<<8) | cfg->def_addr);
+
+		for (i=0; i<priv->hwsup.ndma_chans; i++) {
+			ctrl = REG_READ(&regs->dma[i].ctrl);
+			ctrl &= ~(GRSPW_DMACTRL_PS|GRSPW_DMACTRL_PR|GRSPW_DMA_STATUS_ERROR);
+			if (cfg->dma_nacfg[i].node_en) {
+				ctrl |= GRSPW_DMACTRL_EN;
+				REG_WRITE(&regs->dma[i].addr,
+				          (cfg->dma_nacfg[i].node_addr & 0xff) |
+				          ((cfg->dma_nacfg[i].node_mask & 0xff)<<8));
+			} else {
+				ctrl &= ~GRSPW_DMACTRL_EN;
+			}
+			REG_WRITE(&regs->dma[i].ctrl, ctrl);
+		}
+	}
+
+	/* Read Current Configuration */
+	cfg->promiscuous = REG_READ(&regs->ctrl) & GRSPW_CTRL_PM;
+	nodeaddr = REG_READ(&regs->nodeaddr);
+	cfg->def_addr = (nodeaddr & GRSPW_DEF_ADDR) >> GRSPW_DEF_ADDR_BIT;
+	cfg->def_mask = (nodeaddr & GRSPW_DEF_MASK) >> GRSPW_DEF_MASK_BIT;
+	for (i=0; i<priv->hwsup.ndma_chans; i++) {
+		cfg->dma_nacfg[i].node_en = REG_READ(&regs->dma[i].ctrl) &
+						GRSPW_DMACTRL_EN;
+		ctrl = REG_READ(&regs->dma[i].addr);
+		cfg->dma_nacfg[i].node_addr = (ctrl & GRSPW_DMAADR_ADDR) >>
+						GRSPW_DMAADR_ADDR_BIT;
+		cfg->dma_nacfg[i].node_mask = (ctrl & GRSPW_DMAADR_MASK) >>
+						GRSPW_DMAADR_MASK_BIT;
+	}
+	SPIN_UNLOCK_IRQ(&priv->devlock, irqflags);
+	for (; i<4; i++) {
+		cfg->dma_nacfg[i].node_en = 0;
+		cfg->dma_nacfg[i].node_addr = 0;
+		cfg->dma_nacfg[i].node_mask = 0;
+	}
+}
+EXPORT_SYMBOL(grspw_addr_ctrl);
+
+/* Return Current Status Register */
+unsigned int grspw_link_status(void *d)
+{
+	struct grspw_priv *priv = d;
+
+	return REG_READ(&priv->regs->status);
+}
+EXPORT_SYMBOL(grspw_link_status);
+
+/* Clear Status Register bits */
+void grspw_link_status_clr(void *d, unsigned int mask)
+{
+	struct grspw_priv *priv = d;
+
+	REG_WRITE(&priv->regs->status, mask);
+}
+EXPORT_SYMBOL(grspw_link_status_clr);
+
+/* Return Current Link State */
+spw_link_state_t grspw_link_state(void *d)
+{
+	struct grspw_priv *priv = d;
+	unsigned int status = REG_READ(&priv->regs->status);
+
+	return (status & GRSPW_STS_LS) >> GRSPW_STS_LS_BIT;
+}
+EXPORT_SYMBOL(grspw_link_state);
+
+/* options [in/out]: set to -1 to only read current config */
+void grspw_link_ctrl(void *d, int *options, int *stscfg, int *clkdiv)
+{
+	struct grspw_priv *priv = d;
+	struct grspw_regs *regs = priv->regs;
+	unsigned int ctrl;
+	IRQFLAGS_TYPE irqflags;
+
+	/* Write? */
+	if (clkdiv) {
+		if (*clkdiv != -1)
+			REG_WRITE(&regs->clkdiv, *clkdiv & GRSPW_CLKDIV_MASK);
+		*clkdiv = REG_READ(&regs->clkdiv) & GRSPW_CLKDIV_MASK;
+	}
+	if (options) {
+		SPIN_LOCK_IRQ(&priv->devlock, irqflags);
+		ctrl = REG_READ(&regs->ctrl);
+		if (*options != -1) {
+			ctrl = (ctrl & ~GRSPW_LINK_CFG) |
+				(*options & GRSPW_LINK_CFG);
+
+			/* Enable Global IRQ only of LI or TQ is set */
+			if (ctrl & (GRSPW_CTRL_LI|GRSPW_CTRL_TQ))
+				ctrl |= GRSPW_CTRL_IE;
+			else
+				ctrl &= ~GRSPW_CTRL_IE;
+
+			REG_WRITE(&regs->ctrl, ctrl);
+			/* Store the link disable events for use in
+			ISR. The LINKOPTS_DIS_ON_* options are actually the
+			corresponding bits in the status register, shifted
+			by 16. */
+			priv->dis_link_on_err = *options &
+				(LINKOPTS_MASK_DIS_ON | LINKOPTS_DIS_ONERR);
+		}
+		SPIN_UNLOCK_IRQ(&priv->devlock, irqflags);
+		*options = (ctrl & GRSPW_LINK_CFG) | priv->dis_link_on_err;
+	}
+	if (stscfg) {
+		if (*stscfg != -1) {
+			priv->stscfg = *stscfg & LINKSTS_MASK;
+		}
+		*stscfg = priv->stscfg;
+	}
+}
+EXPORT_SYMBOL(grspw_link_ctrl);
+
+/* Generate Tick-In (increment Time Counter, Send Time Code) */
+void grspw_tc_tx(void *d)
+{
+	struct grspw_priv *priv = d;
+	struct grspw_regs *regs = priv->regs;
+	IRQFLAGS_TYPE irqflags;
+
+	SPIN_LOCK_IRQ(&priv->devlock, irqflags);
+	REG_WRITE(&regs->ctrl, REG_READ(&regs->ctrl) | GRSPW_CTRL_TI);
+	SPIN_UNLOCK_IRQ(&priv->devlock, irqflags);
+}
+EXPORT_SYMBOL(grspw_tc_tx);
+
+void grspw_tc_ctrl(void *d, int *options)
+{
+	struct grspw_priv *priv = d;
+	struct grspw_regs *regs = priv->regs;
+	unsigned int ctrl;
+	IRQFLAGS_TYPE irqflags;
+
+	if (options == NULL)
+		return;
+
+	/* Write? */
+	if (*options != -1) {
+		SPIN_LOCK_IRQ(&priv->devlock, irqflags);
+		ctrl = REG_READ(&regs->ctrl);
+		ctrl &= ~(GRSPW_CTRL_TR|GRSPW_CTRL_TT|GRSPW_CTRL_TQ);
+		ctrl |= (*options & 0xd) << GRSPW_CTRL_TQ_BIT;
+
+		/* Enable Global IRQ only of LI or TQ is set */
+		if (ctrl & (GRSPW_CTRL_LI|GRSPW_CTRL_TQ))
+			ctrl |= GRSPW_CTRL_IE;
+		else
+			ctrl &= ~GRSPW_CTRL_IE;
+
+		REG_WRITE(&regs->ctrl, ctrl);
+		SPIN_UNLOCK_IRQ(&priv->devlock, irqflags);
+	} else
+		ctrl = REG_READ(&regs->ctrl);
+	*options = (ctrl >> GRSPW_CTRL_TQ_BIT) & 0xd;
+}
+EXPORT_SYMBOL(grspw_tc_ctrl);
+
+/* Assign ISR Function to TimeCode RX IRQ */
+void grspw_tc_isr(void *d, void (*tcisr)(void *data, int tc), void *data)
+{
+	struct grspw_priv *priv = d;
+
+	priv->tcisr_arg = data;
+	priv->tcisr = tcisr;
+}
+EXPORT_SYMBOL(grspw_tc_isr);
+
+/* Read/Write TCTRL and TIMECNT. Write if not -1, always read current value
+ * TCTRL   = bits 7 and 6
+ * TIMECNT = bits 5 to 0
+ */
+void grspw_tc_time(void *d, int *time)
+{
+	struct grspw_priv *priv = d;
+	struct grspw_regs *regs = priv->regs;
+
+	if (time == NULL)
+		return;
+	if (*time != -1)
+		REG_WRITE(&regs->time, *time & (GRSPW_TIME_TCNT | GRSPW_TIME_CTRL));
+	*time = REG_READ(&regs->time) & (GRSPW_TIME_TCNT | GRSPW_TIME_CTRL);
+}
+EXPORT_SYMBOL(grspw_tc_time);
+
+/* Set (not -1) and/or read RMAP options. */
+int grspw_rmap_ctrl(void *d, int *options, int *dstkey)
+{
+	struct grspw_priv *priv = d;
+	struct grspw_regs *regs = priv->regs;
+	unsigned int ctrl;
+	IRQFLAGS_TYPE irqflags;
+
+	if (dstkey) {
+		if (*dstkey != -1)
+			REG_WRITE(&regs->destkey, *dstkey & GRSPW_DK_DESTKEY);
+		*dstkey = REG_READ(&regs->destkey) & GRSPW_DK_DESTKEY;
+	}
+	if (options) {
+		if (*options != -1) {
+			if ((*options & RMAPOPTS_EN_RMAP) && !priv->hwsup.rmap)
+				return -1;
+
+
+			SPIN_LOCK_IRQ(&priv->devlock, irqflags);
+			ctrl = REG_READ(&regs->ctrl);
+			ctrl &= ~(GRSPW_CTRL_RE|GRSPW_CTRL_RD);
+			ctrl |= (*options & 0x3) << GRSPW_CTRL_RE_BIT;
+			REG_WRITE(&regs->ctrl, ctrl);
+			SPIN_UNLOCK_IRQ(&priv->devlock, irqflags);
+		}
+		*options = (REG_READ(&regs->ctrl) >> GRSPW_CTRL_RE_BIT) & 0x3;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(grspw_rmap_ctrl);
+
+void grspw_rmap_support(void *d, char *rmap, char *rmap_crc)
+{
+	struct grspw_priv *priv = d;
+
+	if (rmap)
+		*rmap = priv->hwsup.rmap;
+	if (rmap_crc)
+		*rmap_crc = priv->hwsup.rmap_crc;
+}
+EXPORT_SYMBOL(grspw_rmap_support);
+
+/* Select port, if 
+ * -1=The current selected port is returned
+ * 0=Port 0
+ * 1=Port 1
+ * Others=Both Port0 and Port1
+ */
+int grspw_port_ctrl(void *d, int *port)
+{
+	struct grspw_priv *priv = d;
+	struct grspw_regs *regs = priv->regs;
+	unsigned int ctrl;
+	IRQFLAGS_TYPE irqflags;
+
+	if (port == NULL)
+		return -1;
+
+	if ((*port == 1) || (*port == 0)) {
+		/* Select port user selected */
+		if ((*port == 1) && (priv->hwsup.nports < 2))
+			return -1; /* Changing to Port 1, but only one port available */
+		SPIN_LOCK_IRQ(&priv->devlock, irqflags);
+		ctrl = REG_READ(&regs->ctrl);
+		ctrl &= ~(GRSPW_CTRL_NP | GRSPW_CTRL_PS);
+		ctrl |= (*port & 1) << GRSPW_CTRL_PS_BIT;
+		REG_WRITE(&regs->ctrl, ctrl);
+		SPIN_UNLOCK_IRQ(&priv->devlock, irqflags);
+	} else if (*port > 1) {
+		/* Select both ports */
+		SPIN_LOCK_IRQ(&priv->devlock, irqflags);
+		REG_WRITE(&regs->ctrl, REG_READ(&regs->ctrl) | GRSPW_CTRL_NP);
+		SPIN_UNLOCK_IRQ(&priv->devlock, irqflags);
+	}
+
+	/* Get current settings */
+	ctrl = REG_READ(&regs->ctrl);
+	if (ctrl & GRSPW_CTRL_NP) {
+		/* Any port, selected by hardware */
+		if (priv->hwsup.nports > 1)
+			*port = 3;
+		else
+			*port = 0; /* Port0 the only port available */
+	} else {
+		*port = (ctrl & GRSPW_CTRL_PS) >> GRSPW_CTRL_PS_BIT;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(grspw_port_ctrl);
+
+/* Returns Number ports available in hardware */
+int grspw_port_count(void *d)
+{
+	struct grspw_priv *priv = d;
+
+	return priv->hwsup.nports;
+}
+EXPORT_SYMBOL(grspw_port_count);
+
+/* Current active port: 0 or 1 */
+int grspw_port_active(void *d)
+{
+	struct grspw_priv *priv = d;
+	unsigned int status;
+
+	status = REG_READ(&priv->regs->status);
+
+	return (status & GRSPW_STS_AP) >> GRSPW_STS_AP_BIT;
+}
+EXPORT_SYMBOL(grspw_port_active);
+
+void grspw_stats_read(void *d, struct grspw_core_stats *sts)
+{
+	struct grspw_priv *priv = d;
+
+	if (sts == NULL)
+		return;
+	memcpy(sts, &priv->stats, sizeof(priv->stats));
+}
+EXPORT_SYMBOL(grspw_stats_read);
+
+void grspw_stats_clr(void *d)
+{
+	struct grspw_priv *priv = d;
+
+	/* Clear most of the statistics */	
+	memset(&priv->stats, 0, sizeof(priv->stats));
+}
+EXPORT_SYMBOL(grspw_stats_clr);
+
+/*** DMA Interface ***/
+
+/* Initialize the RX and TX Descriptor Ring, empty of packets */
+STATIC void grspw_bdrings_init(struct grspw_dma_priv *dma)
+{
+	struct grspw_ring *r;
+	int i;
+
+	/* Empty BD rings */
+	dma->rx_ring_head = dma->rx_ring_base;
+	dma->rx_ring_tail = dma->rx_ring_base;
+	dma->tx_ring_head = dma->tx_ring_base;
+	dma->tx_ring_tail = dma->tx_ring_base;
+
+	/* Init RX Descriptors */
+	r = (struct grspw_ring *)dma->rx_ring_base;
+	for (i=0; i<GRSPW_RXBD_NR; i++) {
+
+		/* Init Ring Entry */
+		r[i].next = &r[i+1];
+		r[i].bd.rx = &dma->rx_bds[i];
+		r[i].pkt = NULL;
+
+		/* Init HW Descriptor */
+		BD_WRITE(&r[i].bd.rx->ctrl, 0);
+		BD_WRITE(&r[i].bd.rx->addr, 0);
+	}
+	r[GRSPW_RXBD_NR-1].next = &r[0];
+
+	/* Init TX Descriptors */
+	r = (struct grspw_ring *)dma->tx_ring_base;
+	for (i=0; i<GRSPW_TXBD_NR; i++) {
+
+		/* Init Ring Entry */
+		r[i].next = &r[i+1];
+		r[i].bd.tx = &dma->tx_bds[i];
+		r[i].pkt = NULL;
+
+		/* Init HW Descriptor */
+		BD_WRITE(&r[i].bd.tx->ctrl, 0);
+		BD_WRITE(&r[i].bd.tx->haddr, 0);
+		BD_WRITE(&r[i].bd.tx->dlen, 0);
+		BD_WRITE(&r[i].bd.tx->daddr, 0);
+	}
+	r[GRSPW_TXBD_NR-1].next = &r[0];
+}
+
+/* Try to populate descriptor ring with as many as possible READY unused packet
+ * buffers. The packets assigned with to a descriptor are put in the end of 
+ * the scheduled list.
+ *
+ * The number of Packets scheduled is returned.
+ *
+ *  - READY List -> RX-SCHED List
+ *  - Descriptors are initialized and enabled for reception
+ */
+STATIC int grspw_rx_schedule_ready(struct grspw_dma_priv *dma)
+{
+	int cnt;
+	unsigned int ctrl, dmactrl;
+	struct grspw_rxring *curr_bd;
+	struct grspw_pkt *curr_pkt, *last_pkt;
+	struct grspw_list lst;
+	IRQFLAGS_TYPE irqflags;
+
+	/* Is Ready Q empty? */
+	if (grspw_list_is_empty(&dma->ready))
+		return 0;
+
+	cnt = 0;
+	lst.head = curr_pkt = dma->ready.head;
+	curr_bd = dma->rx_ring_head;
+	while (!curr_bd->pkt) {
+
+		/* Assign Packet to descriptor */
+		curr_bd->pkt = curr_pkt;
+
+		/* Prepare descriptor address. */
+		BD_WRITE(&curr_bd->bd->addr, curr_pkt->data);
+
+		ctrl = GRSPW_RXBD_EN;
+		if (curr_bd->next == dma->rx_ring_base) {
+			/* Wrap around (only needed when smaller descriptor
+			 * table)
+			 */
+			ctrl |= GRSPW_RXBD_WR;
+		}
+
+		/* Is this Packet going to be an interrupt Packet? */
+		if ((--dma->rx_irq_en_cnt_curr) <= 0) {
+			if (dma->cfg.rx_irq_en_cnt == 0) {
+				/* IRQ is disabled. A big number to avoid
+				 * equal to zero too often
+				 */
+				dma->rx_irq_en_cnt_curr = 0x3fffffff;
+			} else {
+				dma->rx_irq_en_cnt_curr = dma->cfg.rx_irq_en_cnt;
+				ctrl |= GRSPW_RXBD_IE;
+			}
+		}
+
+		if (curr_pkt->flags & RXPKT_FLAG_IE)
+			ctrl |= GRSPW_RXBD_IE;
+
+		/* Enable descriptor */
+		BD_WRITE(&curr_bd->bd->ctrl, ctrl);
+
+		last_pkt = curr_pkt;
+		curr_bd = curr_bd->next;
+		cnt++;
+
+		/* Get Next Packet from Ready Queue */
+		if (curr_pkt == dma->ready.tail) {
+			/* Handled all in ready queue. */
+			curr_pkt = NULL;
+			break;
+		}
+		curr_pkt = curr_pkt->next;
+	}
+
+	/* Has Packets been scheduled? */
+	if (cnt > 0) {
+		/* Prepare list for insertion/deleation */
+		lst.tail = last_pkt;
+
+		/* Remove scheduled packets from ready queue */
+		grspw_list_remove_head_list(&dma->ready, &lst);
+		dma->ready_cnt -= cnt;
+		if (dma->stats.ready_cnt_min > dma->ready_cnt)
+			dma->stats.ready_cnt_min = dma->ready_cnt;
+
+		/* Insert scheduled packets into scheduled queue */
+		grspw_list_append_list(&dma->rx_sched, &lst);
+		dma->rx_sched_cnt += cnt;
+		if (dma->stats.rx_sched_cnt_max < dma->rx_sched_cnt)
+			dma->stats.rx_sched_cnt_max = dma->rx_sched_cnt;
+
+		/* Update TX ring posistion */
+		dma->rx_ring_head = curr_bd;
+
+		/* Make hardware aware of the newly enabled descriptors 
+		 * We must protect from ISR which writes RI|TI
+		 */
+		SPIN_LOCK_IRQ(&dma->core->devlock, irqflags);
+		dmactrl = REG_READ(&dma->regs->ctrl);
+		dmactrl &= ~(GRSPW_DMACTRL_PS|GRSPW_DMACTRL_PR|GRSPW_DMA_STATUS_ERROR);
+		dmactrl |= GRSPW_DMACTRL_RE | GRSPW_DMACTRL_RD;
+		REG_WRITE(&dma->regs->ctrl, dmactrl);
+		SPIN_UNLOCK_IRQ(&dma->core->devlock, irqflags);
+	}
+
+	return cnt;
+}
+
+/* Scans the RX desciptor table for scheduled Packet that has been received,
+ * and moves these Packet from the head of the scheduled queue to the
+ * tail of the recv queue.
+ *
+ * Also, for all packets the status is updated.
+ *
+ *  - SCHED List -> SENT List
+ *
+ * Return Value
+ * Number of packets moved
+ */
+STATIC int grspw_rx_process_scheduled(struct grspw_dma_priv *dma)
+{
+	struct grspw_rxring *curr;
+	struct grspw_pkt *last_pkt;
+	int recv_pkt_cnt = 0;
+	unsigned int ctrl;
+	struct grspw_list lst;
+
+	curr = dma->rx_ring_tail;
+
+	/* Step into RX ring to find if packets have been scheduled for 
+	 * reception.
+	 */
+	if (!curr->pkt)
+		return 0; /* No scheduled packets, thus no received, abort */
+
+	/* There has been Packets scheduled ==> scheduled Packets may have been
+	 * received and needs to be collected into RECV List.
+	 *
+	 * A temporary list "lst" with all received packets is created.
+	 */
+	lst.head = curr->pkt;
+
+	/* Loop until first enabled "unrecveived" SpW Packet is found.
+	 * An unused descriptor is indicated by an unassigned pkt field.
+	 */
+	while (curr->pkt && !((ctrl=BD_READ(&curr->bd->ctrl)) & GRSPW_RXBD_EN)) {
+		/* Handle one received Packet */
+
+		/* Remember last handled Packet so that insertion/removal from
+		 * Packet lists go fast.
+		 */
+		last_pkt = curr->pkt;
+
+		/* Get Length of Packet in bytes, and reception options */
+		last_pkt->dlen = (ctrl & GRSPW_RXBD_LEN) >> GRSPW_RXBD_LEN_BIT;
+
+		/* Set flags to indicate error(s) and CRC information,
+		 * and Mark Received.
+		 */
+		last_pkt->flags = (last_pkt->flags & ~RXPKT_FLAG_OUTPUT_MASK) |
+		                  ((ctrl >> 20) & RXPKT_FLAG_OUTPUT_MASK) |
+		                  RXPKT_FLAG_RX;
+
+		/* Packet was Truncated? */
+		if (ctrl & GRSPW_RXBD_TR)
+			dma->stats.rx_err_trunk++;
+
+		/* Error End-Of-Packet? */
+		if (ctrl & GRSPW_RXBD_EP)
+			dma->stats.rx_err_endpkt++;
+		curr->pkt = NULL; /* Mark descriptor unused */
+
+		/* Increment */
+		curr = curr->next;
+		recv_pkt_cnt++;
+	}
+
+	/* 1. Remove all handled packets from scheduled queue
+	 * 2. Put all handled packets into recv queue
+	 */
+	if (recv_pkt_cnt > 0) {
+
+		/* Update Stats, Number of Received Packets */
+		dma->stats.rx_pkts += recv_pkt_cnt;
+
+		/* Save RX ring posistion */
+		dma->rx_ring_tail = curr;
+
+		/* Prepare list for insertion/deleation */
+		lst.tail = last_pkt;
+
+		/* Remove received Packets from RX-SCHED queue */
+		grspw_list_remove_head_list(&dma->rx_sched, &lst);
+		dma->rx_sched_cnt -= recv_pkt_cnt;
+		if (dma->stats.rx_sched_cnt_min > dma->rx_sched_cnt)
+			dma->stats.rx_sched_cnt_min = dma->rx_sched_cnt;
+
+		/* Insert received Packets into RECV queue */
+		grspw_list_append_list(&dma->recv, &lst);
+		dma->recv_cnt += recv_pkt_cnt;
+		if (dma->stats.recv_cnt_max < dma->recv_cnt)
+			dma->stats.recv_cnt_max = dma->recv_cnt;
+	}
+
+	return recv_pkt_cnt;
+}
+
+/* Try to populate descriptor ring with as many SEND packets as possible. The
+ * packets assigned with to a descriptor are put in the end of 
+ * the scheduled list.
+ *
+ * The number of Packets scheduled is returned.
+ *
+ *  - SEND List -> TX-SCHED List
+ *  - Descriptors are initialized and enabled for transmission
+ */
+STATIC int grspw_tx_schedule_send(struct grspw_dma_priv *dma)
+{
+	int cnt;
+	unsigned int ctrl, dmactrl;
+	struct grspw_txring *curr_bd;
+	struct grspw_pkt *curr_pkt, *last_pkt;
+	struct grspw_list lst;
+	IRQFLAGS_TYPE irqflags;
+
+	/* Is Ready Q empty? */
+	if (grspw_list_is_empty(&dma->send))
+		return 0;
+
+	cnt = 0;
+	lst.head = curr_pkt = dma->send.head;
+	curr_bd = dma->tx_ring_head;
+	while (!curr_bd->pkt) {
+
+		/* Assign Packet to descriptor */
+		curr_bd->pkt = curr_pkt;
+
+		/* Prepare descriptor address. */
+		if ( curr_pkt->data && curr_pkt->dlen ) {
+			BD_WRITE(&curr_bd->bd->daddr, curr_pkt->data);
+			BD_WRITE(&curr_bd->bd->dlen, curr_pkt->dlen);
+		} else {
+			BD_WRITE(&curr_bd->bd->daddr, 0);
+			BD_WRITE(&curr_bd->bd->dlen, 0);
+			}
+
+		/* Set up header transmission */
+		if (curr_pkt->hdr && curr_pkt->hlen) {
+			BD_WRITE(&curr_bd->bd->haddr, curr_pkt->hdr);
+			ctrl = GRSPW_TXBD_EN | curr_pkt->hlen;
+		} else {
+			ctrl = GRSPW_TXBD_EN;
+		}
+		/* Enable IRQ generation and CRC options as specified
+		 * by user.
+		 */
+		ctrl |= (curr_pkt->flags & TXPKT_FLAG_INPUT_MASK) << 8;
+
+		if (curr_bd->next == dma->tx_ring_base) {
+			/* Wrap around (only needed when smaller descriptor table) */
+			ctrl |= GRSPW_TXBD_WR;
+		}
+
+		/* Is this Packet going to be an interrupt Packet? */
+		if ((--dma->tx_irq_en_cnt_curr) <= 0) {
+			if (dma->cfg.tx_irq_en_cnt == 0) {
+				/* IRQ is disabled.
+				 * A big number to avoid equal to zero too often 
+				 */
+				dma->tx_irq_en_cnt_curr = 0x3fffffff;
+			} else {
+				dma->tx_irq_en_cnt_curr = dma->cfg.tx_irq_en_cnt;
+				ctrl |= GRSPW_TXBD_IE;
+			}
+		}
+
+		/* Enable descriptor */
+		BD_WRITE(&curr_bd->bd->ctrl, ctrl);
+
+		last_pkt = curr_pkt;
+		curr_bd = curr_bd->next;
+		cnt++;
+
+		/* Get Next Packet from Ready Queue */
+		if (curr_pkt == dma->send.tail) {
+			/* Handled all in ready queue. */
+			curr_pkt = NULL;
+			break;
+		}
+		curr_pkt = curr_pkt->next;
+	}
+
+	/* Have Packets been scheduled? */
+	if (cnt > 0) {
+		/* Prepare list for insertion/deleation */
+		lst.tail = last_pkt;
+
+		/* Remove scheduled packets from ready queue */
+		grspw_list_remove_head_list(&dma->send, &lst);
+		dma->send_cnt -= cnt;
+		if (dma->stats.send_cnt_min > dma->send_cnt)
+			dma->stats.send_cnt_min = dma->send_cnt;
+
+		/* Insert scheduled packets into scheduled queue */
+		grspw_list_append_list(&dma->tx_sched, &lst);
+		dma->tx_sched_cnt += cnt;
+		if (dma->stats.tx_sched_cnt_max < dma->tx_sched_cnt)
+			dma->stats.tx_sched_cnt_max = dma->tx_sched_cnt;
+
+		/* Update TX ring posistion */
+		dma->tx_ring_head = curr_bd;
+
+		/* Make hardware aware of the newly enabled descriptors */
+		SPIN_LOCK_IRQ(&dma->core->devlock, irqflags);
+		dmactrl = REG_READ(&dma->regs->ctrl);
+		dmactrl &= ~(GRSPW_DMACTRL_PS|GRSPW_DMACTRL_PR|GRSPW_DMA_STATUS_ERROR);
+		dmactrl |= GRSPW_DMACTRL_TE;
+		REG_WRITE(&dma->regs->ctrl, dmactrl);
+		SPIN_UNLOCK_IRQ(&dma->core->devlock, irqflags);
+	}
+	return cnt;
+}
+
+/* Scans the TX desciptor table for transmitted packets, and moves these 
+ * packets from the head of the scheduled queue to the tail of the sent queue.
+ *
+ * Also, for all packets the status is updated.
+ *
+ *  - SCHED List -> SENT List
+ *
+ * Return Value
+ * Number of packet moved
+ */
+STATIC int grspw_tx_process_scheduled(struct grspw_dma_priv *dma)
+{
+	struct grspw_txring *curr;
+	struct grspw_pkt *last_pkt;
+	int sent_pkt_cnt = 0;
+	unsigned int ctrl;
+	struct grspw_list lst;
+
+	curr = dma->tx_ring_tail;
+
+	/* Step into TX ring to find if packets have been scheduled for 
+	 * transmission.
+	 */
+	if (!curr->pkt)
+		return 0; /* No scheduled packets, thus no sent, abort */
+
+	/* There has been Packets scheduled ==> scheduled Packets may have been
+	 * transmitted and needs to be collected into SENT List.
+	 *
+	 * A temporary list "lst" with all sent packets is created.
+	 */
+	lst.head = curr->pkt;
+
+	/* Loop until first enabled "un-transmitted" SpW Packet is found.
+	 * An unused descriptor is indicated by an unassigned pkt field.
+	 */
+	while (curr->pkt && !((ctrl=BD_READ(&curr->bd->ctrl)) & GRSPW_TXBD_EN)) {
+		/* Handle one sent Packet */
+
+		/* Remember last handled Packet so that insertion/removal from
+		 * packet lists go fast.
+		 */
+		last_pkt = curr->pkt;
+
+		/* Set flags to indicate error(s) and Mark Sent.
+		 */
+		last_pkt->flags = (last_pkt->flags & ~TXPKT_FLAG_OUTPUT_MASK) |
+					(ctrl & TXPKT_FLAG_LINKERR) |
+					TXPKT_FLAG_TX;
+
+		/* Sent packet experienced link error? */
+		if (ctrl & GRSPW_TXBD_LE)
+			dma->stats.tx_err_link++;
+
+		curr->pkt = NULL; /* Mark descriptor unused */
+
+		/* Increment */
+		curr = curr->next;
+		sent_pkt_cnt++;
+	}
+
+	/* 1. Remove all handled packets from TX-SCHED queue
+	 * 2. Put all handled packets into SENT queue
+	 */
+	if (sent_pkt_cnt > 0) {
+		/* Update Stats, Number of Transmitted Packets */
+		dma->stats.tx_pkts += sent_pkt_cnt;
+
+		/* Save TX ring posistion */
+		dma->tx_ring_tail = curr;
+
+		/* Prepare list for insertion/deleation */
+		lst.tail = last_pkt;
+
+		/* Remove sent packets from TX-SCHED queue */
+		grspw_list_remove_head_list(&dma->tx_sched, &lst);
+		dma->tx_sched_cnt -= sent_pkt_cnt;
+		if (dma->stats.tx_sched_cnt_min > dma->tx_sched_cnt)
+			dma->stats.tx_sched_cnt_min = dma->tx_sched_cnt;
+
+		/* Insert received packets into SENT queue */
+		grspw_list_append_list(&dma->sent, &lst);
+		dma->sent_cnt += sent_pkt_cnt;
+		if (dma->stats.sent_cnt_max < dma->sent_cnt)
+			dma->stats.sent_cnt_max = dma->sent_cnt;
+	}
+
+	return sent_pkt_cnt;
+}
+
+void *grspw_dma_open(void *d, int chan_no)
+{
+	struct grspw_priv *priv = d;
+	struct grspw_dma_priv *dma;
+	int size;
+	int semrc;
+
+	if ((chan_no < 0) || (priv->hwsup.ndma_chans <= chan_no))
+		return NULL;
+
+	dma = &priv->dma[chan_no];
+
+	/* Take GRSPW lock */
+	while ( (semrc=down_interruptible(&grspw_sem)) )
+		printk(KERN_WARNING "grspw_dma_open: down_interruptible %d\n", semrc);
+
+	if (dma->open) {
+		dma = NULL;
+		goto out;
+	}
+
+	dma->started = 0;
+
+	/* Set Default Configuration:
+	 *
+	 *  - MAX RX Packet Length = 
+	 *  - Disable IRQ generation
+	 *  -
+	 */
+	dma->cfg.rxmaxlen = DEFAULT_RXMAX;
+	dma->cfg.rx_irq_en_cnt = 0;
+	dma->cfg.tx_irq_en_cnt = 0;
+	dma->cfg.flags = DMAFLAG_NO_SPILL;
+
+	init_MUTEX(&dma->sem_rxdma);
+	init_MUTEX(&dma->sem_txdma);
+
+	/* Allocate memory for the two descriptor rings */
+	size = sizeof(struct grspw_ring) * (GRSPW_RXBD_NR + GRSPW_TXBD_NR);
+	dma->rx_ring_base = (struct grspw_rxring *)kmalloc(size, GFP_KERNEL);
+	if (!dma->rx_ring_base) {
+		dma = NULL;
+		goto out;
+	}
+	dma->tx_ring_base = (struct grspw_txring *)&dma->rx_ring_base[GRSPW_RXBD_NR];
+
+	/* Reset software structures */
+	grspw_dma_reset(dma);
+
+	/* Take the device */
+	dma->open = 1;
+out:
+	/* Return GRSPW Lock */
+	up(&grspw_sem);
+
+	return dma;
+}
+EXPORT_SYMBOL(grspw_dma_open);
+
+/* Initialize Software Structures:
+ *  - Clear all Queues 
+ *  - init BD ring 
+ *  - init IRQ counter
+ *  - clear statistics counters
+ *  - init wait structures and semaphores
+ */
+STATIC void grspw_dma_reset(struct grspw_dma_priv *dma)
+{
+	init_MUTEX_LOCKED(&dma->rx_wait.sem_wait);
+	init_MUTEX_LOCKED(&dma->tx_wait.sem_wait);
+
+	/* Empty RX and TX queues */
+	grspw_list_clr(&dma->ready);
+	grspw_list_clr(&dma->rx_sched);
+	grspw_list_clr(&dma->recv);
+	grspw_list_clr(&dma->send);
+	grspw_list_clr(&dma->tx_sched);
+	grspw_list_clr(&dma->sent);
+	dma->ready_cnt = 0;
+	dma->rx_sched_cnt = 0;
+	dma->recv_cnt = 0;
+	dma->send_cnt = 0;
+	dma->tx_sched_cnt = 0;
+	dma->sent_cnt = 0;
+
+	dma->rx_irq_en_cnt_curr = 0;
+	dma->tx_irq_en_cnt_curr = 0;
+
+	grspw_bdrings_init(dma);
+
+	dma->rx_wait.waiting = 0;
+	dma->tx_wait.waiting = 0;
+
+	grspw_dma_stats_clr(dma);
+}
+
+int grspw_dma_close(void *c)
+{
+	struct grspw_dma_priv *dma = c;
+	int semrc;
+
+	if (!dma->open)
+		return 0;
+
+	/* Take device lock - Wait until we get semaphore */
+	while ( (semrc=down_interruptible(&dma->sem_rxdma)) )
+		printk(KERN_WARNING "grspw_dma_close: down_interruptible %d\n", semrc);
+	while ( (semrc=down_interruptible(&dma->sem_txdma)) )
+		printk(KERN_WARNING "grspw_dma_close: down_interruptible %d\n", semrc);
+
+	/* Can not close active DMA channel. User must stop DMA and make sure
+	 * no threads are active/blocked within driver.
+	 */
+	if (dma->started || dma->rx_wait.waiting || dma->tx_wait.waiting) {
+		up(&dma->sem_txdma);
+		up(&dma->sem_rxdma);
+		return 1;
+	}
+
+	/* Free memory */
+	if (dma->rx_ring_base)
+		kfree(dma->rx_ring_base);
+	dma->rx_ring_base = NULL;
+	dma->tx_ring_base = NULL;
+
+	dma->open = 0;
+	return 0;
+}
+EXPORT_SYMBOL(grspw_dma_close);
+
+/* Schedule List of packets for transmission at some point in
+ * future.
+ *
+ * 1. Move transmitted packets to SENT List (SCHED->SENT)
+ * 2. Add the requested packets to the SEND List (USER->SEND)
+ * 3. Schedule as many packets as possible (SEND->SCHED)
+ */
+int grspw_dma_tx_send(void *c, int opts, struct grspw_list *pkts, int count)
+{
+	struct grspw_dma_priv *dma = c;
+	int ret;
+	int semrc;
+
+	/* Take device lock */
+	while ( (semrc=down_interruptible(&dma->sem_txdma)) )
+		printk(KERN_WARNING "grspw_dma_tx_send: down_interruptible %d\n", semrc);
+
+	if (dma->started == 0) {
+		ret = 1; /* signal DMA has been stopped */
+		goto out;
+	}
+	ret = 0;
+
+	/* 1. Move transmitted packets to SENT List (SCHED->SENT) */
+	if ((opts & 1) == 0)
+		grspw_tx_process_scheduled(dma);
+
+	/* 2. Add the requested packets to the SEND List (USER->SEND) */
+	if (pkts) {
+		grspw_list_append_list(&dma->send, pkts);
+		dma->send_cnt += count;
+		if (dma->stats.send_cnt_max < dma->send_cnt)
+			dma->stats.send_cnt_max = dma->send_cnt;
+	}
+
+	/* 3. Schedule as many packets as possible (SEND->SCHED) */
+	if ((opts & 2) == 0)
+		grspw_tx_schedule_send(dma);
+
+out:
+	/* Unlock Device */
+	up(&dma->sem_txdma);
+
+	return ret;
+}
+EXPORT_SYMBOL(grspw_dma_tx_send);
+
+int grspw_dma_tx_reclaim(void *c, int opts, struct grspw_list *pkts, int *count)
+{
+	struct grspw_dma_priv *dma = c;
+	struct grspw_pkt *pkt, *lastpkt;
+	int cnt, started;
+	int semrc;
+
+	/* Take device lock */
+	while ( (semrc=down_interruptible(&dma->sem_txdma)) )
+		printk(KERN_WARNING "grspw_dma_tx_reclaim: down_interruptible %d\n", semrc);
+
+	/* 1. Move transmitted packets to SENT List (SCHED->SENT) */
+	started = dma->started;
+	if ((started > 0) && ((opts & 1) == 0))
+		grspw_tx_process_scheduled(dma);
+
+	/* Move all/count SENT packet to the callers list (SENT->USER) */
+	if (pkts) {
+		if ((count == NULL) || (*count == -1) ||
+		    (*count >= dma->sent_cnt)) {
+			/* Move all SENT Packets */
+			*pkts = dma->sent;
+			grspw_list_clr(&dma->sent);
+			if (count)
+				*count = dma->sent_cnt;
+			dma->sent_cnt = 0;
+		} else {
+			/* Move a number of SENT Packets */
+			pkts->head = pkt = lastpkt = dma->sent.head;
+			cnt = 0;
+			while (cnt < *count) {
+				lastpkt = pkt;
+				pkt = pkt->next;
+				cnt++;
+			}
+			if (cnt > 0) {
+				pkts->tail = lastpkt;
+				grspw_list_remove_head_list(&dma->sent, pkts);
+				dma->sent_cnt -= cnt;
+			} else {
+				grspw_list_clr(pkts);
+			}
+		}
+	} else if (count) {
+		*count = 0;
+	}
+
+	/* 3. Schedule as many packets as possible (SEND->SCHED) */
+	if ((started > 0) && ((opts & 2) == 0))
+		grspw_tx_schedule_send(dma);
+
+	/* Unlock Device */
+	up(&dma->sem_txdma);
+
+	return (~started) & 1; /* signal DMA has been stopped */
+}
+EXPORT_SYMBOL(grspw_dma_tx_reclaim);
+
+void grspw_dma_tx_count(void *c, int *send, int *sched, int *sent, int *hw)
+{
+	struct grspw_dma_priv *dma = c;
+	int sched_cnt, diff, semrc;
+	unsigned int hwbd;
+	struct grspw_txbd *tailbd;
+
+	/* Take device lock - Wait until we get semaphore.
+	 * The lock is taken so that the counters are in sync with each other
+	 * and that DMA descriptor table and tx_ring_tail is not being updated
+	 * during HW counter processing in this function.
+	 */
+	while ( (semrc=down_interruptible(&dma->sem_txdma)) )
+		printk(KERN_WARNING "grspw_dma_tx_count: down_interruptible %d\n", semrc);
+
+	if (send)
+		*send = dma->send_cnt;
+	sched_cnt = dma->tx_sched_cnt;
+	if (sched)
+		*sched = sched_cnt;
+	if (sent)
+		*sent = dma->sent_cnt;
+	if (hw) {
+		/* Calculate number of descriptors (processed by HW) between
+		 * HW pointer and oldest SW pointer.
+		 */
+		hwbd = REG_READ(&dma->regs->txdesc);
+		tailbd = dma->tx_ring_tail->bd;
+		diff = ((hwbd - (unsigned long)tailbd) / GRSPW_TXBD_SIZE) &
+			(GRSPW_TXBD_NR - 1);
+		/* Handle special case when HW and SW pointers are equal
+		 * because all TX descriptors have been processed by HW.
+		 */
+		if ((diff == 0) && (sched_cnt == GRSPW_TXBD_NR) &&
+		    ((BD_READ(&tailbd->ctrl) & GRSPW_TXBD_EN) == 0)) {
+			diff = GRSPW_TXBD_NR;
+		}
+		*hw = diff;
+	}
+
+	/* Unlock DMA channel */
+	up(&dma->sem_txdma);
+}
+EXPORT_SYMBOL(grspw_dma_tx_count);
+
+static inline int grspw_tx_wait_eval(struct grspw_dma_priv *dma)
+{
+	int send_val, sent_val;
+
+	if (dma->tx_wait.send_cnt >= (dma->send_cnt + dma->tx_sched_cnt))
+		send_val = 1;
+	else
+		send_val = 0;
+
+	if (dma->tx_wait.sent_cnt <= dma->sent_cnt)
+		sent_val = 1;
+	else
+		sent_val = 0;
+
+	/* AND or OR ? */
+	if (dma->tx_wait.op == 0)
+		return send_val & sent_val; /* AND */
+	else
+		return send_val | sent_val; /* OR */
+}
+
+/* Block until send_cnt or fewer packets are Queued in "Send and Scheduled" Q,
+ * op (AND or OR), sent_cnt or more packet "have been sent" (Sent Q) condition
+ * is met.
+ * If a link error occurs and the Stop on Link error is defined, this function
+ * will also return to caller.
+ */
+int grspw_dma_tx_wait(void *c, int send_cnt, int op, int sent_cnt, long timeout)
+{
+	struct grspw_dma_priv *dma = c;
+	int ret, rc, initialized = 0;
+	int semrc;
+	long long jiffies_64_end = 0;
+	long left;
+
+	if (timeout > 0)
+		jiffies_64_end = jiffies_64 + timeout;
+
+check_condition:
+
+	/* Take DMA channel lock */
+	while ( (semrc=down_interruptible(&dma->sem_txdma)) )
+		printk(KERN_WARNING "grspw_dma_tx_wait: down_interruptible %d\n", semrc);
+
+	/* Check so that no other thread is waiting, this driver only supports
+	 * one waiter at a time.
+	 */
+	if (initialized == 0 && dma->tx_wait.waiting) {
+		ret = 3;
+		goto out_release;
+	}
+
+	/* Stop if link error or similar (DMA stopped), abort */
+	if (dma->started == 0) {
+		ret = 1;
+		goto out_release;
+	}
+
+	/* Set up Condition */
+	dma->tx_wait.send_cnt = send_cnt;
+	dma->tx_wait.op = op;
+	dma->tx_wait.sent_cnt = sent_cnt;
+
+	if (grspw_tx_wait_eval(dma) == 0) {
+		/* Prepare Wait */
+		initialized = 1;
+		dma->tx_wait.waiting = 1;
+
+		/* Release device lock */
+		up(&dma->sem_txdma);
+
+		/* Try to take Wait lock */
+		if (timeout > 0) {
+			if (jiffies_64_end <= jiffies_64) {
+				ret = 2;
+				goto out;
+			}
+			left = jiffies_64_end - jiffies_64;
+			rc = down_timeout(&dma->tx_wait.sem_wait, left);
+			if (rc == -ETIME) {
+				ret = 2;
+				goto out;
+			}
+		} else {
+			rc = down_interruptible(&dma->tx_wait.sem_wait);
+		}
+		if (rc != 0) {
+			/* Unknown Error */
+			printk(KERN_WARNING "grspw_dma_tx_wait: sem down failed %d\n", rc);
+			ret = -1;
+			goto out;
+		} else if (dma->started == 0) {
+			ret = 1;
+			goto out;
+		}
+
+		/* Check condition once more */
+		goto check_condition;
+	}
+
+	ret = 0;
+
+out_release:
+	/* Unlock DMA channel */
+	up(&dma->sem_txdma);
+
+out:
+	if (initialized)
+		dma->tx_wait.waiting = 0;
+	return ret;
+}
+EXPORT_SYMBOL(grspw_dma_tx_wait);
+
+int grspw_dma_rx_recv(void *c, int opts, struct grspw_list *pkts, int *count)
+{
+	struct grspw_dma_priv *dma = c;
+	struct grspw_pkt *pkt, *lastpkt;
+	int cnt, started;
+	int semrc;
+
+	/* Take device lock */
+	while ( (semrc=down_interruptible(&dma->sem_rxdma)) )
+		printk(KERN_WARNING "grspw_dma_rx_recv: down_interruptible %d\n", semrc);
+
+	/* 1. Move Scheduled packets to RECV List (SCHED->RECV) */
+	started = dma->started;
+	if (((opts & 1) == 0) && (started > 0))
+		grspw_rx_process_scheduled(dma);
+
+	/* Move all RECV packet to the callers list */
+	if (pkts) {
+		if ((count == NULL) || (*count == -1) ||
+		    (*count >= dma->recv_cnt)) {
+			/* Move all Received packets */
+			*pkts = dma->recv;
+			grspw_list_clr(&dma->recv);
+			if ( count )
+				*count = dma->recv_cnt;
+			dma->recv_cnt = 0;
+		} else {
+			/* Move a number of RECV Packets */
+			pkts->head = pkt = lastpkt = dma->recv.head;
+			cnt = 0;
+			while (cnt < *count) {
+				lastpkt = pkt;
+				pkt = pkt->next;
+				cnt++;
+			}
+			if (cnt > 0) {
+				pkts->tail = lastpkt;
+				grspw_list_remove_head_list(&dma->recv, pkts);
+				dma->recv_cnt -= cnt;
+			} else {
+				grspw_list_clr(pkts);
+			}
+		}
+	} else if (count) {
+		*count = 0;
+	}
+
+	/* 3. Schedule as many free packet buffers as possible (READY->SCHED) */
+	if (((opts & 2) == 0) && (started > 0))
+		grspw_rx_schedule_ready(dma);
+
+	/* Unlock Device */
+	up(&dma->sem_rxdma);
+
+	return (~started) & 1;
+}
+EXPORT_SYMBOL(grspw_dma_rx_recv);
+
+int grspw_dma_rx_prepare(void *c, int opts, struct grspw_list *pkts, int count)
+{
+	struct grspw_dma_priv *dma = c;
+	int ret;
+	int semrc;
+
+	/* Take device lock */
+	while ( (semrc=down_interruptible(&dma->sem_rxdma)) )
+		printk(KERN_WARNING "grspw_dma_rx_prepare: down_interruptible %d\n", semrc);
+
+	if (dma->started == 0) {
+		ret = 1;
+		goto out;
+	}
+
+	/* 1. Move Received packets to RECV List (SCHED->RECV) */
+	if ((opts & 1) == 0)
+		grspw_rx_process_scheduled(dma);
+
+	/* 2. Add the "free/ready" packet buffers to the READY List (USER->READY) */
+	if (pkts && (count > 0)) {
+		grspw_list_append_list(&dma->ready, pkts);
+		dma->ready_cnt += count;
+		if (dma->stats.ready_cnt_max < dma->ready_cnt)
+			dma->stats.ready_cnt_max = dma->ready_cnt;
+	}
+
+	/* 3. Schedule as many packets as possible (READY->SCHED) */
+	if ((opts & 2) == 0)
+		grspw_rx_schedule_ready(dma);
+
+	ret = 0;
+out:
+	/* Unlock Device */
+	up(&dma->sem_rxdma);
+
+	return ret;
+}
+EXPORT_SYMBOL(grspw_dma_rx_prepare);
+
+void grspw_dma_rx_count(void *c, int *ready, int *sched, int *recv, int *hw)
+{
+	struct grspw_dma_priv *dma = c;
+	int sched_cnt, diff, semrc;
+	unsigned int hwbd;
+	struct grspw_rxbd *tailbd;
+
+	/* Take device lock - Wait until we get semaphore.
+	 * The lock is taken so that the counters are in sync with each other
+	 * and that DMA descriptor table and rx_ring_tail is not being updated
+	 * during HW counter processing in this function.
+	 */
+	while ( (semrc=down_interruptible(&dma->sem_rxdma)) )
+		printk(KERN_WARNING "grspw_dma_rx_count: down_interruptible %d\n", semrc);
+
+	if (ready)
+		*ready = dma->ready_cnt;
+	sched_cnt = dma->rx_sched_cnt;
+	if (sched)
+		*sched = sched_cnt;
+	if (recv)
+		*recv = dma->recv_cnt;
+	if (hw) {
+		/* Calculate number of descriptors (processed by HW) between
+		 * HW pointer and oldest SW pointer.
+		 */
+		hwbd = REG_READ(&dma->regs->rxdesc);
+		tailbd = dma->rx_ring_tail->bd;
+		diff = ((hwbd - (unsigned long)tailbd) / GRSPW_RXBD_SIZE) &
+			(GRSPW_RXBD_NR - 1);
+		/* Handle special case when HW and SW pointers are equal
+		 * because all RX descriptors have been processed by HW.
+		 */
+		if ((diff == 0) && (sched_cnt == GRSPW_RXBD_NR) &&
+		    ((BD_READ(&tailbd->ctrl) & GRSPW_RXBD_EN) == 0)) {
+			diff = GRSPW_RXBD_NR;
+		}
+		*hw = diff;
+	}
+
+	/* Unlock DMA channel */
+	up(&dma->sem_rxdma);
+}
+EXPORT_SYMBOL(grspw_dma_rx_count);
+
+static inline int grspw_rx_wait_eval(struct grspw_dma_priv *dma)
+{
+	int ready_val, recv_val;
+
+	if (dma->rx_wait.ready_cnt >= (dma->ready_cnt + dma->rx_sched_cnt))
+		ready_val = 1;
+	else
+		ready_val = 0;
+
+	if (dma->rx_wait.recv_cnt <= dma->recv_cnt)
+		recv_val = 1;
+	else
+		recv_val = 0;
+
+	/* AND or OR ? */
+	if (dma->rx_wait.op == 0)
+		return ready_val & recv_val; /* AND */
+	else
+		return ready_val | recv_val; /* OR */
+}
+
+/* Block until recv_cnt or more packets are Queued in RECV Q, op (AND or OR), 
+ * ready_cnt or fewer packet buffers are available in the "READY and Scheduled" Q,
+ * condition is met.
+ * If a link error occurs and the Stop on Link error is defined, this function
+ * will also return to caller, however with an error.
+ */
+int grspw_dma_rx_wait(void *c, int recv_cnt, int op, int ready_cnt, long timeout)
+{
+	struct grspw_dma_priv *dma = c;
+	int ret, rc, initialized = 0;
+	int semrc;
+	long long jiffies_64_end = 0;
+	long left;
+
+	if (timeout > 0)
+		jiffies_64_end = jiffies_64 + timeout;
+
+check_condition:
+
+	/* Take device lock */
+	while ( (semrc=down_interruptible(&dma->sem_rxdma)) )
+		printk(KERN_WARNING "grspw_dma_rx_wait: down_interruptible %d\n", semrc);
+
+	/* Check so that no other thread is waiting, this driver only supports
+	 * one waiter at a time.
+	 */
+	if (initialized == 0 && dma->rx_wait.waiting) {
+		ret = 3;
+		goto out_release;
+	}
+
+	/* Stop if link error or similar (DMA stopped), abort */
+	if (dma->started == 0) {
+		ret = 1;
+		goto out_release;
+	}
+
+	/* Set up Condition */
+	dma->rx_wait.recv_cnt = recv_cnt;
+	dma->rx_wait.op = op;
+	dma->rx_wait.ready_cnt = ready_cnt;
+
+	if (grspw_rx_wait_eval(dma) == 0) {
+		/* Prepare Wait */
+		initialized = 1;
+		dma->rx_wait.waiting = 1;
+
+		/* Release device lock */
+		up(&dma->sem_rxdma);
+
+		/* Try to take Wait lock */
+		if (timeout > 0) {
+			if (jiffies_64_end <= jiffies_64) {
+			    	ret = 2;
+				goto out;
+			}
+			left = jiffies_64_end - jiffies_64;
+			rc = down_timeout(&dma->rx_wait.sem_wait, left);
+			if (rc == -ETIME) {
+				ret = 2;
+				goto out;
+			}
+		} else {
+			rc = down_interruptible(&dma->rx_wait.sem_wait);
+		}
+		if (rc != 0) {
+			/* Unknown Error */
+			printk(KERN_WARNING "grspw_dma_rx_wait: sem down failed %d\n", rc);
+			ret = -1;
+			goto out;
+		} else if (dma->started == 0) {
+			ret = 1;
+			goto out;
+		}
+
+		/* Check condition once more */
+		goto check_condition;
+	}
+
+	ret = 0;
+
+out_release:
+	/* Unlock DMA channel */
+	up(&dma->sem_rxdma);
+
+out:
+	if (initialized)
+		dma->rx_wait.waiting = 0;
+	return ret;
+}
+EXPORT_SYMBOL(grspw_dma_rx_wait);
+
+int grspw_dma_config(void *c, struct grspw_dma_config *cfg)
+{
+	struct grspw_dma_priv *dma = c;
+
+	if (dma->started || !cfg)
+		return -1;
+
+	if (cfg->flags & ~(DMAFLAG_MASK | DMAFLAG2_MASK))
+		return -1;
+
+	/* Update Configuration */
+	memcpy(&dma->cfg, cfg, sizeof(*cfg));
+
+	return 0;
+}
+EXPORT_SYMBOL(grspw_dma_config);
+
+void grspw_dma_config_read(void *c, struct grspw_dma_config *cfg)
+{
+	struct grspw_dma_priv *dma = c;
+
+	/* Copy Current Configuration */
+	memcpy(cfg, &dma->cfg, sizeof(*cfg));
+}
+EXPORT_SYMBOL(grspw_dma_config_read);
+
+void grspw_dma_stats_read(void *c, struct grspw_dma_stats *sts)
+{
+	struct grspw_dma_priv *dma = c;
+
+	memcpy(sts, &dma->stats, sizeof(dma->stats));
+}
+EXPORT_SYMBOL(grspw_dma_stats_read);
+
+void grspw_dma_stats_clr(void *c)
+{
+	struct grspw_dma_priv *dma = c;
+
+	/* Clear most of the statistics */	
+	memset(&dma->stats, 0, sizeof(dma->stats));
+
+	/* Init proper default values so that comparisons will work the
+	 * first time.
+	 */
+	dma->stats.send_cnt_min = 0x3fffffff;
+	dma->stats.tx_sched_cnt_min = 0x3fffffff;
+	dma->stats.ready_cnt_min = 0x3fffffff;
+	dma->stats.rx_sched_cnt_min = 0x3fffffff;
+}
+EXPORT_SYMBOL(grspw_dma_stats_clr);
+
+int grspw_dma_start(void *c)
+{
+	struct grspw_dma_priv *dma = c;
+	struct grspw_dma_regs *dregs = dma->regs;
+	unsigned int ctrl;
+	IRQFLAGS_TYPE irqflags;
+
+	if (dma->started)
+		return 0;
+
+	/* Initialize Software Structures:
+	 *  - Clear all Queues
+	 *  - init BD ring 
+	 *  - init IRQ counter
+	 *  - clear statistics counters
+	 *  - init wait structures and semaphores
+	 */
+	grspw_dma_reset(dma);
+
+	/* RX&RD and TX is not enabled until user fills SEND and READY Queue
+	 * with SpaceWire Packet buffers. So we do not have to worry about
+	 * IRQs for this channel just yet. However other DMA channels
+	 * may be active.
+	 *
+	 * Some functionality that is not changed during started mode is set up
+	 * once and for all here:
+	 *
+	 *   - RX MAX Packet length
+	 *   - TX Descriptor base address to first BD in TX ring (not enabled)
+	 *   - RX Descriptor base address to first BD in RX ring (not enabled)
+	 *   - IRQs (TX DMA, RX DMA, DMA ERROR)
+	 *   - Strip PID
+	 *   - Strip Address
+	 *   - No Spill
+	 *   - Receiver Enable
+	 *   - disable on link error (LE)
+	 *
+	 * Note that the address register and the address enable bit in DMACTRL
+	 * register must be left untouched, they are configured on a GRSPW
+	 * core level.
+	 *
+	 * Note that the receiver is enabled here, but since descriptors are
+	 * not enabled the GRSPW core may stop/pause RX (if NS bit set) until
+	 * descriptors are enabled or it may ignore RX packets (NS=0) until
+	 * descriptors are enabled (writing RD bit).
+	 */
+	REG_WRITE(&dregs->txdesc, dma->tx_bds_pa);
+	REG_WRITE(&dregs->rxdesc, dma->rx_bds_pa);
+
+	/* MAX Packet length */
+	REG_WRITE(&dma->regs->rxmax, dma->cfg.rxmaxlen);
+
+	ctrl =  GRSPW_DMACTRL_AI | GRSPW_DMACTRL_PS | GRSPW_DMACTRL_PR |
+		GRSPW_DMACTRL_TA | GRSPW_DMACTRL_RA | GRSPW_DMACTRL_RE |
+		(dma->cfg.flags & DMAFLAG_MASK) << GRSPW_DMACTRL_NS_BIT;
+	if (dma->core->dis_link_on_err & LINKOPTS_DIS_ONERR)
+		ctrl |= GRSPW_DMACTRL_LE;
+	if (dma->cfg.rx_irq_en_cnt != 0 || dma->cfg.flags & DMAFLAG2_RXIE)
+		ctrl |= GRSPW_DMACTRL_RI;
+	if (dma->cfg.tx_irq_en_cnt != 0 || dma->cfg.flags & DMAFLAG2_TXIE)
+		ctrl |= GRSPW_DMACTRL_TI;
+	SPIN_LOCK_IRQ(&dma->core->devlock, irqflags);
+	ctrl |= REG_READ(&dregs->ctrl) & GRSPW_DMACTRL_EN;
+	REG_WRITE(&dregs->ctrl, ctrl);
+	SPIN_UNLOCK_IRQ(&dma->core->devlock, irqflags);
+
+	dma->started = 1; /* open up other DMA interfaces */
+
+	return 0;
+}
+EXPORT_SYMBOL(grspw_dma_start);
+
+static void grspw_dma_stop_locked(struct grspw_dma_priv *dma)
+{
+	struct grspw_priv *priv = dma->core;
+	IRQFLAGS_TYPE irqflags;
+
+	if (dma->started == 0)
+		return;
+	dma->started = 0;
+
+	SPIN_LOCK_IRQ(&priv->devlock, irqflags);
+	grspw_hw_dma_stop(dma);
+	SPIN_UNLOCK_IRQ(&priv->devlock, irqflags);
+
+	/* From here no more packets will be sent, however
+	 * there may still exist scheduled packets that has been
+	 * sent, and packets in the SEND Queue waiting for free
+	 * descriptors. All packets are moved to the SENT Queue
+	 * so that the user may get its buffers back, the user
+	 * must look at the TXPKT_FLAG_TX in order to determine
+	 * if the packet was sent or not.
+	 */
+
+	/* Retreive scheduled all sent packets */
+	grspw_tx_process_scheduled(dma);
+
+	/* Move un-sent packets in SEND and SCHED queue to the
+	 * SENT Queue. (never marked sent)
+	 */
+	if (!grspw_list_is_empty(&dma->tx_sched)) {
+		grspw_list_append_list(&dma->sent, &dma->tx_sched);
+		grspw_list_clr(&dma->tx_sched);
+		dma->sent_cnt += dma->tx_sched_cnt;
+		dma->tx_sched_cnt = 0;
+	}
+	if (!grspw_list_is_empty(&dma->send)) {
+		grspw_list_append_list(&dma->sent, &dma->send);
+		grspw_list_clr(&dma->send);
+		dma->sent_cnt += dma->send_cnt;
+		dma->send_cnt = 0;
+	}
+
+	/* Similar for RX */
+	grspw_rx_process_scheduled(dma);
+	if (!grspw_list_is_empty(&dma->rx_sched)) {
+		grspw_list_append_list(&dma->recv, &dma->rx_sched);
+		grspw_list_clr(&dma->rx_sched);
+		dma->recv_cnt += dma->rx_sched_cnt;
+		dma->rx_sched_cnt = 0;
+	}
+	if (!grspw_list_is_empty(&dma->ready)) {
+		grspw_list_append_list(&dma->recv, &dma->ready);
+		grspw_list_clr(&dma->ready);
+		dma->recv_cnt += dma->ready_cnt;
+		dma->ready_cnt = 0;
+	}
+
+	/* Throw out blocked threads */
+	if (dma->rx_wait.waiting)
+		up(&dma->rx_wait.sem_wait);
+	if (dma->tx_wait.waiting)
+		up(&dma->tx_wait.sem_wait);
+}
+
+void grspw_dma_stop(void *c)
+{
+	struct grspw_dma_priv *dma = c;
+	int semrc;
+
+	/* If DMA channel is closed we should not access the semaphore */
+	if (!dma->open)
+		return;
+
+	/* Take DMA Channel lock */
+	while ( (semrc=down_interruptible(&dma->sem_rxdma)) )
+			printk(KERN_WARNING "grspw_dma_stop: down_interruptible %d\n", semrc);
+	while ( (semrc=down_interruptible(&dma->sem_txdma)) )
+			printk(KERN_WARNING "grspw_dma_stop: down_interruptible %d\n", semrc);
+
+	grspw_dma_stop_locked(dma);
+
+	up(&dma->sem_txdma);
+	up(&dma->sem_rxdma);
+}
+EXPORT_SYMBOL(grspw_dma_stop);
+
+/* Do general work, invoked indirectly from ISR */
+static void grspw_work_func(struct work_struct *work)
+{
+	struct grspw_priv *priv = container_of(work, struct grspw_priv, work);
+	int i;
+
+	/* Link is down for some reason, and the user has configured
+	 * that we stop all (open) DMA channels and throw out all their
+	 * blocked threads.
+	 */
+	for (i=0; i<priv->hwsup.ndma_chans; i++)
+		grspw_dma_stop(&priv->dma[i]);
+	grspw_hw_stop(priv);
+}
+
+/* Do DMA work on one channel, invoked indirectly from ISR */
+static void grspw_work_dma_func(struct work_struct *work)
+{
+	struct grspw_dma_priv *dma = container_of(work, struct grspw_dma_priv, work);
+	struct grspw_priv *priv = dma->core;
+	int tx_cond_true, rx_cond_true;
+	unsigned int ctrl;
+	int semrc;
+	IRQFLAGS_TYPE irqflags;
+
+	/* If DMA channel is closed we should not access the semaphore */
+	if (dma->open == 0)
+		return;
+
+	rx_cond_true = 0;
+	tx_cond_true = 0;
+	dma->stats.irq_cnt++;
+
+	/* Look at cause we were woken up and clear source */
+	SPIN_LOCK_IRQ(&priv->devlock, irqflags);
+	if (dma->started == 0) {
+		SPIN_UNLOCK_IRQ(&priv->devlock, irqflags);
+		return;
+	}
+	ctrl = REG_READ(&dma->regs->ctrl);
+
+	/* Read/Write DMA error ? */
+	if (ctrl & GRSPW_DMA_STATUS_ERROR) {
+		/* DMA error -> Stop DMA channel (both RX and TX) */
+		SPIN_UNLOCK_IRQ(&priv->devlock, irqflags);
+		grspw_dma_stop(dma);
+	} else if (ctrl & (GRSPW_DMACTRL_PR | GRSPW_DMACTRL_PS)) {
+		/* DMA has finished a TX/RX packet
+		 * Clear Source.
+		 */
+		ctrl &= ~GRSPW_DMACTRL_AT;
+		if (dma->cfg.rx_irq_en_cnt != 0 ||
+		    (dma->cfg.flags & DMAFLAG2_RXIE))
+			ctrl |= GRSPW_DMACTRL_RI;
+		if (dma->cfg.tx_irq_en_cnt != 0 ||
+		    (dma->cfg.flags & DMAFLAG2_TXIE))
+			ctrl |= GRSPW_DMACTRL_TI;
+		REG_WRITE(&dma->regs->ctrl, ctrl);
+		SPIN_UNLOCK_IRQ(&priv->devlock, irqflags);
+		if (ctrl & GRSPW_DMACTRL_PR) {
+			/* Do RX Work */
+
+			/* Take DMA Channel RX lock */
+			while ( (semrc=down_interruptible(&dma->sem_rxdma)) )
+				printk(KERN_WARNING "grspw_work_dma_func: down_interruptible %d\n", semrc);		
+
+			dma->stats.rx_work_cnt++;
+			grspw_rx_process_scheduled(dma);
+			if (dma->started) {
+				dma->stats.rx_work_enabled +=
+					grspw_rx_schedule_ready(dma);
+				/* Check to see if condition for waking blocked
+			 	 * USER task is fullfilled.
+				 */
+				if (dma->rx_wait.waiting)
+					rx_cond_true = grspw_rx_wait_eval(dma);
+			}
+			up(&dma->sem_rxdma);
+		}
+		if (ctrl & GRSPW_DMACTRL_PS) {
+			/* Do TX Work */
+
+			/* Take DMA Channel TX lock */
+			while ( (semrc=down_interruptible(&dma->sem_txdma)) )
+				printk(KERN_WARNING "grspw_work_dma_func: down_interruptible %d\n", semrc);
+
+			dma->stats.tx_work_cnt++;
+			grspw_tx_process_scheduled(dma);
+			if (dma->started) {
+				dma->stats.tx_work_enabled +=
+					grspw_tx_schedule_send(dma);
+				/* Check to see if condition for waking blocked
+			 	 * USER task is fullfilled.
+				 */
+				if (dma->tx_wait.waiting)
+					tx_cond_true = grspw_tx_wait_eval(dma);
+			}
+			up(&dma->sem_txdma);
+		}
+	} else
+		SPIN_UNLOCK_IRQ(&priv->devlock, irqflags);
+
+	if (rx_cond_true)
+		up(&dma->rx_wait.sem_wait);
+
+	if (tx_cond_true)
+		up(&dma->tx_wait.sem_wait);
+}
+
+static irqreturn_t grspw_isr(int irq, void *data)
+{
+	struct grspw_priv *priv = data;
+	unsigned int dma_stat, stat, stat_clrmsk, ctrl, timecode;
+	int i;
+	irqreturn_t ret = 0;
+
+	/* Get Status from Hardware */
+	stat = REG_READ(&priv->regs->status);
+	stat_clrmsk = stat & (GRSPW_STS_TO | GRSPW_STAT_ERROR) & priv->stscfg;
+
+	/* Make sure to put the timecode handling first in order to get the
+	 * smallest possible interrupt latency
+	 */
+	if ((stat & GRSPW_STS_TO) && (priv->tcisr != NULL)) {
+		ctrl = REG_READ(&priv->regs->ctrl);
+		if (ctrl & GRSPW_CTRL_TQ) {
+			/* Timecode received. Let custom function handle this */
+			timecode = REG_READ(&priv->regs->time) &
+					(GRSPW_TIME_CTRL | GRSPW_TIME_TCNT);
+			(priv->tcisr)(priv->tcisr_arg, timecode);
+		}
+	}
+
+	/* An Error occured? */
+	if (stat & GRSPW_STAT_ERROR) {
+		/* Wake Global WorkQ */
+		ret = IRQ_HANDLED;
+
+		if (stat & GRSPW_STS_EE)
+			priv->stats.err_eeop++;
+
+		if (stat & GRSPW_STS_IA)
+			priv->stats.err_addr++;
+
+		if (stat & GRSPW_STS_PE)
+			priv->stats.err_parity++;
+
+		if (stat & GRSPW_STS_DE)
+			priv->stats.err_disconnect++;
+
+		if (stat & GRSPW_STS_ER)
+			priv->stats.err_escape++;
+
+		if (stat & GRSPW_STS_CE)
+			priv->stats.err_credit++;
+
+		if (stat & GRSPW_STS_WE)
+			priv->stats.err_wsync++;
+
+		if ((priv->dis_link_on_err >> 16) & stat) {
+			/* Disable the link, no more transfers are expected
+			 * on any DMA channel.
+			 */
+			SPIN_LOCK(&priv->devlock, irqflags);
+			ctrl = REG_READ(&priv->regs->ctrl);
+			REG_WRITE(&priv->regs->ctrl, GRSPW_CTRL_LD |
+				(ctrl & ~(GRSPW_CTRL_IE|GRSPW_CTRL_LS)));
+			SPIN_UNLOCK(&priv->devlock, irqflags);
+			/* Start Work to clean up */
+			queue_work(grspw_workq, &priv->work);
+		}
+	}
+
+	/* Clear Status Flags */
+	if (stat_clrmsk) {
+		ret = IRQ_HANDLED;
+		REG_WRITE(&priv->regs->status, stat_clrmsk);
+	}
+
+	/* A DMA transfer or Error occured? In that case disable more IRQs
+	 * from the DMA channel, then invoke the workQ.
+	 *
+	 * The far most common HW setup is one DMA channel, so we optimize
+	 * it. Also the GI interrupt flag may not be available for older
+	 * designs where (was added together with mutiple DMA channels).
+	 */
+	SPIN_LOCK(&priv->devlock, irqflags);
+	if ( priv->hwsup.ndma_chans == 1 ) {
+		dma_stat = REG_READ(&priv->regs->dma[0].ctrl);
+		/* Check for Errors and if Packets been sent or received if
+		 * respective IRQ are enabled
+		 */
+		if ( (((dma_stat << 3) & (GRSPW_DMACTRL_PR|GRSPW_DMACTRL_PS))
+		     | GRSPW_DMA_STATUS_ERROR) & dma_stat ) {
+			/* Disable Further IRQs (until enabled again)
+			 * from this DMA channel. Let the status
+			 * bit remain so that they can be handled by
+			 * work function.
+			 */
+			REG_WRITE(&priv->regs->dma[0].ctrl, dma_stat & 
+				~(GRSPW_DMACTRL_RI|GRSPW_DMACTRL_TI|
+				GRSPW_DMACTRL_PR|GRSPW_DMACTRL_PS|
+				GRSPW_DMACTRL_RA|GRSPW_DMACTRL_TA|
+				GRSPW_DMACTRL_AT));
+			queue_work(grspw_workq, &priv->dma[0].work);
+			ret = IRQ_HANDLED;
+		}
+	} else {
+		for (i=0; i<priv->hwsup.ndma_chans; i++) {
+			dma_stat = REG_READ(&priv->regs->dma[i].ctrl);
+#ifdef HW_WITH_GI
+		if ( dma_stat & (GRSPW_DMA_STATUS_ERROR | GRSPW_DMACTRL_GI) ) {
+#else
+		if ( (((dma_stat << 3) & (GRSPW_DMACTRL_PR | GRSPW_DMACTRL_PS))
+		     | GRSPW_DMA_STATUS_ERROR) & dma_stat ) {
+#endif
+			/* Disable Further IRQs (until enabled again)
+			 * from this DMA channel. Let the status
+			 * bit remain so that they can be handled by
+			 * work function.
+			 */
+			REG_WRITE(&priv->regs->dma[i].ctrl, dma_stat & 
+				~(GRSPW_DMACTRL_RI|GRSPW_DMACTRL_TI|
+				GRSPW_DMACTRL_PR|GRSPW_DMACTRL_PS|
+				GRSPW_DMACTRL_RA|GRSPW_DMACTRL_TA|
+				GRSPW_DMACTRL_AT));
+				queue_work(grspw_workq, &priv->dma[i].work);
+				ret = IRQ_HANDLED;
+		}
+		}
+	}
+	SPIN_UNLOCK(&priv->devlock, irqflags);
+
+	if ( ret == IRQ_HANDLED )
+		priv->stats.irq_cnt++;
+
+	return ret;
+}
+
+STATIC void grspw_hw_dma_stop(struct grspw_dma_priv *dma)
+{
+	unsigned int ctrl;
+	struct grspw_dma_regs *dregs = dma->regs;
+
+	ctrl = REG_READ(&dregs->ctrl) & (GRSPW_DMACTRL_LE | GRSPW_DMACTRL_EN |
+	       GRSPW_DMACTRL_SP | GRSPW_DMACTRL_SA | GRSPW_DMACTRL_NS);
+	ctrl |=	GRSPW_DMACTRL_AT;
+	REG_WRITE(&dregs->ctrl, ctrl);
+}
+
+STATIC void grspw_hw_dma_softreset(struct grspw_dma_priv *dma)
+{
+	unsigned int ctrl;
+	struct grspw_dma_regs *dregs = dma->regs;
+
+	ctrl = REG_READ(&dregs->ctrl) & (GRSPW_DMACTRL_LE | GRSPW_DMACTRL_EN);
+	REG_WRITE(&dregs->ctrl, ctrl);
+
+	REG_WRITE(&dregs->rxmax, DEFAULT_RXMAX);
+	REG_WRITE(&dregs->txdesc, 0);
+	REG_WRITE(&dregs->rxdesc, 0);
+}
+
+/* Hardware Action:
+ *  - stop DMA
+ *  - do not bring down the link (RMAP may be active)
+ *  - RMAP settings untouched (RMAP may be active)
+ *  - port select untouched (RMAP may be active)
+ *  - timecodes are disabled
+ *  - IRQ generation disabled
+ *  - status not cleared (let user analyze it if requested later on)
+ *  - Node address / First DMA channels Node address
+ *    is untouched (RMAP may be active)
+ */
+STATIC void grspw_hw_stop(struct grspw_priv *priv)
+{
+	int i;
+	unsigned int ctrl;
+	IRQFLAGS_TYPE irqflags;
+
+	SPIN_LOCK_IRQ(&priv->devlock, irqflags);
+
+	for (i=0; i<priv->hwsup.ndma_chans; i++)
+		grspw_hw_dma_stop(&priv->dma[i]);
+
+	ctrl = REG_READ(&priv->regs->ctrl);
+	REG_WRITE(&priv->regs->ctrl, ctrl & (
+		GRSPW_CTRL_LD | GRSPW_CTRL_LS | GRSPW_CTRL_AS |
+		GRSPW_CTRL_RE | GRSPW_CTRL_RD |
+		GRSPW_CTRL_NP | GRSPW_CTRL_PS));
+
+	SPIN_UNLOCK_IRQ(&priv->devlock, irqflags);
+}
+
+/* Soft reset of GRSPW core registers */
+STATIC void grspw_hw_softreset(struct grspw_priv *priv)
+{
+	int i;
+	unsigned int tmp;
+
+	for (i=0; i<priv->hwsup.ndma_chans; i++)
+		grspw_hw_dma_softreset(&priv->dma[i]);
+
+	REG_WRITE(&priv->regs->status, 0xffffffff);
+	REG_WRITE(&priv->regs->time, 0);
+	/* Clear all but valuable reset values of ICCTRL */
+	tmp = REG_READ(&priv->regs->icctrl);
+	tmp &= GRSPW_ICCTRL_INUM | GRSPW_ICCTRL_BIRQ | GRSPW_ICCTRL_TXIRQ;
+	tmp |= GRSPW_ICCTRL_ID;
+	REG_WRITE(&priv->regs->icctrl, tmp);
+	REG_WRITE(&priv->regs->icrx, 0xffffffff);
+	REG_WRITE(&priv->regs->icack, 0xffffffff);
+	REG_WRITE(&priv->regs->ictimeout, 0xffffffff);
+}
+
+int grspw_dev_count(void)
+{
+	return grspw_count;
+}
+EXPORT_SYMBOL(grspw_dev_count);
+
+void grspw_initialize_user(void *(*devfound)(int), void (*devremove)(int,void*))
+{
+	int i;
+	struct grspw_priv *priv;
+
+	/* Set new Device Found Handler */
+	grspw_dev_add = devfound;
+	grspw_dev_del = devremove;
+
+	if (grspw_initialized == 1 && grspw_dev_add) {
+		/* Call callback for every previously found device */
+		for (i=0; i<grspw_count; i++) {
+			priv = priv_tab[i];
+			if (priv)
+				priv->data = grspw_dev_add(i);
+		}
+	}
+}
+EXPORT_SYMBOL(grspw_initialize_user);
+
+static int grspw_of_probe(struct platform_device *of_dev)
+{
+	struct device_node *onp = of_dev->dev.of_node;
+	struct grspw_priv *priv;
+	int i, size, err, len;
+	unsigned int ctrl;
+	const int *pampopts, *pversion, *pindex, *pdevice;
+	int devid = 0;
+	char *devname;
+
+	if ( grspw_count >= GRSPW_MAX )
+		return -ENOMEM;
+
+	if (!of_dev || !of_dev->resource) {
+		printk(KERN_NOTICE "GRSPW: of_dev=%p has no resources.. ignoring\n",
+			of_dev);
+		return -EINVAL;
+	}
+
+	/* Skip this device if user has set ampopts to zero, the device may
+	 * be used by another OS instance in AMP systems.
+	 */
+	pampopts = of_get_property(of_dev->dev.of_node, "ampopts", &len);
+	if ( pampopts && (len == 4) && (*pampopts == 0) )
+		return 0;
+
+	priv = (struct grspw_priv *)kmalloc(sizeof(*priv), GFP_KERNEL);
+	if ( !priv )
+		return -ENOMEM;
+	memset(priv, 0, sizeof(*priv));
+
+
+	/* MAP Registers into Kernel Virtual Address Space */
+	priv->regs = devm_platform_ioremap_resource(of_dev, 0);
+	if (IS_ERR(priv->regs)) {
+		err = PTR_ERR(priv->regs);
+		goto error;
+	}
+
+	/* Get IRQ number of GRSPW device */
+	priv->irq = irq_of_parse_and_map(onp, 0);
+	if (!priv->irq) {
+		dev_err(&of_dev->dev, "no irq found\n");
+		err = -ENODEV;
+		goto error;
+	}
+	priv->dev = &of_dev->dev;
+
+	SPIN_INIT(&priv->devlock);
+
+	/* Register character device in registered region */
+	priv->index = grspw_count;
+	priv_tab[priv->index] = priv;
+
+	strcat(priv->devname, "GRSPW_N");
+	priv->devname[6] = '0' + priv->index;
+
+	/* Read Hardware Support from Control Register */
+	ctrl = REG_READ(&priv->regs->ctrl);
+	priv->hwsup.rmap = (ctrl & GRSPW_CTRL_RA) >> GRSPW_CTRL_RA_BIT;
+	priv->hwsup.rmap_crc = (ctrl & GRSPW_CTRL_RC) >> GRSPW_CTRL_RC_BIT;
+	priv->hwsup.rx_unalign = (ctrl & GRSPW_CTRL_RX) >> GRSPW_CTRL_RX_BIT;
+	priv->hwsup.nports = 1 + ((ctrl & GRSPW_CTRL_PO) >> GRSPW_CTRL_PO_BIT);
+	priv->hwsup.ndma_chans = 1 + ((ctrl & GRSPW_CTRL_NCH) >> GRSPW_CTRL_NCH_BIT);
+
+	/* Find Device ID and Version */
+	priv->hwsup.hw_version = 0;
+	pversion = of_get_property(of_dev->dev.of_node, "version", &len);
+	if ( pversion && (len == 4) )
+		priv->hwsup.hw_version = *pversion;
+	pdevice = 0;
+	pdevice = of_get_property(of_dev->dev.of_node, "device", &len);
+	if ( pdevice && (len == 4) )
+		devid = *pdevice;
+	priv->hwsup.hw_version |= devid << 16;
+	if ( (devid == GAISLER_SPW2) || (devid == GAISLER_SPW2_DMA) ) {
+		priv->hwsup.strip_adr = 1; /* All GRSPW2 can strip Address */
+		priv->hwsup.strip_pid = 1; /* All GRSPW2 can strip PID */
+		if ( devid == GAISLER_SPW2 )
+			devname = "GRSPW2";
+		else
+			devname = "GRSPW2_ROUTER";
+	} else {
+		/* Autodetect GRSPW1 features? */
+		priv->hwsup.strip_adr = 0;
+		priv->hwsup.strip_pid = 0;
+		devname = "GRSPW1";
+	}
+
+	/* Get device Index just for Information */
+	priv->core_index = 0;
+	pindex = of_get_property(of_dev->dev.of_node, "index", &len);
+	if ( pindex && (len == 4) )
+		priv->core_index = *pindex;
+
+	printk(KERN_NOTICE "GRSPW[%d]: type=%s, core_index=%d, vadr_regs=%p\n",
+		priv->index, devname, priv->core_index, priv->regs);
+
+	/* Allocate Memory for all DMA channels */
+	size = sizeof(struct grspw_dma_priv) * priv->hwsup.ndma_chans;
+	priv->dma = (struct grspw_dma_priv *) kmalloc(size, GFP_KERNEL);
+	memset(priv->dma, 0, size);
+
+	/* Initializing Work for this particular GRSPW Core */
+	INIT_WORK(&priv->work, grspw_work_func);
+	for (i=0; i<priv->hwsup.ndma_chans; i++) {
+		priv->dma[i].core = priv;
+		priv->dma[i].index = i;
+		priv->dma[i].regs = &priv->regs->dma[i];
+		INIT_WORK(&priv->dma[i].work, grspw_work_dma_func);
+	}
+
+	/* Startup Action:
+	 *  - stop DMA
+	 *  - do not bring down the link (RMAP may be active)
+	 *  - RMAP settings untouched (RMAP may be active)
+	 *  - port select untouched (RMAP may be active)
+	 *  - timecodes are diabled
+	 *  - IRQ generation disabled
+	 *  - status cleared
+	 *  - Node address / First DMA channels Node address
+	 *    is untouched (RMAP may be active)
+	 */
+	grspw_hw_stop(priv);
+	grspw_hw_softreset(priv);
+
+	/* Register IRQ Handler */
+	err = request_irq(priv->irq, grspw_isr, IRQF_SHARED,
+				priv->devname, priv);
+	if ( err ) {
+		printk(KERN_NOTICE "GRSPW[%d]: Failed installing IRQ %d\n",
+			priv->index, priv->irq);
+		goto error;
+	}
+
+	dev_set_drvdata(&of_dev->dev, priv);
+
+	grspw_count++;
+
+	/* Tell above layer about new device */
+	if (grspw_dev_add)
+		priv->data = grspw_dev_add(priv->index);
+
+	return 0;
+
+error:
+	return err;
+}
+
+static int grspw_of_remove(struct platform_device *of_dev)
+{
+	struct grspw_priv *priv = dev_get_drvdata(&of_dev->dev);
+
+	if ( grspw_dev_del )
+		grspw_dev_del(priv->index, priv->data);
+
+	/* Remove IRQ handler */
+	free_irq(priv->irq, priv);
+
+	priv_tab[priv->index] = NULL;
+
+	kfree(priv);
+	dev_set_drvdata(&of_dev->dev, NULL);
+
+	return 0;
+}
+
+static const struct of_device_id grspw_of_match[] = {
+#if 0 /* GRSPW1 not tested yet... */
+	{ /* SpaceWire 1 Interface */
+	 .name = "GAISLER_SPW",
+	 .data = (void *)GAISLER_SPW,
+	 },
+	{ /* SpaceWire 1 Interface, in numbers */
+	 .name = "01_01f",
+	 .data = (void *)GAISLER_SPW,
+	 },
+#endif
+	{ /* SpaceWire 2 Interface */
+	 .name = "GAISLER_SPW2",
+	 .data = (void *)GAISLER_SPW2,
+	 },
+	{ /* SpaceWire 2 Interface, in numbers */
+	 .name = "01_029",
+	 .data = (void *)GAISLER_SPW2,
+	 },
+	{ /* SpaceWire Router DMA Interface */
+	 .name = "GAISLER_SPW2_DMA",
+	 .data = (void *)GAISLER_SPW2_DMA,
+	 },
+	{
+	 .name = "01_08a",
+	 .data = (void *)GAISLER_SPW2_DMA,
+	 },
+	{ /* SpaceWire 2 Interface */
+	 .compatible = "gaisler,grspw2",
+	 .data = (void *)GAISLER_SPW2,
+	 },
+	{ /* SpaceWire Router DMA Interface */
+	 .compatible = "gaisler,grspw2_dma",
+	 .data = (void *)GAISLER_SPW2_DMA,
+	 },
+	{}, /* Mark end */
+};
+
+MODULE_DEVICE_TABLE(of, grspw_of_match);
+
+static struct platform_driver grspw_of_driver = {
+	.driver = {
+		.name = "grlib-grspw",
+		.owner = THIS_MODULE,
+		.of_match_table = grspw_of_match,
+	},
+	.probe = grspw_of_probe,
+	.remove = grspw_of_remove,
+};
+
+static int __init grspw_init(void)
+{
+	grspw_count = 0;
+	memset(priv_tab, 0, sizeof(priv_tab));
+	init_MUTEX(&grspw_sem);
+
+	/* Create Work queue for all GRSPW driver instances */
+	grspw_workq = create_workqueue("GRSPW_WORK");
+	if ( grspw_workq == NULL ) {
+		printk(KERN_WARNING "GRSPW: Failed to create Work Queue\n");
+			return -1;
+	}
+
+	grspw_initialized = 1;
+
+	/* Register Platform Driver */
+	return platform_driver_register(&grspw_of_driver);
+}
+
+static void __exit grspw_exit (void)
+{
+	grspw_initialized = 0;
+	if ( grspw_workq ) {
+		destroy_workqueue(grspw_workq);
+		grspw_workq = NULL;
+	}
+	grspw_count = 0;
+
+	platform_driver_unregister(&grspw_of_driver);
+}
+
+module_init(grspw_init);
+module_exit(grspw_exit);
+
+MODULE_AUTHOR("Cobham Gaisler AB.");
+MODULE_DESCRIPTION("Cobham Gaisler GRSPW SpaceWire Library");
+MODULE_LICENSE("GPL");
+MODULE_ALIAS("platform:grlib-grspw");
diff -Nru a/drivers/grlib/spw/grspw_router.c b/drivers/grlib/spw/grspw_router.c
--- a/drivers/grlib/spw/grspw_router.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/grlib/spw/grspw_router.c	2022-01-29 23:16:14.137064547 +0100
@@ -0,0 +1,533 @@
+/*
+ * Cobham Gaisler GRSPW-ROUTER APB-Register access driver.
+ *
+ * Copyright (c) 2016-2022 Cobham Gaisler AB
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston,
+ * MA 02110-1301 USA
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/fs.h>
+#include <linux/cdev.h>
+#include <asm/uaccess.h>
+#include <linux/ioctl.h>
+#include <linux/slab.h>
+
+#include <linux/of_device.h>
+#include <linux/of_platform.h>
+#include <linux/version.h>
+
+#include <linux/grlib/grspw_router.h>
+#include <linux/grlib/devno.h>
+
+#define REG_READ(adr) (*(volatile u32 *)(adr))
+#define REG_WRITE(adr, value) (*(volatile u32 *)(adr) = (value))
+
+struct router_regs {
+	u32 resv1;		/* 0x000 */
+	u32 psetup[255];	/* 0x004 */
+	u32 resv2[32];		/* 0x400 */
+	u32 routes[224];	/* 0x480 */
+	u32 pctrl[32];		/* 0x800 */
+	u32 psts[32];		/* 0x880 */
+	u32 tprescaler;		/* 0x900 */
+	u32 treload[31];	/* 0x904 */
+	u32 resv3[32];		/* 0x980 */
+	u32 cfgsts;		/* 0xA00 */
+	u32 timecode;		/* 0xA04 */
+	u32 ver;		/* 0xA08 */
+	u32 idiv;		/* 0xA0C */
+	u32 cfgwe;		/* 0xA10 */
+};
+
+struct router_priv {
+	struct router_regs *regs;
+	struct device *dev;
+	int minor;
+	struct cdev cdev;	/* Char device */
+	int open;
+	struct router_hw_info hwinfo;
+	int nports;
+};
+
+static int router_count;
+
+static int router_open(struct inode *inode, struct file *file);
+static int router_release(struct inode *inode, struct file *file);
+static long router_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);
+
+struct file_operations router_fops = {
+	.owner   = THIS_MODULE,
+	.open    = router_open,
+	.release = router_release,
+	.unlocked_ioctl   = router_ioctl,
+};
+
+static int router_open(struct inode *inode, struct file *file)
+{
+	struct router_priv *priv;
+
+	priv = container_of(inode->i_cdev, struct router_priv, cdev);
+
+	if (priv == NULL) {
+		printk (KERN_WARNING "GRSPW_ROUTER: device doesn't exist\n");
+		return -ENODEV;
+	}
+
+	if ( priv->open ) {
+		return 0;
+	}
+
+	priv->open = 1;
+	file->private_data = priv;
+
+	return 0;
+}
+
+static int router_release(struct inode *inode, struct file *file)
+{
+	struct router_priv *priv;
+
+	priv = container_of(inode->i_cdev, struct router_priv, cdev);
+
+	priv->open = 0;
+
+	return 0;
+}
+
+void router_hwinfo(struct router_priv *priv, struct router_hw_info *hwinfo)
+{
+	unsigned int tmp;
+
+	tmp = REG_READ(&priv->regs->cfgsts);
+	hwinfo->nports_spw   = (tmp >> 27) & 0x1f;
+	hwinfo->nports_amba  = (tmp >> 22) & 0x1f;
+	hwinfo->nports_fifo  = (tmp >> 17) & 0x1f;
+	hwinfo->timers_avail = (tmp >>  1) & 0x1;
+	hwinfo->pnp_avail    = (tmp >>  0) & 0x1;
+
+	tmp = REG_READ(&priv->regs->ver);
+	hwinfo->ver_major   = (tmp >> 24) & 0xff;
+	hwinfo->ver_minor   = (tmp >> 16) & 0xff;
+	hwinfo->ver_patch   = (tmp >>  8) & 0xff;
+	hwinfo->iid         = (tmp >>  0) & 0xff;
+}
+
+int router_config_set(struct router_priv *priv, struct router_config *cfg)
+{
+	int i;
+
+	/* Write only configuration bits in Config register */
+	if ( cfg->flags & ROUTER_FLG_CFG ) {
+		REG_WRITE(&priv->regs->cfgsts, cfg->config & ~0x4);
+	}
+
+	/* Write Instance ID to Version Register */
+	if ( cfg->flags & ROUTER_FLG_IID ) {
+		REG_WRITE(&priv->regs->ver, cfg->iid);
+	}
+
+	/* Write startup-clock-divisor Register */
+	if ( cfg->flags & ROUTER_FLG_IDIV ) {
+		REG_WRITE(&priv->regs->idiv, cfg->idiv);
+	}
+
+	/* Write Timer Prescaler Register */
+	if ( cfg->flags & ROUTER_FLG_TPRES ) {
+		REG_WRITE(&priv->regs->tprescaler, cfg->timer_prescaler);
+	}
+
+	/* Write Timer Reload Register */
+	if ( cfg->flags & ROUTER_FLG_TRLD ) {
+		for (i=0; i<priv->nports; i++)
+			REG_WRITE(&priv->regs->treload[i], cfg->timer_reload[i-1]);
+	}
+
+	return 0;
+}
+
+int router_config_read(struct router_priv *priv, struct router_config *cfg)
+{
+	int i;
+
+	cfg->config = REG_READ(&priv->regs->cfgsts) & ~0xffff0007;
+	cfg->iid = REG_READ(&priv->regs->ver) & 0xff;
+	cfg->idiv = REG_READ(&priv->regs->idiv) & 0xff;
+	cfg->timer_prescaler = REG_READ(&priv->regs->tprescaler);
+	for (i=0; i<priv->nports; i++)
+		cfg->timer_reload[i] = REG_READ(&priv->regs->treload[i]);
+
+	return 0;
+}
+
+int router_routes_set(struct router_priv *priv, struct router_routes *routes)
+{
+	int i;
+	for (i=0; i<224; i++)
+		REG_WRITE(&priv->regs->routes[i], routes->route[i]);
+	return 0;
+}
+
+int router_routes_read(struct router_priv *priv, struct router_routes *routes)
+{
+	int i;
+	for (i=0; i<224; i++)
+		routes->route[i] = REG_READ(&priv->regs->routes[i]);
+	return 0;
+}
+
+int router_ps_set(struct router_priv *priv, struct router_ps *ps)
+{
+	int i;
+	unsigned int *p = &ps->ps[0];
+	for (i=0; i<255; i++,p++) 
+		REG_WRITE(&priv->regs->psetup[i], *p);
+	return 0;
+}
+
+int router_ps_read(struct router_priv *priv, struct router_ps *ps)
+{
+	int i;
+	unsigned int *p = &ps->ps[0];
+	for (i=0; i<255; i++,p++) 
+		REG_WRITE(&priv->regs->psetup[i], *p);
+	return 0;
+}
+
+int router_we_set(struct router_priv *priv, int we)
+{
+	REG_WRITE(&priv->regs->cfgwe, we & 0x1);
+	return 0;
+}
+
+int router_port_ctrl(struct router_priv *priv, struct router_port *port)
+{
+	unsigned int ctrl, sts;
+
+	if ( port->port > priv->nports )
+		return -EINVAL;
+
+	ctrl = port->ctrl;
+	if ( port->flag & ROUTER_PORTFLG_GET_CTRL ) {
+		ctrl = REG_READ(&priv->regs->pctrl[port->port]);
+	}
+	sts = port->sts;
+	if ( port->flag & ROUTER_PORTFLG_GET_STS ) {
+		sts = REG_READ(&priv->regs->psts[port->port]);
+	}
+
+	if ( port->flag & ROUTER_PORTFLG_SET_CTRL ) {
+		REG_WRITE(&priv->regs->pctrl[port->port], port->ctrl);
+	}
+	if ( port->flag & ROUTER_PORTFLG_SET_STS ) {
+		REG_WRITE(&priv->regs->psts[port->port], port->sts);
+	}
+
+	port->ctrl = ctrl;
+	port->sts = sts;
+	return 0;
+}
+
+int router_cfgsts_set(struct router_priv *priv, unsigned int cfgsts)
+{
+	REG_WRITE(&priv->regs->cfgsts, cfgsts);
+	return 0;
+}
+
+int router_cfgsts_read(struct router_priv *priv, unsigned int *cfgsts)
+{
+	*cfgsts = REG_READ(&priv->regs->cfgsts);
+	return 0;
+}
+
+int router_tc_read(struct router_priv *priv, unsigned int *tc)
+{
+	*tc = REG_READ(&priv->regs->timecode);
+	return 0;
+}
+
+static long router_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	struct router_priv *priv;
+	void __user *argp = (void __user *)arg;
+
+	priv = (struct router_priv *)filp->private_data;
+
+	if (priv == NULL) {
+		printk (KERN_WARNING "GRSPW_ROUTER: ioctl device doesn't exist\n");
+		return -ENODEV;
+	}
+
+	/* Verify READ/WRITE Access to user provided buffer */
+	if (_IOC_DIR(cmd) & (_IOC_WRITE | _IOC_READ))
+		if (!access_ok((void __user *)arg, _IOC_SIZE(cmd)))
+			return -EFAULT;
+
+	switch (cmd) {
+
+
+	/* Get Hardware support/information available */
+	case GRSPWR_IOCTL_HWINFO:
+	{
+		struct router_hw_info hwinfo;
+		router_hwinfo(priv, &hwinfo);
+		if(copy_to_user(argp, &hwinfo, sizeof(struct router_hw_info)))
+			return -EFAULT;
+		break;
+	}
+
+	/* Set Router Configuration */
+	case GRSPWR_IOCTL_CFG_SET:
+	{
+		struct router_config cfg;
+		if(copy_from_user(&cfg, argp, sizeof(struct router_config)))
+			return -EFAULT;
+		return router_config_set(priv, &cfg);
+	}
+
+	/* Read Router Configuration */
+	case GRSPWR_IOCTL_CFG_GET:
+	{
+		struct router_config cfg;
+		router_config_read(priv, &cfg);
+		if(copy_to_user(argp, &cfg, sizeof(struct router_config)))
+			return -EFAULT;
+		break;
+	}
+
+	/* Routes */
+	case GRSPWR_IOCTL_ROUTES_SET:
+	{
+		struct router_routes routes;
+		if(copy_from_user(&routes, argp, sizeof(struct router_routes)))
+			return -EFAULT;
+		return router_routes_set(priv, &routes);
+	}
+
+	case GRSPWR_IOCTL_ROUTES_GET:
+	{
+		struct router_routes routes;
+		router_routes_read(priv, &routes);
+		if(copy_to_user(argp, &routes, sizeof(struct router_routes)))
+			return -EFAULT;
+		break;
+	}
+
+	/* Port Setup */
+	case GRSPWR_IOCTL_PS_SET:
+	{
+		struct router_ps ps;
+		if(copy_from_user(&ps, argp, sizeof(struct router_ps)))
+			return -EFAULT;
+		return router_ps_set(priv, &ps);
+	}
+
+	case GRSPWR_IOCTL_PS_GET:
+	{
+		struct router_ps ps;
+		router_ps_read(priv, &ps);
+		if(copy_to_user(argp, &ps, sizeof(struct router_ps)))
+			return -EFAULT;
+		break;
+	}
+
+	/* Set configuration write enable */
+	case GRSPWR_IOCTL_WE_SET:
+	{
+		return router_we_set(priv, (long)argp);
+	}
+
+	/* Set/Get Port Control/Status */
+	case GRSPWR_IOCTL_PORT:
+	{
+		struct router_port port;
+		int result;
+		if(copy_from_user(&port, argp, sizeof(struct router_port)))
+			return -EFAULT;
+		if ( (result=router_port_ctrl(priv, &port)) )
+			return result;
+		if(copy_to_user(argp, &port, sizeof(struct router_port)))
+			return -EFAULT;
+		break;
+	}
+
+	/* Set Router Configuration/Status Register */
+	case GRSPWR_IOCTL_CFGSTS_SET:
+	{
+		return router_cfgsts_set(priv, (long)argp);
+	}
+
+	/* Get Router Configuration/Status Register */
+	case GRSPWR_IOCTL_CFGSTS_GET:
+	{
+		unsigned int cfgsts;
+		router_cfgsts_read(priv, &cfgsts);
+		if(copy_to_user(argp, &cfgsts, sizeof(unsigned int)))
+			return -EFAULT;
+		break;
+	}
+
+	/* Get Current Time-Code Register */
+	case GRSPWR_IOCTL_TC_GET:
+	{
+		unsigned int tc;
+		router_tc_read(priv, &tc);
+		if(copy_to_user(argp, &tc, sizeof(unsigned int)))
+			return -EFAULT;
+		break;
+	}
+
+	default: return -ENOSYS;
+	}
+
+	return 0;
+}
+
+static int router_of_probe(struct platform_device *of_dev)
+{
+	struct router_priv *priv;
+	int result, err, len;
+	dev_t dev_id;
+	const int *pampopts;
+
+	/* Skip this device if user has set ampopts to zero, the device may
+	 * be used by another OS instance in AMP systems.
+	 */
+	pampopts = of_get_property(of_dev->dev.of_node, "ampopts", &len);
+	if ( pampopts && (len == 4) && (*pampopts == 0) )
+		return 0;
+
+	priv = (struct router_priv *)kmalloc(sizeof(*priv), GFP_KERNEL);
+	if ( !priv )
+		return -ENOMEM;
+
+	memset(priv, 0, sizeof(*priv));
+
+	/* MAP Registers into Kernel Virtual Address Space */
+	priv->regs = devm_platform_ioremap_resource(of_dev, 0);
+	if (IS_ERR(priv->regs)) {
+		err = PTR_ERR(priv->regs);
+		goto error;
+	}
+
+	/* Register character device in registered region */
+	priv->minor = router_count;
+	dev_id = MKDEV(SPWROUTER_MAJOR, SPWROUTER_MINOR + priv->minor);
+	cdev_init(&priv->cdev, &router_fops);
+	result = cdev_add(&priv->cdev, dev_id, 1);
+	if ( result ) {
+		printk(KERN_ERR "GRSPW_ROUTER: Failed adding CHAR dev\n");
+		err = -ENODEV;
+		goto error;
+	}
+
+	router_hwinfo(priv, &priv->hwinfo);
+	priv->open = 0;
+	priv->nports = priv->hwinfo.nports_spw + priv->hwinfo.nports_amba +
+			priv->hwinfo.nports_fifo;
+
+	/* Link of_dev with priv */
+	priv->dev = &of_dev->dev;
+	dev_set_drvdata(&of_dev->dev, priv);
+
+	router_count++;
+
+	printk (KERN_INFO "GRSPW_ROUTER[%d]: registered char device\n", priv->minor);
+	printk (KERN_INFO "                 ports: %d, %x:%x:%x %x\n", priv->nports,
+				priv->hwinfo.ver_major, priv->hwinfo.ver_minor,
+				priv->hwinfo.ver_patch, priv->hwinfo.iid);
+
+	return 0;
+
+error:
+	return err;
+}
+
+static int router_of_remove(struct platform_device *of_dev)
+{
+	struct router_priv *priv = dev_get_drvdata(&of_dev->dev);
+
+	cdev_del(&priv->cdev);
+
+	kfree(priv);
+	dev_set_drvdata(&of_dev->dev, NULL);
+
+	return 0;
+}
+
+static const struct of_device_id router_of_match[] = {
+	{
+	 .name = "GAISLER_SPWROUTER",
+	 },
+	{
+	 .name = "01_08b",
+	 },
+	{
+	 .compatible = "gaisler,spwrtr",
+	 },
+	{},
+};
+
+MODULE_DEVICE_TABLE(of, router_of_match);
+
+static struct platform_driver router_of_driver = {
+	.driver = {
+		.name = "grlib-spwrouter",
+		.owner = THIS_MODULE,
+		.of_match_table = router_of_match,
+	},
+	.probe = router_of_probe,
+	.remove = router_of_remove,
+};
+
+static int __init grspw_router_init(void)
+{
+	int result;
+	dev_t dev_id;
+
+	/* Register CHAR driver region */
+	dev_id = MKDEV(SPWROUTER_MAJOR, SPWROUTER_MINOR);
+	result = register_chrdev_region(dev_id, SPWROUTER_DEVCNT, "grspw_router");
+	if ( result < 0 ) {
+		printk(KERN_WARNING "ROUTER: Failed to register CHAR region\n");
+		return 0;
+	}
+
+	router_count = 0;
+
+	return platform_driver_register(&router_of_driver);
+}
+
+static void __exit grspw_router_cleanup(void)
+{
+	dev_t dev_id;
+
+	/* Unregister CHAR driver region */
+	dev_id = MKDEV(SPWROUTER_MAJOR, SPWROUTER_MINOR);
+	unregister_chrdev_region(dev_id, SPWROUTER_DEVCNT);
+
+	platform_driver_unregister(&router_of_driver);
+}
+
+module_init(grspw_router_init);
+module_exit(grspw_router_cleanup);
+
+MODULE_AUTHOR("Cobham Gaisler AB.");
+MODULE_DESCRIPTION("Cobham Gaisler SpaceWire Router APB Driver");
+MODULE_LICENSE("GPL");
+MODULE_ALIAS("platform:grlib-grspwrouter");
diff -Nru a/drivers/grlib/spw/grspw_std.c b/drivers/grlib/spw/grspw_std.c
--- a/drivers/grlib/spw/grspw_std.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/grlib/spw/grspw_std.c	2022-01-29 23:16:14.137064547 +0100
@@ -0,0 +1,1416 @@
+/*
+ * Cobham Gaisler GRSPW user space access Driver.
+ *
+ * Copyright (c) 2016 Cobham Gaisler AB
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston,
+ * MA 02110-1301 USA
+ */
+
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/fs.h>
+#include <linux/cdev.h>
+#include <asm/uaccess.h>
+#include <linux/mm.h>
+#include <linux/mman.h>
+#include <linux/slab.h>
+#include <linux/io.h>
+#include <linux/interrupt.h>
+#include <linux/semaphore.h>
+#include <linux/workqueue.h>
+#ifdef CONFIG_LEON
+#include <asm/leon.h>
+#endif
+#include <linux/grlib/grspw.h>
+#include <linux/grlib/grspw_user.h>
+#include <linux/grlib/maplib.h>
+#include <linux/grlib/devno.h>
+
+#if (GRSPW_MAX > GRSPWU_DEVCNT)
+#error GRSPW: not enough CHAR device nodes
+#endif
+
+/* Which Memory MAP device node that should be used. Currently hardcoded.
+ * should be configurable per GRSPW Device during runtime.
+ */
+#define MMAPLIB_POOL 0
+
+/* Do not enable Debug */
+#undef GRSPWU_DEBUG
+
+enum {
+	STATE_STOPPED = 0,
+	STATE_PAUSED = 1,
+	STATE_STARTED = 2,
+};
+
+struct grspwu_priv {
+	char name[8];		/* GRSPWU[N] Name */
+	void *dh;		/* GRSPW Device Handle */
+	int index;		/* Device Index */
+	int chan_cnt;		/* Number of DMA Channels available in HW */
+	unsigned int chan_mask;	/* Enabled (by user) DMA Channel Mask */
+	unsigned int chan_avail_mask; /* DMA Channel Available Mask */
+	void *dma[4];		/* DMA Channel device handles */
+	int dma_rxmaxlen[4];	/* DMA RX Maxlength */
+	int state;		/* If DMA Channels are open and ready */
+	struct cdev cdev;	/* Char device */
+	struct grspw_config cfg;/* Current Configuration */
+	struct grspw_bufcfg bufcfg;/* Current Buffer Configuration */
+	void *_pkt_tx_structs;	/* Base address of packet structures */
+	void *_pkt_rx_structs;	/* Base address of packet structures */
+
+	struct semaphore sem_pktstrs;	/* Free Packet structures Semaphore */
+	struct grspw_list tx_pktstrs;	/* Free TX Packet structures */
+	struct grspw_list rx_pktstrs;	/* Free RX Packet structures */
+
+	/* Memory MAP Library instance */
+	struct maplib_drv mmapdrv;
+};
+
+/* Set this function pointer from another driver to do custom SpaceWire TimeCode
+ * RX handling.
+ */
+void (*grspwu_tc_isr_callback)(void *data, int tc) = NULL;
+void *grspwu_tc_isr_arg = NULL;
+
+struct grspw_bufcfg grspwu_def_bufcfg = 
+{
+	.rx_pkts = 1024,
+	.tx_pkts = 1024,
+};
+
+static struct grspwu_priv *privu_tab[GRSPW_MAX];
+
+static int grspwu_open(struct inode *inode, struct file *file);
+static int grspwu_release(struct inode *inode, struct file *file);
+static long grspwu_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);
+static int grspwu_read(struct file *filp, char __user *buff, size_t count, loff_t *offp);
+static int grspwu_write(struct file *filp, const char __user *buff, size_t count, loff_t *offp);
+
+
+struct file_operations grspwu_fops = {
+	.owner          = THIS_MODULE,
+	.open           = grspwu_open,
+	.release        = grspwu_release,
+	.unlocked_ioctl = grspwu_ioctl,
+	.compat_ioctl   = grspwu_ioctl,
+	.mmap           = NULL,
+	.read           = grspwu_read,
+	.write          = grspwu_write,
+};
+
+void grspwu_stop(struct grspwu_priv *priv);
+void grspwu_unmap(int idx, void *priv);
+
+static int grspwu_open(struct inode *inode, struct file *file)
+{
+	struct grspwu_priv *priv;
+	struct grspw_hw_sup hwcfg;
+	void *dh;
+
+	priv = container_of(inode->i_cdev, struct grspwu_priv, cdev);
+
+	if (priv == NULL) {
+		printk (KERN_WARNING "GRSPWU: device doesn't exist\n");
+		return -ENODEV;
+	}
+
+	/* Take a GRSPW Device, this routine will initialize/clear
+	 * hardware registers of the device.
+	 */
+	dh = grspw_open(priv->index);
+	if ( dh == NULL ) {
+		printk (KERN_WARNING "GRSPWU: device %d already opened\n", priv->index);
+		return -EBUSY;
+	}
+	priv->dh = dh;
+	file->private_data = priv;
+	priv->state = STATE_STOPPED;
+	priv->dma[0] = grspw_dma_open(priv->dh, 0);
+	if ( priv->dma[0] == NULL ) {
+		printk (KERN_WARNING "GRSPWU[%d]: failed to open DMA CHAN[0]\n", priv->index);
+		grspw_close(priv->dh);
+		priv->dh = NULL;
+		return -EIO;
+	}
+	priv->dma[1] = NULL;
+	priv->dma[2] = NULL;
+	priv->dma[3] = NULL;
+	priv->chan_mask = 1; /* Default to only one DMA Channel */
+	/* Get number of DMA Channels */
+	grspw_hw_support(priv->dh, &hwcfg);
+	priv->chan_cnt = hwcfg.ndma_chans;
+	priv->chan_avail_mask = ~(-1 << priv->chan_cnt);
+
+	/* Set Default Configuration */
+	memcpy(&priv->bufcfg, &grspwu_def_bufcfg, sizeof(grspwu_def_bufcfg));
+	priv->_pkt_tx_structs = NULL;
+	priv->_pkt_rx_structs = NULL;
+
+	grspw_list_clr(&priv->tx_pktstrs);
+	grspw_list_clr(&priv->rx_pktstrs);
+
+	init_MUTEX(&priv->sem_pktstrs);
+
+	return 0;
+}
+
+static int grspwu_release(struct inode *inode, struct file *file)
+{
+	struct grspwu_priv *priv = container_of(inode->i_cdev, struct grspwu_priv, cdev);
+
+	if ( priv->state == STATE_STARTED )
+		grspwu_stop(priv);
+	if ( priv->_pkt_tx_structs ) {
+		kfree(priv->_pkt_tx_structs);
+		priv->_pkt_tx_structs = NULL;
+	}
+	if ( priv->_pkt_rx_structs ) {
+		kfree(priv->_pkt_rx_structs);
+		priv->_pkt_rx_structs = NULL;
+	}
+	priv->state = STATE_STOPPED;
+	grspw_close(priv->dh);
+	priv->dh = NULL;
+	file->private_data = NULL;
+
+	return 0;
+}
+
+/* Configure number of packet structures in driver (always called in stopped mode) */
+int grspwu_bufcfg(struct grspwu_priv *priv, struct grspw_bufcfg *bufcfg)
+{
+	/* Check buf configuration */
+	if ( (bufcfg->rx_pkts < 1) || (bufcfg->tx_pkts < 1) )
+		return -EINVAL;
+	/* Configuration will take affect upon next start. If configuration
+	 * differs from last configuration previous allocated memory (if any)
+	 * will be freed.
+	 */
+	if ( (priv->bufcfg.rx_pkts != bufcfg->rx_pkts) ||
+	     (priv->bufcfg.tx_pkts != bufcfg->tx_pkts) ) {
+		memcpy(&priv->bufcfg, bufcfg, sizeof(*bufcfg));
+		if ( priv->_pkt_tx_structs ) {
+			kfree(priv->_pkt_tx_structs);
+			priv->_pkt_tx_structs = NULL;
+			priv->state = STATE_STOPPED;
+		}
+		if ( priv->_pkt_rx_structs ) {
+			kfree(priv->_pkt_rx_structs);
+			priv->_pkt_rx_structs = NULL;
+			priv->state = STATE_STOPPED;
+		}
+	}
+
+	return 0;
+}
+
+/* Configure Core and DMA Channels, RMAP and TimeCode setup */
+int grspwu_config_set(struct grspwu_priv *priv, struct grspw_config *cfg)
+{
+	int rmap_dstkey;
+	int i, retval;
+
+	/* Check RMAP Configuration */
+	if ( cfg->rmap_cfg & ~(RMAPOPTS_EN_RMAP|RMAPOPTS_EN_BUF) )
+		return -EINVAL;
+
+	/* Check Timecode Configuration, enabling RX IRQ not allowed from here */
+	if ( cfg->tc_cfg & ~(TCOPTS_EN_RX|TCOPTS_EN_TX) )
+		return -EINVAL;
+
+	/* Check Channel Enable Mask, report error if trying to enable a 
+	 * channel that is not available. */
+	if ( ~priv->chan_avail_mask & cfg->enable_chan_mask ) {
+		return -EINVAL;
+	}
+
+	/* 1. Configure Node address and Node mask of DMA Channels
+	 *    and default node address of core itself.
+	 */
+	cfg->adrcfg.promiscuous &= 1;
+	grspw_addr_ctrl(priv->dh, &cfg->adrcfg);
+
+	/* 2. Configure RMAP (Enable, Buffer, Destination Key) */
+	rmap_dstkey = cfg->rmap_dstkey;
+	if ( grspw_rmap_ctrl(priv->dh, &cfg->rmap_cfg, &rmap_dstkey) ) {
+		/* RMAP not available in Core, but trying to enable */
+		return -EIO;
+	}
+
+	/* 3. Timecode setup. If a custom timecode ISR callback handler is 
+	 *    installed that routine is installed and RX IRQ is enabled.
+	 */
+	if ( grspwu_tc_isr_callback ) {
+		grspw_tc_isr(priv->dh, grspwu_tc_isr_callback, grspwu_tc_isr_arg);
+		cfg->tc_cfg |= TCOPTS_EN_RXIRQ;
+	}
+	grspw_tc_ctrl(priv->dh, &cfg->tc_cfg);
+
+	/* 4. Configure DMA Channels and remember which channels will be
+	 *    used. Only the enabled channels will be activated and started
+	 *    during the ioctl(START) operation.
+	 */
+	retval = 0;
+	for (i=0; i<priv->chan_cnt; i++) {
+		/* Open or close DMA Channel */
+		if ( cfg->enable_chan_mask & (1<<i) ) {
+			/* Should be opened */
+			if ( priv->dma[i] == NULL ) {
+				priv->dma[i] = grspw_dma_open(priv->dh, i);
+				if ( priv->dma[i] == NULL ) {
+					retval = -EIO;
+					continue;
+				}
+				priv->chan_mask |= (1<<i);
+			}
+
+			/* Configure channel */
+			grspw_dma_config(priv->dma[i], &cfg->chan[i]);
+		} else {
+			/* Should be closed */
+			if ( priv->dma[i] ) {
+				grspw_dma_close(priv->dma[i]);
+				priv->dma[i] = NULL;
+			}
+			priv->chan_mask &= ~(1<<i);
+		}
+	}
+
+	return retval;
+}
+
+void grspwu_config_read(struct grspwu_priv *priv, struct grspw_config *cfg)
+{
+	int i;
+	int rmap_dstkey;
+
+	cfg->adrcfg.promiscuous = -1;
+	grspw_addr_ctrl(priv->dh, &cfg->adrcfg);
+
+	cfg->rmap_cfg = -1;
+	rmap_dstkey = -1;
+	grspw_rmap_ctrl(priv->dh, &cfg->rmap_cfg, &rmap_dstkey);
+	cfg->rmap_dstkey = rmap_dstkey;
+
+	cfg->tc_cfg = -1;
+	grspw_tc_ctrl(priv->dh, &cfg->tc_cfg);
+
+	cfg->enable_chan_mask = priv->chan_mask;
+	for (i=0; i<priv->chan_cnt; i++) {
+		if ( priv->dma[i] ) {
+			grspw_dma_config_read(priv->dma[i], &cfg->chan[i]);
+		}
+	}
+}
+
+void grspwu_stats_read(struct grspwu_priv *priv, struct grspw_stats *stats)
+{
+	int i;
+
+	grspw_stats_read(priv->dh, &stats->stats);
+
+	for (i=0; i<priv->chan_cnt; i++) {
+		if ( priv->dma[i] )
+			grspw_dma_stats_read(priv->dma[i], &stats->chan[i]);
+		else
+			memset(&stats->chan[i], 0, sizeof(struct grspw_dma_stats));
+	}
+}
+
+void grspwu_stats_clr(struct grspwu_priv *priv)
+{
+	int i;
+
+	grspw_stats_clr(priv->dh);
+
+	for (i=0; i<priv->chan_cnt; i++) {
+		if ( priv->dma[i] )
+			grspw_dma_stats_clr(priv->dma[i]);
+	}
+}
+
+int grspwu_link_ctrl(struct grspwu_priv *priv, struct grspw_link_ctrl *ctrl)
+{
+	int clkdiv;
+
+	/* Check Configuration */
+	if ( ctrl->ctrl & ~LINKOPTS_MASK )
+		return -EINVAL;
+
+	/* Configure */
+	clkdiv = ctrl->clkdiv_start<<8 | ctrl->clkdiv_run;
+	grspw_link_ctrl(priv->dh, &ctrl->ctrl, &clkdiv);
+
+	return 0;
+}
+
+int grspwu_port_ctrl(struct grspwu_priv *priv, int options)
+{	
+	int port;
+
+
+	if ( (options != 0) && (options != 1)  )
+		port = 3; /* Let hardware select port */
+	else
+		port = options;
+
+	/* Set selected port, if failure it indicates that the selected
+	 * port does not exist (HW created with only one port).
+	 */
+	if ( grspw_port_ctrl(priv->dh, &port) ) 
+		return -EIO;
+
+	return 0;
+}
+
+void grspwu_link_state_get(struct grspwu_priv *priv, struct grspw_link_state *state)
+{
+	int clkdiv;
+
+	/* Get Current Link Configuration */
+	state->link_ctrl = -1;
+	clkdiv = -1;
+	grspw_link_ctrl(priv->dh, &state->link_ctrl, &clkdiv);
+	state->clkdiv_start = (clkdiv >> 8) & 0xff;
+	state->clkdiv_run = clkdiv & 0xff;
+
+	/* Get Current Link Status */
+	state->link_state = grspw_link_state(priv->dh);
+
+	/* Get Current Port configuration */
+	state->port_cfg = -1;
+	grspw_port_ctrl(priv->dh, &state->port_cfg);
+
+	/* Get Current Active port */
+	state->port_active = grspw_port_active(priv->dh);
+}
+
+/* Set TCTRL and TIMECNT, and Send Time-code */
+void grspwu_tc_send(struct grspwu_priv *priv, int options)
+{
+	int time;
+
+	/* Set TCTRL and TIMECNT? */
+	if ( options & 0x100 ) {
+		time = options & 0xff;
+		grspw_tc_time(priv->dh, &time);
+	}
+
+	/* Send Time Code (Generate Tick-In) */
+	if ( options & 0x200 )
+		grspw_tc_tx(priv->dh);
+}
+
+void grspwu_tc_read(struct grspwu_priv *priv, int *tc)
+{
+	*tc = -1;
+	grspw_tc_time(priv->dh, tc);
+}
+
+void grspwu_qpktcnt_read(struct grspwu_priv *priv, struct grspw_qpktcnt *qpktcnt)
+{
+	int i;
+	for (i=0; i<priv->chan_cnt; i++) {
+		if ( priv->dma[i] ) {
+			grspw_dma_rx_count(
+				priv->dma[i],
+				&qpktcnt->chan[i].rx_ready,
+				&qpktcnt->chan[i].rx_sched,
+				&qpktcnt->chan[i].rx_recv);
+			grspw_dma_tx_count(
+				priv->dma[i],
+				&qpktcnt->chan[i].tx_send,
+				&qpktcnt->chan[i].tx_sched,
+				&qpktcnt->chan[i].tx_sent);
+		}
+	}
+}
+
+int grspwu_start(struct grspwu_priv *priv, int options)
+{
+	int size, i;
+	struct grspw_pkt *pktbase;
+	struct grspw_dma_config cfg;
+
+	if ( priv->state == STATE_STARTED )
+		return 0;
+
+	/* DMA Channels have been opened and configured at this point,
+	 * or user is satisfied with the default configuration.
+	 */
+
+	/* Allocate Packet structures if not already allocated */
+	if ( priv->_pkt_tx_structs == NULL ) {
+		size = sizeof(struct grspw_pkt) * priv->bufcfg.tx_pkts;
+		priv->_pkt_tx_structs = kmalloc(size, GFP_KERNEL);
+		if ( priv->_pkt_tx_structs == NULL )
+			return -ENOMEM;
+	}
+	if ( priv->_pkt_rx_structs == NULL ) {
+		size = sizeof(struct grspw_pkt) * priv->bufcfg.rx_pkts;
+		priv->_pkt_rx_structs = kmalloc(size, GFP_KERNEL);
+		if ( priv->_pkt_rx_structs == NULL )
+			return -ENOMEM;
+	}
+	/* Initialize Packet Structures to one long chain */
+	pktbase = (struct grspw_pkt *)priv->_pkt_tx_structs;
+	for (i=0; i<priv->bufcfg.tx_pkts; i++) {
+		pktbase[i].next = &pktbase[i+1];
+		pktbase[i].pkt_id = 0;
+		pktbase[i].flags = 0;
+		pktbase[i].reserved = 0;
+		pktbase[i].hlen = 0;
+		pktbase[i].dlen = 0;
+		pktbase[i].data = NULL;
+		pktbase[i].hdr = NULL;
+	}
+	/* Initialize TX Packet structure List */
+	priv->tx_pktstrs.head = &pktbase[0];
+	priv->tx_pktstrs.tail = &pktbase[priv->bufcfg.tx_pkts-1];
+	priv->tx_pktstrs.tail->next = NULL;
+	pktbase = (struct grspw_pkt *)priv->_pkt_rx_structs;
+	for (i=0; i<priv->bufcfg.rx_pkts; i++) {
+		pktbase[i].next = &pktbase[i+1];
+		pktbase[i].pkt_id = 0;
+		pktbase[i].flags = 0;
+		pktbase[i].reserved = 0;
+		pktbase[i].hlen = 0;
+		pktbase[i].dlen = 0;
+		pktbase[i].data = NULL;
+		pktbase[i].hdr = NULL;
+	}
+	/* Initialize RX Packet structure List */
+	priv->rx_pktstrs.head = &pktbase[0];
+	priv->rx_pktstrs.tail = &pktbase[priv->bufcfg.rx_pkts-1];
+	priv->rx_pktstrs.tail->next = NULL;
+	priv->state = STATE_STOPPED;
+
+	/* Get DMA Channel Configuration: RX Packet Max Length */
+	for (i=0; i<priv->chan_cnt; i++) {
+		if ( priv->dma[i] ) {
+			grspw_dma_config_read(priv->dma[i], &cfg);
+			priv->dma_rxmaxlen[i] = cfg.rxmaxlen;
+		}
+	}
+
+	/* Register at Memory MAP Library, this ensures that if user try to
+	 * change memory map or similar this driver will be informed and
+	 * all DMA activity to/from the mapped memory will be stopped, it
+	 * will be interpreted as an ioctl(STOP) event.
+	 */
+	priv->mmapdrv.priv = priv;
+	priv->mmapdrv.name = priv->name;
+	priv->mmapdrv.unmap = grspwu_unmap;
+	priv->mmapdrv.next = NULL;
+	if ( maplib_drv_reg(MMAPLIB_POOL, &priv->mmapdrv) ) {
+		/* Failed to register, this is because user has not setup
+		 * or mapped all memory to user space. We cannot proceed
+		 * until user has fixed this.
+		 */
+		return -EPERM;
+	}
+
+	/* START all Channels for DMA operations */
+	for (i=0; i<priv->chan_cnt; i++) {
+		if ( priv->dma[i] ) {
+			if ( grspw_dma_start(priv->dma[i]) ) {
+				/* Failed to start DMA channel */
+				goto err_out;
+			}
+		}
+	}
+
+	priv->state = STATE_STARTED;
+
+	return 0;
+
+err_out:
+	/* Remove MMAP callback */
+	maplib_drv_unreg(MMAPLIB_POOL, &priv->mmapdrv);
+
+	/* Something bad happened, stop all opened DMA channels */
+	for (i=0; i<priv->chan_cnt; i++) {
+		if ( priv->dma[i] )
+			grspw_dma_stop(priv->dma[i]);
+	}
+	return -EIO;
+}
+
+void grspwu_stop(struct grspwu_priv *priv)
+{
+	int i;
+
+	if ( priv->state < STATE_STARTED )
+		return; /* Already stopped */
+
+	/* DMA operation has stopped but user may still read out
+	 * buffers. writes are disabled by this. When internal
+	 * structures are destroyed, then we enter STATE_STOPPED.
+	 */
+	priv->state = STATE_PAUSED;
+
+	/* Stop all opened DMA channels */
+	for (i=0; i<priv->chan_cnt; i++) {
+		if ( priv->dma[i] )
+			grspw_dma_stop(priv->dma[i]);
+	}
+
+	/* Remove MMAP callback now that we do not use any more of that
+	 * memory, user may still access the memory, however this driver
+	 * or hardware will not
+	 */
+	maplib_drv_unreg(MMAPLIB_POOL, &priv->mmapdrv);
+}
+
+void grspwu_unmap(int idx, void *priv)
+{
+	if ( idx != MMAPLIB_POOL )
+		return;
+
+	/* User has done something bad: unmapped memory without stopping
+	 * the current SpaceWire operation. To avoid memory overwriting
+	 * all DMA activity to this memory must stop.
+	 */
+	grspwu_stop(priv);
+}
+
+static long grspwu_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	struct grspwu_priv *priv;
+	int err;
+	void __user *argp = (void __user *)arg;
+
+	priv = (struct grspwu_priv *)filp->private_data;
+
+	if (priv == NULL) {
+		printk (KERN_WARNING "GRSPWU: device %d doesnt exist\n", priv->index);
+		return -ENODEV;
+	}
+
+	/* Verify READ/WRITE Access to user provided buffer */
+	err = 0;
+	if (_IOC_DIR(cmd) & _IOC_WRITE)
+		err = !access_ok(VERIFY_WRITE, (void __user *)arg, _IOC_SIZE(cmd));
+
+	if ((!err) && (_IOC_DIR(cmd) & _IOC_READ))
+		err = !access_ok(VERIFY_READ, (void __user *)arg, _IOC_SIZE(cmd));
+
+	if (err)
+		return -EFAULT;
+
+	switch (cmd) {
+
+	/* Read hardware support */
+	case GRSPW_IOCTL_HWSUP:
+	{
+		struct grspw_hw_sup hwcfg;
+		grspw_hw_support(priv->dh, &hwcfg);
+		if(copy_to_user(argp, &hwcfg, sizeof(struct grspw_hw_sup)))
+			return -EFAULT;
+		break;
+	}
+
+	/* Set buffer configuration */
+	case GRSPW_IOCTL_BUFCFG:
+	{
+		struct grspw_bufcfg bufcfg;
+		if ( priv->state == STATE_STARTED)
+			return -EBUSY;
+		if(copy_from_user(&bufcfg, argp, sizeof(struct grspw_bufcfg)))
+			return -EFAULT;
+		return grspwu_bufcfg(priv, &bufcfg);
+	}
+
+	/* Set Configuration */
+	case GRSPW_IOCTL_CONFIG_SET:
+	{
+		struct grspw_config cfg;
+		if ( priv->state == STATE_STARTED )
+			return -EBUSY;
+		if(copy_from_user(&cfg, argp, sizeof(struct grspw_config)))
+			return -EFAULT;
+		return grspwu_config_set(priv, &cfg);
+	}
+
+	/* Read current configuration */
+	case GRSPW_IOCTL_CONFIG_READ:
+	{
+		struct grspw_config cfg;
+		grspwu_config_read(priv, &cfg);
+		if(copy_to_user(argp, &cfg, sizeof(struct grspw_config)))
+			return -EFAULT;
+		break;
+	}
+
+	/* Read statistics from all DMA channels and global Core paramters */
+	case GRSPW_IOCTL_STATS_READ:
+	{
+		struct grspw_stats stats;
+		grspwu_stats_read(priv, &stats);
+		if(copy_to_user(argp, &stats, sizeof(struct grspw_stats)))
+			return -EFAULT;
+		break;
+	}
+
+	/* Clear Statistics for all DMA channels and Core */
+	case GRSPW_IOCTL_STATS_CLR:
+	{
+		grspwu_stats_clr(priv);
+		break;
+	}
+
+	case GRSPW_IOCTL_LINKCTRL:
+	{
+		struct grspw_link_ctrl ctrl;
+		if(copy_from_user(&ctrl, argp, sizeof(struct grspw_link_ctrl)))
+			return -EFAULT;
+		return grspwu_link_ctrl(priv, &ctrl);
+	}
+
+	case GRSPW_IOCTL_PORTCTRL:
+	{
+		return grspwu_port_ctrl(priv, (int)argp);
+	}
+
+	case GRSPW_IOCTL_LINKSTATE:
+	{
+		struct grspw_link_state state;
+		grspwu_link_state_get(priv, &state);
+		if(copy_to_user(argp, &state, sizeof(struct grspw_link_state)))
+			return -EFAULT;
+		break;
+	}
+
+	case GRSPW_IOCTL_TC_SEND:
+	{
+		grspwu_tc_send(priv, (int)argp);
+		break;
+	}
+
+	case GRSPW_IOCTL_TC_READ:
+	{
+		int tc;
+		grspwu_tc_read(priv, &tc);
+		if(copy_to_user(argp, &tc, sizeof(int)))
+			return -EFAULT;
+		break;
+	}
+
+	case GRSPW_IOCTL_QPKTCNT:
+	{
+		struct grspw_qpktcnt qpktcnt;
+		grspwu_qpktcnt_read(priv, &qpktcnt);
+		if(copy_to_user(argp, &qpktcnt, sizeof(struct grspw_qpktcnt)))
+			return -EFAULT;
+		break;
+	}
+
+	case GRSPW_IOCTL_START:
+	{
+		return grspwu_start(priv, (int)argp);
+	}
+
+	case GRSPW_IOCTL_STOP:
+	{
+		grspwu_stop(priv);
+		break;
+	}
+
+	default: return -ENOSYS;
+	}
+
+	return 0;
+}
+
+/* Reclaim Packets from one particular DMA Channel to user space */
+int grspwu_reclaim(
+	struct grspwu_priv *priv, int dmachan,
+	struct grspw_rtxpkt __user *dst, int max)
+{
+	struct grspw_list sentpkts;
+	struct grspw_pkt *pkt;
+	void *ch = priv->dma[dmachan];
+	int count, pkts_cnt;
+	struct grspw_rtxpkt pkts[64];
+	int retval = 0;
+#ifdef GRSPWU_DEBUG
+	int tmp;
+#endif
+
+	/* Check that DMA Channel is open */
+	if ( ch == NULL )
+		return 0;
+
+	grspw_list_clr(&sentpkts);
+
+	/* Read up to MAX packets */
+	count = max;
+	grspw_dma_tx_reclaim(ch, 0, &sentpkts, &count);
+#ifdef GRSPWU_DEBUG
+	tmp = grspw_list_cnt(&sentpkts);
+	printk(KERN_DEBUG "tx_reclaim(): req %d got %d packets, chain: %d\n", max, count, tmp);
+	if ( tmp != count ) {
+		printk(KERN_DEBUG "tx_reclaim(): check1 failed\n");
+		return -EIO;
+	}
+#endif
+	if ( count < 1 ) {
+		/* If blocking wait until a number of Packets are available */
+#warning IMPLEMENT BLOCKING RECLAIM PER DMA CHANNEL.
+		return 0;
+	}
+
+	/* Convert packet list into "char-stream" in the format of
+	 * 'struct grspw_rtxpkt'
+	 */
+	pkt = sentpkts.head;
+	pkts_cnt = 0;
+	while ( pkt ) {
+		/* Convert Packet from grspw-lib packet to user space packet */
+		pkts[pkts_cnt].flags = pkt->flags;
+		pkts[pkts_cnt].dma_chan = dmachan;
+		pkts[pkts_cnt].resv1 = 0;
+		pkts[pkts_cnt].pkt_id = pkt->pkt_id;
+		pkts_cnt++;
+		
+		/* Sanity check */
+		if ( pkts_cnt > max ) {
+			printk(KERN_DEBUG "grspwu_reclaim(): max=%d overriden %d: internal bug\n", max, count);
+		}
+
+		/* Copy packets to User space in chunks of 64 packets at a time */
+		if ( pkts_cnt >= 64 ) {
+			if ( copy_to_user(dst, &pkts[0], sizeof(pkts)) ) {
+				retval = -EFAULT;
+				goto out;
+			}
+
+			/* Write to user-space */
+			dst += 64;
+			pkts_cnt = 0;
+		}
+		pkt = pkt->next;
+	}
+	if ( pkts_cnt > 0 ) {
+		/* Write remaining prepared packets to user space */
+		if ( copy_to_user(dst, &pkts[0], pkts_cnt*sizeof(struct grspw_rtxpkt)) ) {
+			retval = -EFAULT;
+			goto out;
+		}
+	}
+	retval = count;
+
+out:
+	/* Take List lock */
+	while ( down_interruptible(&priv->sem_pktstrs) )
+		;
+
+	/* Queue up packet structs for reuse */
+	if ( grspw_list_is_empty(&sentpkts) == 0 )
+		grspw_list_prepend_list(&priv->tx_pktstrs, &sentpkts);
+
+	/* Release Lock */
+	up(&priv->sem_pktstrs);
+
+	return retval;
+}
+
+/* Read Received Packets from one particular DMA Channel to user space */
+int grspwu_recv(
+	struct grspwu_priv *priv, int dmachan,
+	struct grspw_rrxpkt __user *dst, int max)
+{
+	struct grspw_list recvpkts;
+	struct grspw_pkt *pkt;
+	void *ch = priv->dma[dmachan];
+	int count, pkts_cnt;
+	struct grspw_rrxpkt pkts[32];
+	int retval = 0;
+#ifdef GRSPWU_DEBUG
+	int tmp;
+#endif
+
+	/* Check that DMA Channel is open */
+	if ( ch == NULL )
+		return 0;
+
+	grspw_list_clr(&recvpkts);
+
+	/* Read up to MAX packets */
+	count = max;
+	grspw_dma_rx_recv(ch, 0, &recvpkts, &count);
+#ifdef GRSPWU_DEBUG
+	tmp = grspw_list_cnt(&recvpkts);
+	printk(KERN_DEBUG "grspwu_recv(): req %d got %d packets, chain: %d\n", max, count, tmp);
+	if ( tmp != count ) {
+		printk(KERN_DEBUG "grspwu_recv(): check1 failed\n");
+		return -EIO;
+	}
+#endif
+	if ( count < 1 ) {
+		/* If blocking wait until a number of Packets are available */
+#warning IMPLEMENT BLOCKING RECV PER DMA CHANNEL.
+		return 0;
+	}
+
+	/* Convert packet list into "char-stream" in the format of
+	 * 'struct grspw_rrxpkt'
+	 */
+	pkt = recvpkts.head;
+	pkts_cnt = 0;
+	while ( pkt ) {
+		/* Convert Packet from grspw-lib packet to user space packet */
+		pkts[pkts_cnt].flags = pkt->flags;
+		pkts[pkts_cnt].dma_chan = dmachan;
+		pkts[pkts_cnt].resv1 = 0;
+		pkts[pkts_cnt].dlen = pkt->dlen;
+		pkts[pkts_cnt].data = pkt->hdr; /* UserSpaceAdr temporarily stored in HDR field */
+		pkts[pkts_cnt].pkt_id = pkt->pkt_id;
+#ifdef GRSPWU_DEBUG
+		if ( (pkts[pkts_cnt].flags | pkt->flags) & 0x0900 ) {
+			printk(KERN_DEBUG "RECV: 0x%x - 0x%x\n", pkts[pkts_cnt].flags, pkt->flags);
+		}
+#endif
+		pkts_cnt++;
+
+		/* Copy packets to User space in chunks of 32 packets at a time */
+		if ( pkts_cnt >= 32 ) {
+			if ( copy_to_user(dst, &pkts[0], sizeof(pkts)) ) {
+				retval = -EFAULT;
+				goto out;
+			}
+
+			/* Write to user-space */
+			dst += 32;
+			pkts_cnt = 0;
+		}
+		pkt = pkt->next;
+	}
+	if ( pkts_cnt > 0 ) {
+		/* Write remaining prepared packets to user space */
+		if ( copy_to_user(dst, &pkts[0], pkts_cnt*sizeof(struct grspw_rrxpkt)) ) {
+			retval = -EFAULT;
+			goto out;
+		}
+	}
+	retval = count;
+out:
+	/* Take List lock */
+	while ( down_interruptible(&priv->sem_pktstrs) )
+		;
+
+	/* Queue up packet structs for reuse */
+	grspw_list_prepend_list(&priv->rx_pktstrs, &recvpkts);
+
+	/* Release Lock */
+	up(&priv->sem_pktstrs);
+
+	return retval;
+}
+
+int grspwu_send(struct grspwu_priv *priv, int dmachan,
+		struct grspw_wtxpkt __user *src, int cnt)
+{
+	struct grspw_list txpkts;
+	struct grspw_pkt *pkt;
+	void *ch = priv->dma[dmachan];
+	int pktidx, count, size, left;
+	struct grspw_wtxpkt pkts[32];
+	int retval = 0;
+#ifdef GRSPWU_DEBUG
+	int tmp, tmp2;
+#endif
+
+	/* Check that DMA Channel is open */
+	if ( ch == NULL )
+		return 0;
+
+	/* One list per DMA Channel */
+	grspw_list_clr(&txpkts);
+	count = 0;
+#ifdef GRSPWU_DEBUG
+	tmp2 = cnt;
+#endif
+
+	while ( down_interruptible(&priv->sem_pktstrs) )
+		;
+
+	/* Take out as many Packet Structs from FREE Packet List as needed */
+	cnt = grspw_list_take_head_list(&priv->tx_pktstrs, &txpkts, cnt);
+
+	/* Release Lock */
+	up(&priv->sem_pktstrs);
+#ifdef GRSPWU_DEBUG
+	tmp = grspw_list_cnt(&txpkts);
+	printk(KERN_DEBUG "grspwu_send(): req %d got %d packets, chain: %d\n", tmp2, count, tmp);
+	if ( tmp != cnt ) {
+		printk(KERN_DEBUG "grspwu_send(): check1 failed\n");
+		return -EIO;
+	}
+#endif
+	if ( cnt < 1 )
+		return 0;
+
+	/* Convert packet list into "char-stream" in the format of
+	 * 'struct grspw_rrxpkt'
+	 */
+	pkt = txpkts.head;
+	pktidx = 32;
+	left = cnt;
+	while ( (left > 0) && pkt ) {
+		if ( pktidx >= 32 ) {
+			if ( left < 32 ) {
+				size = left * sizeof(struct grspw_wtxpkt);
+			} else {
+				size = sizeof(pkts);
+			}
+			if ( copy_from_user(&pkts[0], src, size) ) {
+				retval = -EFAULT;
+				goto out;
+			}
+
+			/* Write to user-space */
+			src += 32; /* wrong to increase when left<32, but never used */
+			pktidx = 0;
+		}
+		/* Convert Packet from grspw-lib packet to user space packet */
+		pkt->flags = pkts[pktidx].flags & TXPKT_FLAG_INPUT_MASK;
+		pkt->reserved = 0;
+		pkt->hlen = pkts[pktidx].hlen;
+		pkt->dlen = pkts[pktidx].dlen;
+		if ( (pkt->dlen > 0) && maplib_lookup_kern(
+				MMAPLIB_POOL,
+				pkts[pktidx].data,
+				NULL, /* Kernel address Unused */
+				&pkt->data, /* hardware address used in DMA */
+				pkt->dlen) ) {
+			/* Address translation Failed, memory may go over a 
+			 * block boundary, or comletely outside. Bug in USER
+			 * packet handling, or an attempt to destory kernel
+			 * memory.
+			 */
+			printk(KERN_DEBUG "GRSPWU: send(data): %d,%p,%d,0x%x\n", pktidx,
+				pkts[pktidx].data,pkt->dlen,pkt->flags);
+			retval = -EPERM;
+			goto out;
+		}
+		if ( (pkt->hlen > 0) && maplib_lookup_kern(
+				MMAPLIB_POOL,
+				pkts[pktidx].hdr,
+				NULL, /* Kernel address unused */
+				&pkt->hdr, /* hardware address used in DMA */
+				pkt->hlen) ) {
+			/* Address translation Failed, memory may go over a 
+			 * block boundary, or comletely outside. Bug in USER
+			 * packet handling, or an attempt to destory kernel
+			 * memory.
+			 */
+			 printk(KERN_DEBUG "GRSPWU: send(hdr): %d,%p,%d,0x%x\n", pktidx,
+			 	pkts[pktidx].hdr,pkt->hlen,pkt->flags);
+			 retval = -EPERM;
+			 goto out;
+		}
+		pkt->pkt_id = pkts[pktidx].pkt_id;
+
+		pktidx++;
+		pkt = pkt->next;
+		left--;
+	}
+	count = cnt - left;
+
+#ifdef GRSPWU_DEBUG
+	tmp = grspw_list_cnt(&txpkts);
+	printk(KERN_DEBUG "grspwu_send(): send %d packets, chain: %d\n", count, tmp);
+	if ( tmp != count ) {
+		printk(KERN_DEBUG "grspwu_send(): check2 failed\n");
+		return -EIO;
+	}
+#endif
+
+	/* Send the packets to respective DMA Channel */
+	if ( count > 0 ) {
+		grspw_dma_tx_send(priv->dma[dmachan], 0, &txpkts, count);
+	}
+
+	return count;
+
+out:
+	/* Return all Packet structures, send none of the read packets due
+	 * to an error. " Rewind all work".
+	 */
+	while ( down_interruptible(&priv->sem_pktstrs) )
+		;
+
+	grspw_list_prepend_list(&priv->tx_pktstrs, &txpkts);
+
+	up(&priv->sem_pktstrs);
+
+	return retval;
+}
+
+int grspwu_prepare(struct grspwu_priv *priv, int dmachan,
+		struct grspw_wrxpkt __user *src, int cnt)
+{
+	struct grspw_list readypkts;
+	struct grspw_pkt *pkt;
+	void *ch = priv->dma[dmachan];
+	int pktidx, count, size, left;
+	struct grspw_wrxpkt pkts[64];
+	int retval = 0;
+#ifdef GRSPWU_DEBUG
+	int tmp;
+#endif
+
+	/* Check that DMA Channel is open */
+	if ( ch == NULL )
+		return 0;
+
+	/* One list per DMA Channel */
+	grspw_list_clr(&readypkts);
+	count = 0;
+
+	while ( down_interruptible(&priv->sem_pktstrs) )
+		;
+
+	/* Take out as many Packet Structs from FREE Packet List as needed */
+	cnt = grspw_list_take_head_list(&priv->rx_pktstrs, &readypkts, cnt);
+
+	up(&priv->sem_pktstrs);
+#ifdef GRSPWU_DEBUG
+	tmp = grspw_list_cnt(&readypkts);
+	printk(KERN_DEBUG "grspwu_prepare(): got %d packets, chain: %d\n", cnt, tmp);
+	if ( tmp != cnt ) {
+		printk(KERN_DEBUG "grspwu_prepare(): check1 failed\n");
+		return -EIO;
+	}
+#endif
+	if ( cnt < 1 )
+		return 0;
+
+	/* Convert "char-stream" to in the format of
+	 * 'struct grspw_wrxpkt'
+	 */
+	pkt = readypkts.head;
+	pktidx = 64;
+	left = cnt;
+	while ( (left > 0) && pkt ) {
+		if ( pktidx >= 64 ) {
+			if ( left < 64 ) {
+				size = left * sizeof(struct grspw_wrxpkt);
+			} else {
+				size = sizeof(pkts);
+			}
+			if ( copy_from_user(&pkts[0], src, size) ) {
+				retval = -EFAULT;
+				goto out;
+			}
+
+			/* Write to user-space */
+			src += 64; /* wrong to increase when left<32, but never used */
+			pktidx = 0;
+		}
+		/* Convert Packet from grspw-lib packet to user space packet */
+		pkt->flags = pkts[pktidx].flags & RXPKT_FLAG_INPUT_MASK;
+		pkt->reserved = 0;
+		pkt->hlen = 0;
+		pkt->dlen = 0; /* Will be overwritten */
+		if ( maplib_lookup_kern(
+				MMAPLIB_POOL,
+				pkts[pktidx].data,
+				NULL, /* Kernel Address unused */
+				&pkt->data, /* hardware address used in DMA */
+				priv->dma_rxmaxlen[dmachan]) ) {
+			/* Address translation Failed, memory may go over a 
+			 * block boundary, or comletely outside. Bug in USER
+			 * packet handling, or an attempt to destory kernel
+			 * memory.
+			 */
+			 printk(KERN_DEBUG "GRSPWU: prepare(): %d,%p,%d,0x%x\n", pktidx,
+			 	pkts[pktidx].data,priv->dma_rxmaxlen[dmachan],pkt->flags);
+			 retval = -EPERM;
+			 goto out;
+		}
+		/* Remember UserSpaceAdr in unused Header Address to avoid one
+		 * extra address translation in recv() function.
+		 */
+		pkt->hdr = pkts[pktidx].data;
+		pkt->pkt_id = pkts[pktidx].pkt_id;
+
+		pktidx++;
+		pkt = pkt->next;
+		left--;
+	}
+	count = cnt - left;
+
+#ifdef GRSPWU_DEBUG
+	tmp = grspw_list_cnt(&readypkts);
+	printk(KERN_DEBUG "grspwu_prepare(): prep %d packets, chain: %d\n", count, tmp);
+	if ( tmp != count ) {
+		printk(KERN_DEBUG "grspwu_prepare(): check2 failed\n");
+		return -EIO;
+	}
+#endif
+
+	/* Send the packets to respective DMA Channel */
+	if ( count > 0 ) {
+		if ( grspw_dma_rx_prepare(priv->dma[dmachan], 0, &readypkts, count) )
+			return -EBUSY;
+	}
+
+	return count;
+
+out:
+	/* Return all Packet structures, pepare none of the read packets due
+	 * to an error. " Rewind all work".
+	 */
+	while ( down_interruptible(&priv->sem_pktstrs) )
+		;
+
+	grspw_list_prepend_list(&priv->rx_pktstrs, &readypkts);
+
+	up(&priv->sem_pktstrs);
+
+	return retval;
+}
+
+static int grspwu_read(struct file *filp, char __user *buff, size_t count, loff_t *offp)
+{
+	struct grspwu_priv *priv;
+	int operation, chan_mask;
+	int cnt, left, i;
+
+	priv = filp->private_data;
+
+	if ( (unsigned int)buff & 0x3 )
+		return -EINVAL;
+
+	if ( priv->state == STATE_STOPPED )
+		return -EBUSY;
+
+	/* Get Operation and channel selection from MSB of Length */
+	operation = count & GRSPW_READ_RECLAIM;
+	chan_mask = (count >> GRSPW_READ_CHANMSK_BIT) & GRSPW_READ_CHANMSK;
+	/* Limit Reads to 65kB */
+	count = count & 0xffff;
+
+	/* Assume first DMA Channel if none given. The most usual configuration 
+	 * of the GRSPW hardware is to have only one Channel.
+	 */
+	if ( chan_mask == 0 )
+		chan_mask = 1;
+
+#ifdef GRSPWU_DEBUG
+	printk(KERN_DEBUG "grspwu_read(): %d, %d, %d, %p\n", operation, chan_mask, count, buff);
+#endif
+	if ( operation ) {
+		/* RECLAIM SENT PACKET BUFFERS */
+		struct grspw_rtxpkt __user *dst = (void *)buff;
+
+		count = count / sizeof(struct grspw_rtxpkt);
+		if ( count < 1 )
+			return -EINVAL;
+		left = count;
+		for (i=0; i<priv->chan_cnt; i++) {
+			if ( chan_mask & (1<<i) ) {
+				cnt = grspwu_reclaim(priv, i, dst, left);
+				/* If Failure then return the number of 
+				 * packets processed, or if none processed
+				 * return an error.
+				 */
+#ifdef GRSPWU_DEBUG
+				printk(KERN_DEBUG "grspwu_read(): recl %d, %d, %d\n", cnt, left, count);
+#endif
+				if ( cnt < 0 ) {
+					if ( left != count )
+						break;
+					return cnt;
+				}
+				if ( cnt == 0 )
+					continue;
+				left -= cnt;
+				dst += cnt;
+			}
+		}
+#ifdef GRSPWU_DEBUG
+		printk(KERN_DEBUG "grspwu_read(): recl return %d %d %d\n", left, count, (count - left) * sizeof(struct grspw_rtxpkt));
+#endif
+		return (count - left) * sizeof(struct grspw_rtxpkt);
+	} else {
+		/* READ RECEIVED PACKETS */
+		struct grspw_rrxpkt __user *dst = (void *)buff;
+		count = count / sizeof(struct grspw_rrxpkt);
+		if ( count < 1 )
+			return -EINVAL;
+		left = count;
+		for (i=0; i<priv->chan_cnt; i++) {
+			if ( chan_mask & (1<<i) ) {
+				cnt = grspwu_recv(priv, i, dst, left);
+				/* If Failure then return the number of 
+				 * packets processed, or if none processed
+				 * return an error.
+				 */
+				if ( cnt < 0 ) {
+					if ( left != count )
+						break;
+					return cnt;
+				}
+				if ( cnt == 0 )
+					continue;
+				left -= cnt;
+				dst += cnt;
+			}
+		}
+		return (count - left) * sizeof(struct grspw_rrxpkt);
+	}
+}
+
+static int grspwu_write(struct file *filp, const char __user *buff, size_t count, loff_t *offp)
+{
+	struct grspwu_priv *priv;
+	int operation, chan, cnt;
+
+	priv = filp->private_data;
+
+	/* The DMA Channels must be started */
+	if ( priv->state < STATE_STARTED )
+		return -EBUSY;
+
+	if ( (unsigned int)buff & 0x3 )
+		return -EINVAL;
+
+	/* Get Operation from MSB of Length */
+	operation = count & GRSPW_WRITE_SEND;
+	chan = (count >> GRSPW_WRITE_CHAN_BIT) & GRSPW_WRITE_CHAN;
+	/* Limit Reads to 65kB */
+	count = count & 0xffff;
+
+	/* The DMA Channels must be started and opened */
+	if ( priv->dma[chan] == NULL )
+		return -EBUSY;
+
+	if ( operation ) {
+		/* SEND PACKETS */
+		struct grspw_wtxpkt __user *src = (void *)buff;
+		int cnt;
+
+		count = count / sizeof(struct grspw_wtxpkt);
+		if ( count < 1 )
+			return -EINVAL;
+		cnt = grspwu_send(priv, chan, src, count);
+		if ( cnt < 0 )
+			return cnt;
+		return cnt * sizeof(struct grspw_wtxpkt);
+	} else {
+		/* PREPARE RECEIVE PACKET BUFFERS */
+		struct grspw_wrxpkt __user *src = (void *)buff;
+		count = count / sizeof(struct grspw_wrxpkt);
+		if ( count < 1 )
+			return -EINVAL;
+		cnt = grspwu_prepare(priv, chan, src, count);
+		if ( cnt < 0 )
+			return cnt;
+		return cnt * sizeof(struct grspw_wrxpkt);
+	}
+}
+
+void *grspwu_dev_add(int index)
+{
+	struct grspwu_priv *priv;
+	int err;
+	dev_t dev_id;
+
+	/* Allocate private memory for all GRSPWU cores */
+	priv = (struct grspwu_priv *)kmalloc(sizeof(*priv), GFP_KERNEL);
+	memset(priv, 0, sizeof(*priv));
+	privu_tab[index] = priv;
+
+	/* Init all private structures */
+	priv->index = index;
+	strcpy(priv->name, "GRSPW[N]");
+	priv->name[6] = '0' + priv->index;
+
+	/* Init and Register CHAR driver */
+	dev_id = MKDEV(GRSPWU_MAJOR, GRSPWU_MINOR+priv->index);
+	cdev_init(&priv->cdev, &grspwu_fops);
+	err = cdev_add(&priv->cdev, dev_id, 1);
+	if ( err ) {
+		printk(KERN_NOTICE "GRSPWU: Failed adding CHAR dev: %d\n", err);
+		privu_tab[priv->index] = 0;
+		kfree(priv);
+		return NULL;
+	}
+
+	return priv;
+}
+
+void grspwu_dev_del(int index, void *data)
+{
+	struct grspwu_priv *priv = data;
+
+	/* Take some action here to throw out user and remove private
+	 * structures.
+	 */
+	cdev_del(&priv->cdev);
+	kfree(priv);
+	privu_tab[index] = 0;
+}
+
+static int __init grspwu_init (void)
+{
+	int result;
+	dev_t dev_id;
+
+	/* Get Major Number of CHAR driver */
+	dev_id = MKDEV(GRSPWU_MAJOR, GRSPWU_MINOR);
+	result = register_chrdev_region(dev_id, GRSPWU_DEVCNT, "grspw");
+	if ( result < 0 ) {
+		printk(KERN_WARNING "GRSPWU: Failed to register CHAR region\n");
+		return 0;
+	}
+
+	memset(privu_tab, 0, sizeof(privu_tab));
+
+	grspw_initialize_user(grspwu_dev_add,grspwu_dev_del);
+
+	return 0;
+}
+
+static void __exit grspwu_exit (void)
+{
+	int i;
+	dev_t devno = MKDEV (GRSPWU_MAJOR, GRSPWU_MINOR);
+	struct grspwu_priv *priv;
+
+	/* Delete devices */
+	for (i = 0; i<GRSPW_MAX; i++) {
+		priv = privu_tab[i];
+		if ( priv )
+			grspwu_dev_del(i, priv);
+	}
+
+	unregister_chrdev_region(devno, GRSPWU_DEVCNT);
+}
+
+module_init(grspwu_init);
+module_exit(grspwu_exit);
+
+MODULE_AUTHOR("Cobham Gaisler AB.");
+MODULE_DESCRIPTION("Cobham Gaisler GRSPW SpaceWire Driver");
+MODULE_LICENSE("GPL");
diff -Nru a/drivers/grlib/spw/grspw_user.c b/drivers/grlib/spw/grspw_user.c
--- a/drivers/grlib/spw/grspw_user.c	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/grlib/spw/grspw_user.c	2022-01-29 23:16:14.137064547 +0100
@@ -0,0 +1,1541 @@
+/*
+ * Cobham Gaisler GRSPW user space access Driver.
+ *
+ * Copyright (c) 2016-2022 Cobham Gaisler AB
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of
+ * the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston,
+ * MA 02110-1301 USA
+ */
+
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/fs.h>
+#include <linux/cdev.h>
+#include <asm/uaccess.h>
+#include <linux/mm.h>
+#include <linux/mman.h>
+#include <linux/slab.h>
+#include <linux/io.h>
+#include <linux/interrupt.h>
+#include <linux/semaphore.h>
+#include <linux/workqueue.h>
+#ifdef CONFIG_LEON
+#include <asm/leon.h>
+#endif
+#include <linux/grlib/grspw.h>
+#include <linux/grlib/grspw_user.h>
+#include <linux/grlib/maplib.h>
+#include <linux/grlib/devno.h>
+
+#if (GRSPW_MAX > GRSPWU_DEVCNT)
+#error GRSPW: not enough CHAR device nodes
+#endif
+
+#ifndef init_MUTEX
+#define init_MUTEX(a) sema_init(a, 1);
+#endif
+#ifndef init_MUTEX_LOCKED
+#define init_MUTEX_LOCKED(a) sema_init(a, 0);
+#endif
+
+/* Which default Memory MAP device node that should be used. User can
+ * config maplib pool index per GRSPW Device during runtime using
+ * GRSPW_IOCTL_BUFCFG ioctl command.
+ */
+#define MMAPLIB_POOL_DEFAULT 0
+
+/* Do not enable Debug */
+#undef GRSPWU_DEBUG
+
+enum {
+	STATE_STOPPED = 0,
+	STATE_PAUSED = 1,
+	STATE_STARTED = 2,
+};
+
+struct grspwu_priv {
+	char name[8];		/* GRSPWU[N] Name */
+	void *dh;		/* GRSPW Device Handle */
+	int index;		/* Device Index */
+	int chan_cnt;		/* Number of DMA Channels available in HW */
+	unsigned int chan_mask;	/* Enabled (by user) DMA Channel Mask */
+	unsigned int chan_avail_mask; /* DMA Channel Available Mask */
+	void *dma[4];		/* DMA Channel device handles */
+	int dma_rxmaxlen[4];	/* DMA RX Maxlength */
+	int state;		/* If DMA Channels are open and ready */
+	struct cdev cdev;	/* Char device */
+	struct grspw_config cfg;/* Current Configuration */
+	struct grspw_bufcfg bufcfg;/* Current Buffer Configuration */
+	void *_pkt_tx_structs;	/* Base address of packet structures */
+	void *_pkt_rx_structs;	/* Base address of packet structures */
+
+	struct semaphore sem_rx_pktstrs;/* Free RX Pkt structures Semaphore */
+	struct semaphore sem_tx_pktstrs;/* Free TX Pkt structure Semaphore */
+	struct grspw_list tx_pktstrs;	/* Free TX Pkt structures list */
+	struct grspw_list rx_pktstrs;	/* Free RX Pkt structures list */
+
+	/* Memory MAP Library instance */
+	struct maplib_drv mmapdrv;
+};
+
+/* Set this function pointer from another driver to do custom SpaceWire TimeCode
+ * RX handling.
+ */
+void (*grspwu_tc_isr_callback)(void *data, int tc) = NULL;
+void *grspwu_tc_isr_arg = NULL;
+
+struct grspw_bufcfg grspwu_def_bufcfg = 
+{
+	.rx_pkts = 1024,
+	.tx_pkts = 1024,
+	.maplib_pool_idx = MMAPLIB_POOL_DEFAULT,
+};
+
+static struct grspwu_priv *privu_tab[GRSPW_MAX];
+
+static int grspwu_open(struct inode *inode, struct file *file);
+static int grspwu_release(struct inode *inode, struct file *file);
+static long grspwu_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);
+static ssize_t grspwu_read(struct file *filp, char __user *buff, size_t count, loff_t *offp);
+static ssize_t grspwu_write(struct file *filp, const char __user *buff, size_t count, loff_t *offp);
+
+
+struct file_operations grspwu_fops = {
+	.owner          = THIS_MODULE,
+	.open           = grspwu_open,
+	.release        = grspwu_release,
+	.unlocked_ioctl = grspwu_ioctl,
+	.compat_ioctl   = grspwu_ioctl,
+	.mmap           = NULL,
+	.read           = grspwu_read,
+	.write          = grspwu_write,
+};
+
+void grspwu_stop(struct grspwu_priv *priv);
+void grspwu_unmap(int idx, void *p);
+
+static int grspwu_open(struct inode *inode, struct file *file)
+{
+	struct grspwu_priv *priv;
+	struct grspw_hw_sup hwcfg;
+	void *dh;
+
+	priv = container_of(inode->i_cdev, struct grspwu_priv, cdev);
+
+	if (priv == NULL) {
+		printk (KERN_WARNING "GRSPWU: device doesn't exist\n");
+		return -ENODEV;
+	}
+
+	/* Take a GRSPW Device, this routine will initialize/clear
+	 * hardware registers of the device.
+	 */
+	dh = grspw_open(priv->index);
+	if ( dh == NULL ) {
+		printk (KERN_WARNING "GRSPWU: device %d already opened\n", priv->index);
+		return -EBUSY;
+	}
+	priv->dh = dh;
+	file->private_data = priv;
+	priv->state = STATE_STOPPED;
+	priv->dma[0] = grspw_dma_open(priv->dh, 0);
+	if ( priv->dma[0] == NULL ) {
+		printk (KERN_WARNING "GRSPWU[%d]: failed to open DMA CHAN[0]\n", priv->index);
+		grspw_close(priv->dh);
+		priv->dh = NULL;
+		return -EIO;
+	}
+	priv->dma[1] = NULL;
+	priv->dma[2] = NULL;
+	priv->dma[3] = NULL;
+	priv->chan_mask = 1; /* Default to only one DMA Channel */
+	/* Get number of DMA Channels */
+	grspw_hw_support(priv->dh, &hwcfg);
+	priv->chan_cnt = hwcfg.ndma_chans;
+	priv->chan_avail_mask = ~(-1 << priv->chan_cnt);
+
+	/* Set Default Configuration */
+	memcpy(&priv->bufcfg, &grspwu_def_bufcfg, sizeof(grspwu_def_bufcfg));
+	priv->_pkt_tx_structs = NULL;
+	priv->_pkt_rx_structs = NULL;
+
+	grspw_list_clr(&priv->tx_pktstrs);
+	grspw_list_clr(&priv->rx_pktstrs);
+
+	init_MUTEX(&priv->sem_rx_pktstrs);
+	init_MUTEX(&priv->sem_tx_pktstrs);
+
+	return 0;
+}
+
+static int grspwu_release(struct inode *inode, struct file *file)
+{
+	struct grspwu_priv *priv = container_of(inode->i_cdev, struct grspwu_priv, cdev);
+	int i;
+
+	if ( priv->state == STATE_STARTED )
+		grspwu_stop(priv);
+	if ( priv->_pkt_tx_structs ) {
+		kfree(priv->_pkt_tx_structs);
+		priv->_pkt_tx_structs = NULL;
+	}
+	if ( priv->_pkt_rx_structs ) {
+		kfree(priv->_pkt_rx_structs);
+		priv->_pkt_rx_structs = NULL;
+	}
+	priv->state = STATE_STOPPED;
+	for (i=0; i<priv->chan_cnt; i++) {
+		if (priv->dma[i]) {
+			grspw_dma_close(priv->dma[i]);
+			priv->dma[i] = NULL;
+		}
+	}
+	grspw_close(priv->dh);
+	priv->dh = NULL;
+	file->private_data = NULL;
+
+	return 0;
+}
+
+/* Configure number of packet structures in driver (always called in stopped mode) */
+int grspwu_bufcfg(struct grspwu_priv *priv, struct grspw_bufcfg *bufcfg)
+{
+	/* Check buf configuration */
+	if ( (bufcfg->rx_pkts < 1) || (bufcfg->tx_pkts < 1) )
+		return -EINVAL;
+	/* Configuration will take affect upon next start. If configuration
+	 * differs from last configuration previous allocated memory (if any)
+	 * will be freed.
+	 */
+	if ( (priv->bufcfg.rx_pkts != bufcfg->rx_pkts) ||
+	     (priv->bufcfg.tx_pkts != bufcfg->tx_pkts) ) {
+		priv->bufcfg.rx_pkts = bufcfg->rx_pkts;
+		priv->bufcfg.tx_pkts = bufcfg->tx_pkts;
+		if ( priv->_pkt_tx_structs ) {
+			kfree(priv->_pkt_tx_structs);
+			priv->_pkt_tx_structs = NULL;
+			priv->state = STATE_STOPPED;
+		}
+		if ( priv->_pkt_rx_structs ) {
+			kfree(priv->_pkt_rx_structs);
+			priv->_pkt_rx_structs = NULL;
+			priv->state = STATE_STOPPED;
+		}
+	}
+
+	/* Basic sanity check of the input argument */
+	if (maplib_drv_chkvalid(bufcfg->maplib_pool_idx))
+		return -EINVAL;
+	priv->bufcfg.maplib_pool_idx = bufcfg->maplib_pool_idx;
+
+	return 0;
+}
+
+/* Configure Core and DMA Channels, RMAP and TimeCode setup */
+int grspwu_config_set(struct grspwu_priv *priv, struct grspw_config *cfg)
+{
+	int rmap_dstkey;
+	int i, retval;
+
+	/* Check RMAP Configuration */
+	if ( cfg->rmap_cfg & ~(RMAPOPTS_EN_RMAP|RMAPOPTS_EN_BUF) )
+		return -EINVAL;
+
+	/* Check Timecode Configuration, enabling RX IRQ not allowed from here */
+	if ( cfg->tc_cfg & ~(TCOPTS_EN_RX|TCOPTS_EN_TX) )
+		return -EINVAL;
+
+	/* Check Channel Enable Mask, report error if trying to enable a 
+	 * channel that is not available. */
+	if ( ~priv->chan_avail_mask & cfg->enable_chan_mask ) {
+		return -EINVAL;
+	}
+
+	/* 1. Configure Node address and Node mask of DMA Channels
+	 *    and default node address of core itself.
+	 */
+	cfg->adrcfg.promiscuous &= 1;
+	grspw_addr_ctrl(priv->dh, &cfg->adrcfg);
+
+	/* 2. Configure RMAP (Enable, Buffer, Destination Key) */
+	rmap_dstkey = cfg->rmap_dstkey;
+	if ( grspw_rmap_ctrl(priv->dh, &cfg->rmap_cfg, &rmap_dstkey) ) {
+		/* RMAP not available in Core, but trying to enable */
+		return -EIO;
+	}
+
+	/* 3. Timecode setup. If a custom timecode ISR callback handler is 
+	 *    installed that routine is installed and RX IRQ is enabled.
+	 */
+	if ( grspwu_tc_isr_callback ) {
+		grspw_tc_isr(priv->dh, grspwu_tc_isr_callback, grspwu_tc_isr_arg);
+		cfg->tc_cfg |= TCOPTS_EN_RXIRQ;
+	}
+	grspw_tc_ctrl(priv->dh, &cfg->tc_cfg);
+
+	/* 4. Configure DMA Channels and remember which channels will be
+	 *    used. Only the enabled channels will be activated and started
+	 *    during the ioctl(START) operation.
+	 */
+	retval = 0;
+	for (i=0; i<priv->chan_cnt; i++) {
+		/* Open or close DMA Channel */
+		if ( cfg->enable_chan_mask & (1<<i) ) {
+			/* Should be opened */
+			if ( priv->dma[i] == NULL ) {
+				priv->dma[i] = grspw_dma_open(priv->dh, i);
+				if ( priv->dma[i] == NULL ) {
+					retval = -EIO;
+					continue;
+				}
+				priv->chan_mask |= (1<<i);
+			}
+
+			/* Configure channel */
+			grspw_dma_config(priv->dma[i], &cfg->chan[i]);
+		} else {
+			/* Should be closed */
+			if ( priv->dma[i] ) {
+				grspw_dma_stop(priv->dma[i]);
+				grspw_dma_close(priv->dma[i]);
+				priv->dma[i] = NULL;
+			}
+			priv->chan_mask &= ~(1<<i);
+		}
+	}
+
+	return retval;
+}
+
+void grspwu_config_read(struct grspwu_priv *priv, struct grspw_config *cfg)
+{
+	int i;
+	int rmap_dstkey;
+
+	cfg->adrcfg.promiscuous = -1;
+	grspw_addr_ctrl(priv->dh, &cfg->adrcfg);
+
+	cfg->rmap_cfg = -1;
+	rmap_dstkey = -1;
+	grspw_rmap_ctrl(priv->dh, &cfg->rmap_cfg, &rmap_dstkey);
+	cfg->rmap_dstkey = rmap_dstkey;
+
+	cfg->tc_cfg = -1;
+	grspw_tc_ctrl(priv->dh, &cfg->tc_cfg);
+
+	cfg->enable_chan_mask = priv->chan_mask;
+	for (i=0; i<priv->chan_cnt; i++) {
+		if ( priv->dma[i] ) {
+			grspw_dma_config_read(priv->dma[i], &cfg->chan[i]);
+		}
+	}
+}
+
+void grspwu_stats_read(struct grspwu_priv *priv, struct grspw_stats *stats)
+{
+	int i;
+
+	grspw_stats_read(priv->dh, &stats->stats);
+
+	for (i=0; i<priv->chan_cnt; i++) {
+		if ( priv->dma[i] )
+			grspw_dma_stats_read(priv->dma[i], &stats->chan[i]);
+		else
+			memset(&stats->chan[i], 0, sizeof(struct grspw_dma_stats));
+	}
+}
+
+void grspwu_stats_clr(struct grspwu_priv *priv)
+{
+	int i;
+
+	grspw_stats_clr(priv->dh);
+
+	for (i=0; i<priv->chan_cnt; i++) {
+		if ( priv->dma[i] )
+			grspw_dma_stats_clr(priv->dma[i]);
+	}
+}
+
+int grspwu_link_ctrl(struct grspwu_priv *priv, struct grspw_link_ctrl *ctrl)
+{
+	int clkdiv, stscfg;
+
+	/* Check Configuration */
+	if ( ctrl->ctrl & ~LINKOPTS_MASK )
+		return -EINVAL;
+
+	/* Configure */
+	clkdiv = ctrl->clkdiv_start<<8 | ctrl->clkdiv_run;
+	grspw_link_ctrl(priv->dh, &ctrl->ctrl, &stscfg, &clkdiv);
+
+	return 0;
+}
+
+int grspwu_port_ctrl(struct grspwu_priv *priv, int options)
+{	
+	int port;
+
+
+	if ( (options != 0) && (options != 1)  )
+		port = 3; /* Let hardware select port */
+	else
+		port = options;
+
+	/* Set selected port, if failure it indicates that the selected
+	 * port does not exist (HW created with only one port).
+	 */
+	if ( grspw_port_ctrl(priv->dh, &port) ) 
+		return -EIO;
+
+	return 0;
+}
+
+void grspwu_link_state_get(struct grspwu_priv *priv, struct grspw_link_state *state)
+{
+	int clkdiv;
+
+	/* Get Current Link Configuration */
+	state->link_ctrl = -1;
+	clkdiv = -1;
+	grspw_link_ctrl(priv->dh, &state->link_ctrl, NULL, &clkdiv);
+	state->clkdiv_start = (clkdiv >> 8) & 0xff;
+	state->clkdiv_run = clkdiv & 0xff;
+
+	/* Get Current Link Status */
+	state->link_state = grspw_link_state(priv->dh);
+
+	/* Get Current Port configuration */
+	state->port_cfg = -1;
+	grspw_port_ctrl(priv->dh, &state->port_cfg);
+
+	/* Get Current Active port */
+	state->port_active = grspw_port_active(priv->dh);
+}
+
+/* Set TCTRL and TIMECNT, and Send Time-code */
+void grspwu_tc_send(struct grspwu_priv *priv, unsigned int options)
+{
+	int time;
+
+	/* Set TCTRL and TIMECNT? */
+	if ( options & GRSPW_TC_SEND_OPTION_SET_TCTRL ) {
+		time = options & GRSPW_TC_SEND_OPTION_TCTRL;
+		grspw_tc_time(priv->dh, &time);
+	}
+
+	/* Send Time Code (Generate Tick-In) */
+	if ( options & GRSPW_TC_SEND_OPTION_TCTX )
+		grspw_tc_tx(priv->dh);
+}
+
+void grspwu_tc_read(struct grspwu_priv *priv, int *tc)
+{
+	*tc = -1;
+	grspw_tc_time(priv->dh, tc);
+}
+
+void grspwu_qpktcnt_read(struct grspwu_priv *priv, struct grspw_qpktcnt *qpktcnt)
+{
+	int i;
+	for (i=0; i<priv->chan_cnt; i++) {
+		if ( priv->dma[i] ) {
+			grspw_dma_rx_count(
+				priv->dma[i],
+				&qpktcnt->chan[i].rx_ready,
+				&qpktcnt->chan[i].rx_sched,
+				&qpktcnt->chan[i].rx_recv,
+				&qpktcnt->chan[i].rx_hw);
+			grspw_dma_tx_count(
+				priv->dma[i],
+				&qpktcnt->chan[i].tx_send,
+				&qpktcnt->chan[i].tx_sched,
+				&qpktcnt->chan[i].tx_sent,
+				&qpktcnt->chan[i].tx_hw);
+		}
+	}
+}
+
+int grspwu_start(struct grspwu_priv *priv, int options)
+{
+	int size, i;
+	struct grspw_pkt *pktbase;
+	struct grspw_dma_config cfg;
+
+	if ( priv->state == STATE_STARTED )
+		return 0;
+
+	/* DMA Channels have been opened and configured at this point,
+	 * or user is satisfied with the default configuration.
+	 */
+
+	/* Allocate Packet structures if not already allocated */
+	if ( priv->_pkt_tx_structs == NULL ) {
+		size = sizeof(struct grspw_pkt) * priv->bufcfg.tx_pkts;
+		priv->_pkt_tx_structs = kmalloc(size, GFP_KERNEL);
+		if ( priv->_pkt_tx_structs == NULL )
+			return -ENOMEM;
+	}
+	if ( priv->_pkt_rx_structs == NULL ) {
+		size = sizeof(struct grspw_pkt) * priv->bufcfg.rx_pkts;
+		priv->_pkt_rx_structs = kmalloc(size, GFP_KERNEL);
+		if ( priv->_pkt_rx_structs == NULL )
+			return -ENOMEM;
+	}
+	/* Initialize Packet Structures to one long chain */
+	pktbase = (struct grspw_pkt *)priv->_pkt_tx_structs;
+	for (i=0; i<priv->bufcfg.tx_pkts; i++) {
+		pktbase[i].next = &pktbase[i+1];
+		pktbase[i].pkt_id = 0;
+		pktbase[i].flags = 0;
+		pktbase[i].reserved = 0;
+		pktbase[i].hlen = 0;
+		pktbase[i].dlen = 0;
+		pktbase[i].data = GRSPW_PKT_DMAADDR_INVALID;
+		pktbase[i].hdr = GRSPW_PKT_DMAADDR_INVALID;
+	}
+	/* Initialize TX Packet structure List */
+	priv->tx_pktstrs.head = &pktbase[0];
+	priv->tx_pktstrs.tail = &pktbase[priv->bufcfg.tx_pkts-1];
+	priv->tx_pktstrs.tail->next = NULL;
+	pktbase = (struct grspw_pkt *)priv->_pkt_rx_structs;
+	for (i=0; i<priv->bufcfg.rx_pkts; i++) {
+		pktbase[i].next = &pktbase[i+1];
+		pktbase[i].pkt_id = 0;
+		pktbase[i].flags = 0;
+		pktbase[i].reserved = 0;
+		pktbase[i].hlen = 0;
+		pktbase[i].dlen = 0;
+		pktbase[i].data = GRSPW_PKT_DMAADDR_INVALID;
+		pktbase[i].hdr = GRSPW_PKT_DMAADDR_INVALID;
+	}
+	/* Initialize RX Packet structure List */
+	priv->rx_pktstrs.head = &pktbase[0];
+	priv->rx_pktstrs.tail = &pktbase[priv->bufcfg.rx_pkts-1];
+	priv->rx_pktstrs.tail->next = NULL;
+	priv->state = STATE_STOPPED;
+
+	/* Get DMA Channel Configuration: RX Packet Max Length */
+	for (i=0; i<priv->chan_cnt; i++) {
+		if ( priv->dma[i] ) {
+			grspw_dma_config_read(priv->dma[i], &cfg);
+			priv->dma_rxmaxlen[i] = cfg.rxmaxlen;
+		}
+	}
+
+	/* Register at Memory MAP Library, this ensures that if user try to
+	 * change memory map or similar this driver will be informed and
+	 * all DMA activity to/from the mapped memory will be stopped, it
+	 * will be interpreted as an ioctl(STOP) event.
+	 */
+	priv->mmapdrv.priv = priv;
+	priv->mmapdrv.name = priv->name;
+	priv->mmapdrv.unmap = grspwu_unmap;
+	priv->mmapdrv.next = NULL;
+	if ( maplib_drv_reg(priv->bufcfg.maplib_pool_idx,
+			    &priv->mmapdrv) ) {
+		/* Failed to register, this is because user has not setup
+		 * or mapped all memory to user space. We cannot proceed
+		 * until user has fixed this.
+		 */
+		return -EPERM;
+	}
+
+	/* START all Channels for DMA operations */
+	for (i=0; i<priv->chan_cnt; i++) {
+		if ( priv->dma[i] ) {
+			if ( grspw_dma_start(priv->dma[i]) ) {
+				/* Failed to start DMA channel */
+				goto err_out;
+			}
+		}
+	}
+
+	priv->state = STATE_STARTED;
+
+	return 0;
+
+err_out:
+	/* Remove MMAP callback */
+	maplib_drv_unreg(priv->bufcfg.maplib_pool_idx, &priv->mmapdrv);
+
+	/* Something bad happened, stop all opened DMA channels */
+	for (i=0; i<priv->chan_cnt; i++) {
+		if ( priv->dma[i] )
+			grspw_dma_stop(priv->dma[i]);
+	}
+	return -EIO;
+}
+
+void grspwu_stop(struct grspwu_priv *priv)
+{
+	int i;
+
+	if ( priv->state < STATE_STARTED )
+		return; /* Already stopped */
+
+	/* DMA operation has stopped but user may still read out
+	 * buffers. writes are disabled by this. When internal
+	 * structures are destroyed, then we enter STATE_STOPPED.
+	 */
+	priv->state = STATE_PAUSED;
+
+	/* Stop all opened DMA channels */
+	for (i=0; i<priv->chan_cnt; i++) {
+		if ( priv->dma[i] )
+			grspw_dma_stop(priv->dma[i]);
+	}
+
+	/* Remove MMAP callback now that we do not use any more of that
+	 * memory, user may still access the memory, however this driver
+	 * or hardware will not
+	 */
+	maplib_drv_unreg(priv->bufcfg.maplib_pool_idx, &priv->mmapdrv);
+}
+
+void grspwu_unmap(int idx, void *p)
+{
+	struct grspwu_priv *priv = (struct grspwu_priv *)p;
+
+	if (idx != priv->bufcfg.maplib_pool_idx)
+		return;
+
+	/* User has done something bad: unmapped memory without stopping
+	 * the current SpaceWire operation. To avoid memory overwriting
+	 * all DMA activity to this memory must stop.
+	 */
+	grspwu_stop(priv);
+}
+
+int grspwu_rx_wait(struct grspwu_priv *priv, struct grspw_rx_wait_chan *wait)
+{
+	int rc;
+
+	if (wait->chan >= priv->chan_cnt)
+		return -EINVAL;
+	if (priv->dma[wait->chan] == NULL)
+		return -EBUSY;
+
+	rc = grspw_dma_rx_wait(priv->dma[wait->chan],
+			       wait->recv_cnt, wait->op, wait->ready_cnt,
+			       wait->timeout_ms * HZ/1000);
+	if (rc == -1)
+		return -EAGAIN; /* unknown error */
+	if (rc == 1)
+		return -EIO;
+	else if (rc == 2)
+		return -ETIME;
+	else if (rc == 3)
+		return -EBUSY;
+	else
+		return 0;
+}
+
+int grspwu_tx_wait(struct grspwu_priv *priv, struct grspw_tx_wait_chan *wait)
+{
+	int rc;
+
+	if (wait->chan >= priv->chan_cnt)
+		return -EINVAL;
+	if (priv->dma[wait->chan] == NULL)
+		return -EBUSY;
+
+	rc = grspw_dma_tx_wait(priv->dma[wait->chan],
+			       wait->send_cnt, wait->op, wait->sent_cnt,
+			       wait->timeout_ms * HZ/1000);
+	if (rc == -1)
+		return -EAGAIN; /* unknown error */
+	if (rc == 1)
+		return -EIO;
+	else if (rc == 2)
+		return -ETIME;
+	else if (rc == 3)
+		return -EBUSY;
+	else
+		return 0;
+}
+
+static long grspwu_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	struct grspwu_priv *priv;
+	void __user *argp = (void __user *)arg;
+
+	priv = (struct grspwu_priv *)filp->private_data;
+
+	if (priv == NULL) {
+		printk (KERN_WARNING "GRSPWU: device %d doesnt exist\n", priv->index);
+		return -ENODEV;
+	}
+
+	/* Verify READ/WRITE Access to user provided buffer */
+	if (_IOC_DIR(cmd) & (_IOC_WRITE | _IOC_READ))
+		if (!access_ok((void __user *)arg, _IOC_SIZE(cmd)))
+			return -EFAULT;
+
+	switch (cmd) {
+
+	/* Read hardware support */
+	case GRSPW_IOCTL_HWSUP:
+	{
+		struct grspw_hw_sup hwcfg;
+		grspw_hw_support(priv->dh, &hwcfg);
+		if(copy_to_user(argp, &hwcfg, sizeof(struct grspw_hw_sup)))
+			return -EFAULT;
+		break;
+	}
+
+	/* Set buffer configuration */
+	case GRSPW_IOCTL_BUFCFG:
+	{
+		struct grspw_bufcfg bufcfg;
+		if ( priv->state == STATE_STARTED)
+			return -EBUSY;
+		if(copy_from_user(&bufcfg, argp, sizeof(struct grspw_bufcfg)))
+			return -EFAULT;
+		return grspwu_bufcfg(priv, &bufcfg);
+	}
+
+	/* Set Configuration */
+	case GRSPW_IOCTL_CONFIG_SET:
+	{
+		struct grspw_config cfg;
+		if ( priv->state == STATE_STARTED )
+			return -EBUSY;
+		if(copy_from_user(&cfg, argp, sizeof(struct grspw_config)))
+			return -EFAULT;
+		return grspwu_config_set(priv, &cfg);
+	}
+
+	/* Read current configuration */
+	case GRSPW_IOCTL_CONFIG_READ:
+	{
+		struct grspw_config cfg;
+		grspwu_config_read(priv, &cfg);
+		if(copy_to_user(argp, &cfg, sizeof(struct grspw_config)))
+			return -EFAULT;
+		break;
+	}
+
+	/* Read statistics from all DMA channels and global Core paramters */
+	case GRSPW_IOCTL_STATS_READ:
+	{
+		struct grspw_stats stats;
+		grspwu_stats_read(priv, &stats);
+		if(copy_to_user(argp, &stats, sizeof(struct grspw_stats)))
+			return -EFAULT;
+		break;
+	}
+
+	/* Clear Statistics for all DMA channels and Core */
+	case GRSPW_IOCTL_STATS_CLR:
+	{
+		grspwu_stats_clr(priv);
+		break;
+	}
+
+	case GRSPW_IOCTL_LINKCTRL:
+	{
+		struct grspw_link_ctrl ctrl;
+		if(copy_from_user(&ctrl, argp, sizeof(struct grspw_link_ctrl)))
+			return -EFAULT;
+		return grspwu_link_ctrl(priv, &ctrl);
+	}
+
+	case GRSPW_IOCTL_PORTCTRL:
+	{
+		return grspwu_port_ctrl(priv, (long)argp);
+	}
+
+	case GRSPW_IOCTL_LINKSTATE:
+	{
+		struct grspw_link_state state;
+		grspwu_link_state_get(priv, &state);
+		if(copy_to_user(argp, &state, sizeof(struct grspw_link_state)))
+			return -EFAULT;
+		break;
+	}
+
+	case GRSPW_IOCTL_TC_SEND:
+	{
+		grspwu_tc_send(priv, (unsigned long)argp);
+		break;
+	}
+
+	case GRSPW_IOCTL_TC_READ:
+	{
+		int tc;
+		grspwu_tc_read(priv, &tc);
+		if(copy_to_user(argp, &tc, sizeof(int)))
+			return -EFAULT;
+		break;
+	}
+
+	case GRSPW_IOCTL_QPKTCNT:
+	{
+		struct grspw_qpktcnt qpktcnt;
+		grspwu_qpktcnt_read(priv, &qpktcnt);
+		if(copy_to_user(argp, &qpktcnt, sizeof(struct grspw_qpktcnt)))
+			return -EFAULT;
+		break;
+	}
+
+	case GRSPW_IOCTL_STATUS_READ:
+	{
+		unsigned int status;
+		status = grspw_link_status(priv->dh);
+		if(copy_to_user(argp, &status, sizeof(unsigned int)))
+			return -EFAULT;
+		break;
+	}
+
+	case GRSPW_IOCTL_STATUS_CLR:
+	{
+		unsigned int clr_mask;
+		if ( priv->state == STATE_STARTED)
+			return -EBUSY;
+		if(copy_from_user(&clr_mask, argp, sizeof(unsigned int)))
+			return -EFAULT;
+		grspw_link_status_clr(priv->dh, clr_mask);
+		break;
+	}
+
+	case GRSPW_IOCTL_START:
+	{
+		return grspwu_start(priv, (long)argp);
+	}
+
+	case GRSPW_IOCTL_STOP:
+	{
+		grspwu_stop(priv);
+		break;
+	}
+
+	case GRSPW_IOCTL_RX_WAIT:
+	{
+		struct grspw_rx_wait_chan wait;
+		if(copy_from_user(&wait, argp, sizeof(struct grspw_rx_wait_chan)))
+			return -EFAULT;
+		return grspwu_rx_wait(priv, &wait);
+	}
+
+	case GRSPW_IOCTL_TX_WAIT:
+	{
+		struct grspw_tx_wait_chan wait;
+		if(copy_from_user(&wait, argp, sizeof(struct grspw_tx_wait_chan)))
+			return -EFAULT;
+		return grspwu_tx_wait(priv, &wait);
+	}
+	
+	default: return -ENOSYS;
+	}
+
+	return 0;
+}
+
+/* Reclaim Packets from one particular DMA Channel to user space */
+int grspwu_reclaim(
+	struct grspwu_priv *priv, int dmachan,
+	struct grspw_rtxpkt __user *dst, int max)
+{
+	struct grspw_list sentpkts;
+	struct grspw_pkt *pkt;
+	void *ch = priv->dma[dmachan];
+	int count, pkts_cnt;
+	struct grspw_rtxpkt pkts[64];
+	int retval = 0;
+#ifdef GRSPWU_DEBUG
+	int tmp;
+#endif
+
+	/* Check that DMA Channel is open */
+	if ( ch == NULL )
+		return 0;
+
+	grspw_list_clr(&sentpkts);
+
+	/* Read up to MAX packets */
+	count = max;
+	grspw_dma_tx_reclaim(ch, 0, &sentpkts, &count);
+#ifdef GRSPWU_DEBUG
+	tmp = grspw_list_cnt(&sentpkts);
+	printk(KERN_DEBUG "tx_reclaim(): req %d got %d packets, chain: %d\n", max, count, tmp);
+	if ( tmp != count ) {
+		printk(KERN_DEBUG "tx_reclaim(): check1 failed\n");
+		return -EIO;
+	}
+#endif
+	if ( count < 1 ) {
+		/* IMPLEMENT BLOCKING RECLAIM PER DMA CHANNEL HERE.
+		 * The ioctl() implements the TX_WAIT currenlty.
+		 */
+
+		/* If blocking wait until a number of Packets are available */
+		
+		return 0;
+	}
+
+	/* Convert packet list into "char-stream" in the format of
+	 * 'struct grspw_rtxpkt'
+	 */
+	pkt = sentpkts.head;
+	pkts_cnt = 0;
+	while ( pkt ) {
+		/* Convert Packet from grspw-lib packet to user space packet */
+		pkts[pkts_cnt].flags = pkt->flags;
+		pkts[pkts_cnt].dma_chan = dmachan;
+		pkts[pkts_cnt].resv1 = 0;
+		pkts[pkts_cnt].pkt_id = pkt->pkt_id;
+		pkts_cnt++;
+		
+		/* Sanity check */
+		if ( pkts_cnt > max ) {
+			printk(KERN_DEBUG "grspwu_reclaim(): max=%d overriden %d: internal bug\n", max, count);
+		}
+
+		/* Copy packets to User space in chunks of 64 packets at a time */
+		if ( pkts_cnt >= 64 ) {
+			if ( copy_to_user(dst, &pkts[0], sizeof(pkts)) ) {
+				retval = -EFAULT;
+				goto out;
+			}
+
+			/* Write to user-space */
+			dst += 64;
+			pkts_cnt = 0;
+		}
+		pkt = pkt->next;
+	}
+	if ( pkts_cnt > 0 ) {
+		/* Write remaining prepared packets to user space */
+		if ( copy_to_user(dst, &pkts[0], pkts_cnt*sizeof(struct grspw_rtxpkt)) ) {
+			retval = -EFAULT;
+			goto out;
+		}
+	}
+	retval = count;
+
+out:
+	/* Take List lock */
+	while ( down_interruptible(&priv->sem_tx_pktstrs) )
+		;
+
+	/* Queue up packet structs for reuse */
+	if ( grspw_list_is_empty(&sentpkts) == 0 )
+		grspw_list_prepend_list(&priv->tx_pktstrs, &sentpkts);
+
+	/* Release Lock */
+	up(&priv->sem_tx_pktstrs);
+
+	return retval;
+}
+
+/* Read Received Packets from one particular DMA Channel to user space */
+int grspwu_recv(
+	struct grspwu_priv *priv, int dmachan,
+	struct grspw_rrxpkt __user *dst, int max)
+{
+	struct grspw_list recvpkts;
+	struct grspw_pkt *pkt;
+	void *ch = priv->dma[dmachan];
+	int count, pkts_cnt;
+	struct grspw_rrxpkt pkts[32];
+	int retval = 0;
+#ifdef GRSPWU_DEBUG
+	int tmp;
+#endif
+
+	/* Check that DMA Channel is open */
+	if ( ch == NULL )
+		return 0;
+
+	grspw_list_clr(&recvpkts);
+
+	/* Read up to MAX packets */
+	count = max;
+	grspw_dma_rx_recv(ch, 0, &recvpkts, &count);
+#ifdef GRSPWU_DEBUG
+	tmp = grspw_list_cnt(&recvpkts);
+	printk(KERN_DEBUG "grspwu_recv(): req %d got %d packets, chain: %d\n", max, count, tmp);
+	if ( tmp != count ) {
+		printk(KERN_DEBUG "grspwu_recv(): check1 failed\n");
+		return -EIO;
+	}
+#endif
+	if ( count < 1 ) {
+		/* IMPLEMENT BLOCKING RECV PER DMA CHANNEL HERE.
+		 * Currently ioctl() implements the RX_WAIT instead.
+		 */
+
+		/* If blocking wait until a number of Packets are available */
+		return 0;
+	}
+
+	/* Convert packet list into "char-stream" in the format of
+	 * 'struct grspw_rrxpkt'
+	 */
+	pkt = recvpkts.head;
+	pkts_cnt = 0;
+	while ( pkt ) {
+		/* Convert Packet from grspw-lib packet to user space packet */
+		pkts[pkts_cnt].flags = pkt->flags;
+		pkts[pkts_cnt].dma_chan = dmachan;
+		pkts[pkts_cnt].resv1 = 0;
+		pkts[pkts_cnt].dlen = pkt->dlen;
+		/* UserSpaceAdr temporarily stored in HDR field */
+		pkts[pkts_cnt].data = (void *)((unsigned long)pkt->hdr);
+		pkts[pkts_cnt].pkt_id = pkt->pkt_id;
+#ifdef GRSPWU_DEBUG
+		if ( (pkts[pkts_cnt].flags | pkt->flags) & 0x0900 ) {
+			printk(KERN_DEBUG "RECV: 0x%x - 0x%x\n", pkts[pkts_cnt].flags, pkt->flags);
+		}
+#endif
+		pkts_cnt++;
+
+		/* Copy packets to User space in chunks of 32 packets at a time */
+		if ( pkts_cnt >= 32 ) {
+			if ( copy_to_user(dst, &pkts[0], sizeof(pkts)) ) {
+				retval = -EFAULT;
+				goto out;
+			}
+
+			/* Write to user-space */
+			dst += 32;
+			pkts_cnt = 0;
+		}
+		pkt = pkt->next;
+	}
+	if ( pkts_cnt > 0 ) {
+		/* Write remaining prepared packets to user space */
+		if ( copy_to_user(dst, &pkts[0], pkts_cnt*sizeof(struct grspw_rrxpkt)) ) {
+			retval = -EFAULT;
+			goto out;
+		}
+	}
+	retval = count;
+out:
+	/* Take List lock */
+	while ( down_interruptible(&priv->sem_rx_pktstrs) )
+		;
+
+	/* Queue up packet structs for reuse */
+	grspw_list_prepend_list(&priv->rx_pktstrs, &recvpkts);
+
+	/* Release Lock */
+	up(&priv->sem_rx_pktstrs);
+
+	return retval;
+}
+
+int grspwu_send(struct grspwu_priv *priv, int dmachan,
+		struct grspw_wtxpkt __user *src, int cnt)
+{
+	struct grspw_list txpkts;
+	struct grspw_pkt *pkt;
+	void *ch = priv->dma[dmachan];
+	int pktidx, count, size, left;
+	struct grspw_wtxpkt pkts[32];
+	int retval = 0;
+	phys_addr_t pa;
+#ifdef GRSPWU_DEBUG
+	int tmp, tmp2;
+#endif
+
+	/* Check that DMA Channel is open */
+	if ( ch == NULL )
+		return 0;
+
+	/* One list per DMA Channel */
+	grspw_list_clr(&txpkts);
+	count = 0;
+#ifdef GRSPWU_DEBUG
+	tmp2 = cnt;
+#endif
+
+	while ( down_interruptible(&priv->sem_tx_pktstrs) )
+		;
+
+	/* Take out as many Packet Structs from FREE Packet List as needed */
+	cnt = grspw_list_take_head_list(&priv->tx_pktstrs, &txpkts, cnt);
+
+	/* Release Lock */
+	up(&priv->sem_tx_pktstrs);
+#ifdef GRSPWU_DEBUG
+	tmp = grspw_list_cnt(&txpkts);
+	printk(KERN_DEBUG "grspwu_send(): req %d got %d packets, chain: %d\n", tmp2, count, tmp);
+	if ( tmp != cnt ) {
+		printk(KERN_DEBUG "grspwu_send(): check1 failed\n");
+		return -EIO;
+	}
+#endif
+	if ( cnt < 1 )
+		return 0;
+
+	/* Convert packet list into "char-stream" in the format of
+	 * 'struct grspw_rrxpkt'
+	 */
+	pkt = txpkts.head;
+	pktidx = 32;
+	left = cnt;
+	while ( (left > 0) && pkt ) {
+		if ( pktidx >= 32 ) {
+			if ( left < 32 ) {
+				size = left * sizeof(struct grspw_wtxpkt);
+			} else {
+				size = sizeof(pkts);
+			}
+			if ( copy_from_user(&pkts[0], src, size) ) {
+				retval = -EFAULT;
+				goto out;
+			}
+
+			/* Write to user-space */
+			src += 32; /* wrong to increase when left<32, but never used */
+			pktidx = 0;
+		}
+		/* Convert Packet from grspw-lib packet to user space packet */
+		pkt->flags = pkts[pktidx].flags & TXPKT_FLAG_INPUT_MASK;
+		pkt->reserved = 0;
+		pkt->hlen = pkts[pktidx].hlen;
+		pkt->dlen = pkts[pktidx].dlen;
+		if ( (pkt->dlen > 0) && maplib_lookup_kern(
+				priv->bufcfg.maplib_pool_idx,
+				pkts[pktidx].data,
+				NULL, /* Kernel address Unused */
+				&pa, /* hardware address used in DMA */
+				pkt->dlen)) {
+			/* Address translation Failed, memory may go over a 
+			 * block boundary, or comletely outside. Bug in USER
+			 * packet handling, or an attempt to destory kernel
+			 * memory.
+			 */
+			printk(KERN_DEBUG "GRSPWU: send(data): %d,%p,%d,0x%x\n", pktidx,
+				pkts[pktidx].data,pkt->dlen,pkt->flags);
+			retval = -EPERM;
+			goto out;
+		}
+		/* Truncate: Currently GRSPW2 IPs are only capable of performing DMA to
+		 * lower 32-bit address space
+		 */
+		pkt->data = (u32)(pa & 0xFFFFFFFFUL);
+		if ( (pkt->hlen > 0) && maplib_lookup_kern(
+				priv->bufcfg.maplib_pool_idx,
+				pkts[pktidx].hdr,
+				NULL, /* Kernel address unused */
+				&pa, /* hardware address used in DMA */
+				pkt->hlen) ) {
+			/* Address translation Failed, memory may go over a 
+			 * block boundary, or comletely outside. Bug in USER
+			 * packet handling, or an attempt to destory kernel
+			 * memory.
+			 */
+			 printk(KERN_DEBUG "GRSPWU: send(hdr): %d,%p,%d,0x%x\n", pktidx,
+			 	pkts[pktidx].hdr,pkt->hlen,pkt->flags);
+			 retval = -EPERM;
+			 goto out;
+		}
+		pkt->hdr = (u32)(pa & 0xFFFFFFFFUL);
+		pkt->pkt_id = pkts[pktidx].pkt_id;
+
+		pktidx++;
+		pkt = pkt->next;
+		left--;
+	}
+	count = cnt - left;
+
+#ifdef GRSPWU_DEBUG
+	tmp = grspw_list_cnt(&txpkts);
+	printk(KERN_DEBUG "grspwu_send(): send %d packets, chain: %d\n", count, tmp);
+	if ( tmp != count ) {
+		printk(KERN_DEBUG "grspwu_send(): check2 failed\n");
+		return -EIO;
+	}
+#endif
+
+	/* Send the packets to respective DMA Channel */
+	if ( count > 0 ) {
+		grspw_dma_tx_send(priv->dma[dmachan], 0, &txpkts, count);
+	}
+
+	return count;
+
+out:
+	/* Return all Packet structures, send none of the read packets due
+	 * to an error. " Rewind all work".
+	 */
+	while ( down_interruptible(&priv->sem_tx_pktstrs) )
+		;
+
+	grspw_list_prepend_list(&priv->tx_pktstrs, &txpkts);
+
+	up(&priv->sem_tx_pktstrs);
+
+	return retval;
+}
+
+int grspwu_prepare(struct grspwu_priv *priv, int dmachan,
+		struct grspw_wrxpkt __user *src, int cnt)
+{
+	struct grspw_list readypkts;
+	struct grspw_pkt *pkt;
+	void *ch = priv->dma[dmachan];
+	int pktidx, count, size, left;
+	struct grspw_wrxpkt pkts[64];
+	int retval = 0;
+	phys_addr_t pa;
+#ifdef GRSPWU_DEBUG
+	int tmp;
+#endif
+
+	/* Check that DMA Channel is open */
+	if ( ch == NULL )
+		return 0;
+
+	/* One list per DMA Channel */
+	grspw_list_clr(&readypkts);
+	count = 0;
+
+	while ( down_interruptible(&priv->sem_rx_pktstrs) )
+		;
+
+	/* Take out as many Packet Structs from FREE Packet List as needed */
+	cnt = grspw_list_take_head_list(&priv->rx_pktstrs, &readypkts, cnt);
+
+	up(&priv->sem_rx_pktstrs);
+#ifdef GRSPWU_DEBUG
+	tmp = grspw_list_cnt(&readypkts);
+	printk(KERN_DEBUG "grspwu_prepare(): got %d packets, chain: %d\n", cnt, tmp);
+	if ( tmp != cnt ) {
+		printk(KERN_DEBUG "grspwu_prepare(): check1 failed\n");
+		return -EIO;
+	}
+#endif
+	if ( cnt < 1 )
+		return 0;
+
+	/* Convert "char-stream" to in the format of
+	 * 'struct grspw_wrxpkt'
+	 */
+	pkt = readypkts.head;
+	pktidx = 64;
+	left = cnt;
+	while ( (left > 0) && pkt ) {
+		if ( pktidx >= 64 ) {
+			if ( left < 64 ) {
+				size = left * sizeof(struct grspw_wrxpkt);
+			} else {
+				size = sizeof(pkts);
+			}
+			if ( copy_from_user(&pkts[0], src, size) ) {
+				retval = -EFAULT;
+				goto out;
+			}
+
+			/* Write to user-space */
+			src += 64; /* wrong to increase when left<32, but never used */
+			pktidx = 0;
+		}
+		/* Convert Packet from grspw-lib packet to user space packet */
+		pkt->flags = pkts[pktidx].flags & RXPKT_FLAG_INPUT_MASK;
+		pkt->reserved = 0;
+		pkt->hlen = 0;
+		pkt->dlen = 0; /* Will be overwritten */
+		if ( maplib_lookup_kern(
+				priv->bufcfg.maplib_pool_idx,
+				pkts[pktidx].data,
+				NULL, /* Kernel Address unused */
+				&pa, /* hardware address used in DMA */
+				priv->dma_rxmaxlen[dmachan]) ) {
+			/* Address translation Failed, memory may go over a 
+			 * block boundary, or comletely outside. Bug in USER
+			 * packet handling, or an attempt to destory kernel
+			 * memory.
+			 */
+			 printk(KERN_DEBUG "GRSPWU: prepare(): %d,%p,%d,0x%x\n", pktidx,
+			 	pkts[pktidx].data,priv->dma_rxmaxlen[dmachan],pkt->flags);
+			 retval = -EPERM;
+			 goto out;
+		}
+		pkt->data = (u32)(pa & 0xFFFFFFFFUL);
+		/* Remember UserSpaceAdr in unused Header Address to avoid one
+		 * extra address translation in recv() function.
+		 */
+		pkt->hdr = (u32)((unsigned long)pkts[pktidx].data & 0xFFFFFFFFUL);
+		pkt->pkt_id = pkts[pktidx].pkt_id;
+
+		pktidx++;
+		pkt = pkt->next;
+		left--;
+	}
+	count = cnt - left;
+
+#ifdef GRSPWU_DEBUG
+	tmp = grspw_list_cnt(&readypkts);
+	printk(KERN_DEBUG "grspwu_prepare(): prep %d packets, chain: %d\n", count, tmp);
+	if ( tmp != count ) {
+		printk(KERN_DEBUG "grspwu_prepare(): check2 failed\n");
+		return -EIO;
+	}
+#endif
+
+	/* Send the packets to respective DMA Channel */
+	if ( count > 0 ) {
+		if ( grspw_dma_rx_prepare(priv->dma[dmachan], 0, &readypkts, count) )
+			return -EBUSY;
+	}
+
+	return count;
+
+out:
+	/* Return all Packet structures, pepare none of the read packets due
+	 * to an error. " Rewind all work".
+	 */
+	while ( down_interruptible(&priv->sem_rx_pktstrs) )
+		;
+
+	grspw_list_prepend_list(&priv->rx_pktstrs, &readypkts);
+
+	up(&priv->sem_rx_pktstrs);
+
+	return retval;
+}
+
+static ssize_t grspwu_read(struct file *filp, char __user *buff, size_t count, loff_t *offp)
+{
+	struct grspwu_priv *priv;
+	int operation, chan_mask;
+	int cnt, left, i;
+
+	priv = filp->private_data;
+
+	if ( (unsigned long)buff & 0x3 )
+		return -EINVAL;
+
+	if ( priv->state == STATE_STOPPED )
+		return -EBUSY;
+
+	/* Get Operation and channel selection from MSB of Length */
+	operation = count & GRSPW_READ_RECLAIM;
+	chan_mask = (count >> GRSPW_READ_CHANMSK_BIT) & GRSPW_READ_CHANMSK;
+	/* Limit Reads to 65kB */
+	count = count & 0xffff;
+
+	/* Assume first DMA Channel if none given. The most usual configuration 
+	 * of the GRSPW hardware is to have only one Channel.
+	 */
+	if ( chan_mask == 0 )
+		chan_mask = 1;
+
+#ifdef GRSPWU_DEBUG
+	printk(KERN_DEBUG "grspwu_read(): %d, %d, %d, %p\n", operation, chan_mask, count, buff);
+#endif
+	if ( operation ) {
+		/* RECLAIM SENT PACKET BUFFERS */
+		struct grspw_rtxpkt __user *dst = (void *)buff;
+
+		count = count / sizeof(struct grspw_rtxpkt);
+		if ( count < 1 )
+			return -EINVAL;
+		left = count;
+		for (i=0; i<priv->chan_cnt; i++) {
+			if ( chan_mask & (1<<i) ) {
+				cnt = grspwu_reclaim(priv, i, dst, left);
+				/* If Failure then return the number of 
+				 * packets processed, or if none processed
+				 * return an error.
+				 */
+#ifdef GRSPWU_DEBUG
+				printk(KERN_DEBUG "grspwu_read(): recl %d, %d, %d\n", cnt, left, count);
+#endif
+				if ( cnt < 0 ) {
+					if ( left != count )
+						break;
+					return cnt;
+				}
+				if ( cnt == 0 )
+					continue;
+				left -= cnt;
+				dst += cnt;
+			}
+		}
+#ifdef GRSPWU_DEBUG
+		printk(KERN_DEBUG "grspwu_read(): recl return %d %d %d\n", left, count, (count - left) * sizeof(struct grspw_rtxpkt));
+#endif
+		return (count - left) * sizeof(struct grspw_rtxpkt);
+	} else {
+		/* READ RECEIVED PACKETS */
+		struct grspw_rrxpkt __user *dst = (void *)buff;
+		count = count / sizeof(struct grspw_rrxpkt);
+		if ( count < 1 )
+			return -EINVAL;
+		left = count;
+		for (i=0; i<priv->chan_cnt; i++) {
+			if ( chan_mask & (1<<i) ) {
+				cnt = grspwu_recv(priv, i, dst, left);
+				/* If Failure then return the number of 
+				 * packets processed, or if none processed
+				 * return an error.
+				 */
+				if ( cnt < 0 ) {
+					if ( left != count )
+						break;
+					return cnt;
+				}
+				if ( cnt == 0 )
+					continue;
+				left -= cnt;
+				dst += cnt;
+			}
+		}
+		return (count - left) * sizeof(struct grspw_rrxpkt);
+	}
+}
+
+static ssize_t grspwu_write(struct file *filp, const char __user *buff, size_t count, loff_t *offp)
+{
+	struct grspwu_priv *priv;
+	int operation, chan, cnt;
+
+	priv = filp->private_data;
+
+	/* The DMA Channels must be started */
+	if ( priv->state < STATE_STARTED )
+		return -EBUSY;
+
+	if ( (unsigned long)buff & 0x3 )
+		return -EINVAL;
+
+	/* Get Operation from MSB of Length */
+	operation = count & GRSPW_WRITE_SEND;
+	chan = (count >> GRSPW_WRITE_CHAN_BIT) & GRSPW_WRITE_CHAN;
+	/* Limit Reads to 65kB */
+	count = count & 0xffff;
+
+	/* The DMA Channels must be started and opened */
+	if ( priv->dma[chan] == NULL )
+		return -EBUSY;
+
+	if ( operation ) {
+		/* SEND PACKETS */
+		struct grspw_wtxpkt __user *src = (void *)buff;
+		int cnt;
+
+		count = count / sizeof(struct grspw_wtxpkt);
+		if ( count < 1 )
+			return -EINVAL;
+		cnt = grspwu_send(priv, chan, src, count);
+		if ( cnt < 0 )
+			return cnt;
+		return cnt * sizeof(struct grspw_wtxpkt);
+	} else {
+		/* PREPARE RECEIVE PACKET BUFFERS */
+		struct grspw_wrxpkt __user *src = (void *)buff;
+		count = count / sizeof(struct grspw_wrxpkt);
+		if ( count < 1 )
+			return -EINVAL;
+		cnt = grspwu_prepare(priv, chan, src, count);
+		if ( cnt < 0 )
+			return cnt;
+		return cnt * sizeof(struct grspw_wrxpkt);
+	}
+}
+
+void *grspwu_dev_add(int index)
+{
+	struct grspwu_priv *priv;
+	int err;
+	dev_t dev_id;
+
+	/* Allocate private memory for all GRSPWU cores */
+	priv = (struct grspwu_priv *)kmalloc(sizeof(*priv), GFP_KERNEL);
+	memset(priv, 0, sizeof(*priv));
+	privu_tab[index] = priv;
+
+	/* Init all private structures */
+	priv->index = index;
+	strcpy(priv->name, "GRSPW[N]");
+	priv->name[6] = '0' + priv->index;
+
+	/* Init and Register CHAR driver */
+	dev_id = MKDEV(GRSPWU_MAJOR, GRSPWU_MINOR+priv->index);
+	cdev_init(&priv->cdev, &grspwu_fops);
+	err = cdev_add(&priv->cdev, dev_id, 1);
+	if ( err ) {
+		printk(KERN_NOTICE "GRSPWU: Failed adding CHAR dev: %d\n", err);
+		privu_tab[priv->index] = 0;
+		kfree(priv);
+		return NULL;
+	}
+
+	return priv;
+}
+
+void grspwu_dev_del(int index, void *data)
+{
+	struct grspwu_priv *priv = data;
+
+	/* Take some action here to throw out user and remove private
+	 * structures.
+	 */
+	cdev_del(&priv->cdev);
+	kfree(priv);
+	privu_tab[index] = 0;
+}
+
+static int __init grspwu_init (void)
+{
+	int result;
+	dev_t dev_id;
+
+	/* Get Major Number of CHAR driver */
+	dev_id = MKDEV(GRSPWU_MAJOR, GRSPWU_MINOR);
+	result = register_chrdev_region(dev_id, GRSPWU_DEVCNT, "grspw");
+	if ( result < 0 ) {
+		printk(KERN_WARNING "GRSPWU: Failed to register CHAR region\n");
+		return 0;
+	}
+
+	memset(privu_tab, 0, sizeof(privu_tab));
+
+	grspw_initialize_user(grspwu_dev_add,grspwu_dev_del);
+
+	return 0;
+}
+
+static void __exit grspwu_exit (void)
+{
+	int i;
+	dev_t devno = MKDEV (GRSPWU_MAJOR, GRSPWU_MINOR);
+	struct grspwu_priv *priv;
+
+	/* Delete devices */
+	for (i = 0; i<GRSPW_MAX; i++) {
+		priv = privu_tab[i];
+		if ( priv )
+			grspwu_dev_del(i, priv);
+	}
+	grspw_initialize_user(NULL, NULL);
+
+	unregister_chrdev_region(devno, GRSPWU_DEVCNT);
+}
+
+module_init(grspwu_init);
+module_exit(grspwu_exit);
+
+MODULE_AUTHOR("Cobham Gaisler AB.");
+MODULE_DESCRIPTION("Cobham Gaisler GRSPW SpaceWire Driver");
+MODULE_LICENSE("GPL");
+MODULE_ALIAS("platform:grlib-grspwu");
diff -Nru a/drivers/grlib/spw/Kbuild b/drivers/grlib/spw/Kbuild
--- a/drivers/grlib/spw/Kbuild	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/grlib/spw/Kbuild	2022-01-29 23:16:14.137064547 +0100
@@ -0,0 +1,9 @@
+#
+# Kbuild for the kernel GRSPW SpaceWire device drivers.
+#
+
+EXTRA_CFLAGS   += -I$(ROOT_PATH)drivers/grlib/include
+
+obj-$(CONFIG_GRLIB_GRSPW) += grspw.o
+obj-$(CONFIG_GRLIB_GRSPWU) += grspw_user.o
+obj-$(CONFIG_GRLIB_GRSPWROUTER) += grspw_router.o
diff -Nru a/drivers/grlib/spw/Kconfig b/drivers/grlib/spw/Kconfig
--- a/drivers/grlib/spw/Kconfig	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/grlib/spw/Kconfig	2022-01-29 23:16:14.137064547 +0100
@@ -0,0 +1,30 @@
+menu "GRSPW SpaceWire drivers"
+
+config GRLIB_GRSPWROUTER
+	tristate "GRSPW Router"
+	default n
+	depends on SPARC_LEON || RISCV
+	help
+	  This enables the GRSPW Router APB-Register interface driver. The 
+	  GRSPW Router is VHDL-model of a highly configurable SpaceWire router
+	  developed by Cobham Gaisler.
+
+config GRLIB_GRSPW
+        tristate "GRLIB GRSPW kernel library"
+        default n
+	depends on SPARC_LEON || RISCV
+        help
+          This adds the GRLIB GRSPW/GRSPW2 SpaceWire core kernel library,
+          it is required by the GRSPW-user driver.
+
+config GRLIB_GRSPWU
+        tristate "GRLIB GRSPW driver"
+        default n
+	depends on SPARC_LEON || RISCV
+        select GRLIB_MAPLIB
+	select GRLIB_GRSPW
+        help
+          Add this GRLIB GRSPW/GRSPW2 SpaceWire core driver if you want to
+	  be able to access the SpaceWire core(s) from User Space.
+
+endmenu # GRSPW SpaceWire drivers
diff -Nru a/drivers/grlib/VERSION b/drivers/grlib/VERSION
--- a/drivers/grlib/VERSION	1970-01-01 01:00:00.000000000 +0100
+++ b/drivers/grlib/VERSION	2022-01-29 23:16:14.137064547 +0100
@@ -0,0 +1,3 @@
+GRLIB_DRIVER_VERSION=1.2.0
+LINUX_GIT_RELEASE=5.10.43
+LINUX_GIT_COMMIT=951358a824f96be927ae50fad1e72e05bbb57b56
